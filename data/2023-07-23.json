[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study aims to develop and evaluate an innovative simulation algorithmfor generating thick-slice CT images that closely resemble actual images in theAAPM-Mayo's 2016 Low Dose CT Grand Challenge dataset. The proposed method wasevaluated using Peak Signal-to-Noise Ratio (PSNR) and Root Mean Square Error(RMSE) metrics, with the hypothesis that our simulation would produce imagesmore congruent with their real counterparts. Our proposed method demonstratedsubstantial enhancements in terms of both PSNR and RMSE over other simulationmethods. The highest PSNR values were obtained with the proposed method,yielding 49.7369 $pm$ 2.5223 and 48.5801 $pm$ 7.3271 for D45 and B30reconstruction kernels, respectively. The proposed method also registered thelowest RMSE with values of 0.0068 $pm$ 0.0020 and 0.0108 $pm$ 0.0099 for D45and B30, respectively, indicating a distribution more closely aligned with theauthentic thick-slice image. Further validation of the proposed simulationalgorithm was conducted using the TCIA LDCT-and-Projection-data dataset. Thegenerated images were then leveraged to train four distinct super-resolution(SR) models, which were subsequently evaluated using the real thick-sliceimages from the 2016 Low Dose CT Grand Challenge dataset. When trained withdata produced by our novel algorithm, all four SR models exhibited enhancedperformance.", "output": "Enhancing Super-Resolution Networks through Realistic Thick-Slice CT Simulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Backdoor attacks pose serious security threats to deep neural networks(DNNs). Backdoored models make arbitrarily (targeted) incorrect predictions oninputs embedded with well-designed triggers while behaving normally on cleaninputs. Many works have explored the invisibility of backdoor triggers toimprove attack stealthiness. However, most of them only consider theinvisibility in the spatial domain without explicitly accounting for thegeneration of invisible triggers in the frequency domain, making the generatedpoisoned images be easily detected by recent defense methods. To address thisissue, in this paper, we propose a DUal stealthy BAckdoor attack method namedDUBA, which simultaneously considers the invisibility of triggers in both thespatial and frequency domains, to achieve desirable attack performance, whileensuring strong stealthiness. Specifically, we first use Discrete WaveletTransform to embed the high-frequency information of the trigger image into theclean image to ensure attack effectiveness. Then, to attain strongstealthiness, we incorporate Fourier Transform and Discrete Cosine Transform tomix the poisoned image and clean image in the frequency domain. Moreover, theproposed DUBA adopts a novel attack strategy, in which the model is trainedwith weak triggers and attacked with strong triggers to further enhance theattack performance and stealthiness. We extensively evaluate DUBA againstpopular image classifiers on four datasets. The results demonstrate that itsignificantly outperforms the state-of-the-art backdoor attacks in terms of theattack success rate and stealthiness", "output": "A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The disruptive application of ChatGPT (GPT-3.5, GPT-4) to a variety ofdomains has become a topic of much discussion in the scientific community andsociety at large. Large Language Models (LLMs), e.g., BERT, Bard, GenerativePre-trained Transformers (GPTs), LLaMA, etc., have the ability to takeinstructions, or prompts, from users and generate answers and solutions basedon very large volumes of text-based training data. This paper assesses theimpact and potential impact of ChatGPT on the field of digital forensics,specifically looking at its latest pre-trained LLM, GPT-4. A series ofexperiments are conducted to assess its capability across several digitalforensic use cases including artefact understanding, evidence searching, codegeneration, anomaly detection, incident response, and education. Across thesetopics, its strengths and risks are outlined and a number of generalconclusions are drawn. Overall this paper concludes that while there are somepotential low-risk applications of ChatGPT within digital forensics, many areeither unsuitable at present, since the evidence would need to be uploaded tothe service, or they require sufficient knowledge of the topic being asked ofthe tool to identify incorrect assumptions, inaccuracies, and mistakes.However, to an appropriately knowledgeable user, it could act as a usefulsupporting tool in some circumstances.", "output": "ChatGPT for Digital Forensic Investigation: The Good, The Bad, and The Unknown."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial Intelligence (AI), a cornerstone of 21st-century technology, hasseen remarkable growth in China. In this paper, we examine China's AIdevelopment process, demonstrating that it is characterized by rapid learningand differentiation, surpassing the export-oriented growth propelled by ForeignDirect Investment seen in earlier Asian industrializers.Our data indicates that China currently leads the USA in the volume ofAI-related research papers. However, when we delve into the quality of thesepapers based on specific metrics, the USA retains a slight edge. Nevertheless,the pace and scale of China's AI development remain noteworthy.We attribute China's accelerated AI progress to several factors, includingglobal trends favoring open access to algorithms and research papers,contributions from China's broad diaspora and returnees, and relatively laxdata protection policies.In the vein of our research, we have developed a novel measure for gaugingChina's imitation of US research. Our analysis shows that by 2018, the time lagbetween China and the USA in addressing AI research topics had evaporated. Thisfinding suggests that China has effectively bridged a significant knowledge gapand could potentially be setting out on an independent research trajectory.While this study compares China and the USA exclusively, it's important tonote that research collaborations between these two nations have resulted inmore highly cited work than those produced by either country independently.This underscores the power of international cooperation in driving scientificprogress in AI.", "output": "Has China caught up to the US in AI research? An exploration of mimetic isomorphism as a model for late industrializers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Divorce is the legal dissolution of a marriage by a court. Since this isusually an unpleasant outcome of a marital union, each party may have reasonsto call the decision to quit which is generally documented in detail in thecourt proceedings. Via a substantial corpus of 17,306 court proceedings, thispaper investigates gender inequality through the lens of divorce courtproceedings. While emerging data sources (e.g., public court records) onsensitive societal issues hold promise in aiding social science research,biases present in cutting-edge natural language processing (NLP) methods mayinterfere with or affect such studies. We thus require a thorough analysis ofpotential gaps and limitations present in extant NLP resources. In this paper,on the methodological side, we demonstrate that existing NLP resources requiredseveral non-trivial modifications to quantify societal inequalities. On thesubstantive side, we find that while a large number of court cases perhapssuggest changing norms in India where women are increasingly challengingpatriarchy, AI-powered analyses of these court proceedings indicate strikinggender inequality with women often subjected to domestic violence.", "output": "Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Discriminatory language and biases are often present in hate speech duringconversations, which usually lead to negative impacts on targeted groups suchas those based on race, gender, and religion. To tackle this issue, we proposean approach that involves a two-step process: first, detecting hate speechusing a classifier, and then utilizing a debiasing component that generatesless biased or unbiased alternatives through prompts. We evaluated our approachon a benchmark dataset and observed reduction in negativity due to hate speechcomments. The proposed method contributes to the ongoing efforts to reducebiases in online discourse and promote a more inclusive and fair environmentfor communication.", "output": "Mitigating Bias in Conversations: A Hate Speech Classifier and Debiaser with Prompts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs)provide additional key-value pairs (i.e., qualifiers) for each KG fact thathelp to better restrict the fact validity. In recent years, there has been anincreasing interest in studying graph reasoning over HKGs. In the meantime, dueto the ever-evolving nature of world knowledge, extensive parallel works havebeen focusing on reasoning over temporal KGs (TKGs), where each TKG fact can beviewed as a KG fact coupled with a timestamp (or time period) specifying itstime validity. The existing HKG reasoning approaches do not consider temporalinformation because it is not explicitly specified in previous benchmarkdatasets. Besides, all the previous TKG reasoning methods only lay emphasis ontemporal reasoning and have no way to learn from qualifiers. To this end, weaim to fill the gap between TKG reasoning and HKG reasoning. We develop two newbenchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and YAGO-hy, andpropose a HTKG reasoning model that efficiently models both temporal facts andqualifiers. We further exploit additional time-invariant relational knowledgefrom the Wikidata knowledge base and study its effectiveness in HTKG reasoning.Time-invariant relational knowledge serves as the knowledge that remainsunchanged in time (e.g., Sasha Obama is the child of Barack Obama), and it hasnever been fully explored in previous TKG reasoning benchmarks and approaches.Experimental results show that our model substantially outperforms previousrelated methods on HTKG link prediction and can be enhanced by jointlyleveraging both temporal and time-invariant relational knowledge.", "output": "Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, the AI/ML research community has indicated an urgent need toestablish Responsible AI (RAI) values and practices as part of the AI/MLlifecycle. Several organizations and communities are responding to this call bysharing RAI guidelines. However, there are gaps in awareness, deliberation, andexecution of such practices for multi-disciplinary ML practitioners. This workcontributes to the discussion by unpacking co-production challenges faced bypractitioners as they align their RAI values. We interviewed 23 individuals,across 10 organizations, tasked to ship AI/ML based products while upholdingRAI norms and found that both top-down and bottom-up institutional structurescreate burden for different roles preventing them from upholding RAI values, achallenge that is further exacerbated when executing conflicted values. Weshare multiple value levers used as strategies by the practitioners to resolvetheir challenges. We end our paper with recommendations for inclusive andequitable RAI value-practices, creating supportive organizational structuresand opportunities to further aid practitioners.", "output": "`It is currently hodgepodge'': Examining AI/ML Practitioners' Challenges during Co-production of Responsible AI Values."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bias evaluation benchmarks and dataset and model documentation have emergedas central processes for assessing the biases and harms of artificialintelligence (AI) systems. However, these auditing processes have beencriticized for their failure to integrate the knowledge of marginalizedcommunities and consider the power dynamics between auditors and thecommunities. Consequently, modes of bias evaluation have been proposed thatengage impacted communities in identifying and assessing the harms of AIsystems (e.g., bias bounties). Even so, asking what marginalized communitieswant from such auditing processes has been neglected. In this paper, we askqueer communities for their positions on, and desires from, auditing processes.To this end, we organized a participatory workshop to critique and redesignbias bounties from queer perspectives. We found that when given space, thescope of feedback from workshop participants goes far beyond what bias bountiesafford, with participants questioning the ownership, incentives, and efficacyof bounties. We conclude by advocating for community ownership of bounties andcomplementing bounties with participatory processes (e.g., co-creation).", "output": "Bound by the Bounty: Collaboratively Shaping Evaluation Processes for Queer AI Harms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual Reinforcement Learning (Visual RL), coupled with high-dimensionalobservations, has consistently confronted the long-standing challenge ofgeneralization. Despite the focus on algorithms aimed at resolving visualgeneralization problems, we argue that the devil is in the existing benchmarksas they are restricted to isolated tasks and generalization categories,undermining a comprehensive evaluation of agents' visual generalizationcapabilities. To bridge this gap, we introduce RL-ViGen: a novel ReinforcementLearning Benchmark for Visual Generalization, which contains diverse tasks anda wide spectrum of generalization types, thereby facilitating the derivation ofmore reliable conclusions. Furthermore, RL-ViGen incorporates the latestgeneralization visual RL algorithms into a unified framework, under which theexperiment results indicate that no single existing algorithm has prevaileduniversally across tasks. Our aspiration is that RL-ViGen will serve as acatalyst in this area, and lay a foundation for the future creation ofuniversal visual generalization RL agents suitable for real-world scenarios.Access to our code and implemented algorithms is provided at", "output": "RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In classical logic, nonBoolean fluents, such as the location of an object,can be naturally described by functions. However, this is not the case inanswer set programs, where the values of functions are pre-defined, andnonmonotonicity of the semantics is related to minimizing the extents ofpredicates but has nothing to do with functions. We extend the first-orderstable model semantics by Ferraris, Lee, and Lifschitz to allow intensionalfunctions -- functions that are specified by a logic program just likepredicates are specified. We show that many known properties of the stablemodel semantics are naturally extended to this formalism and compare it withother related approaches to incorporating intensional functions. Furthermore,we use this extension as a basis for defining Answer Set Programming ModuloTheories (ASPMT), analogous to the way that Satisfiability Modulo Theories(SMT) is defined, allowing for SMT-like effective first-order reasoning in thecontext of ASP. Using SMT solving techniques involving functions, ASPMT can beapplied to domains containing real numbers and alleviates the groundingproblem. We show that other approaches to integrating ASP and CSP/SMT can berelated to special cases of ASPMT in which functions are limited tonon-intensional ones.", "output": "First-Order Stable Model Semantics with Intensional Functions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently Ferraris, Lee and Lifschitz proposed a new definition of stablemodels that does not refer to grounding, which applies to the syntax ofarbitrary first-order sentences. We show its relation to the idea of loopformulas with variables by Chen, Lin, Wang and Zhang, and generalize their loopformulas to disjunctive programs and to arbitrary first-order sentences. Wealso extend the syntax of logic programs to allow explicit quantifiers, anddefine its semantics as a subclass of the new language of stable models byFerraris et al. Such programs inherit from the general language the ability tohandle nonmonotonic reasoning under the stable model semantics even in theabsence of the unique name and the domain closure assumptions, while yieldingmore succinct loop formulas than the general language due to the restrictedsyntax. We also show certain syntactic conditions under which query answeringfor an extended program can be reduced to entailment checking in first-orderlogic, providing a way to apply first-order theorem provers to reasoning aboutnon-Herbrand stable models.", "output": "On Loop Formulas with Variables."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper continues the line of work on representing properties of actionsin nonmonotonic formalisms that stresses the distinction between being \"true\"and being \"caused\", as in the system of causal logic introduced by McCain andTurner and in the action language C proposed by Giunchiglia and Lifschitz. Theonly fluents directly representable in language C+ are truth-valued fluents,which is often inconvenient. We show that both causal logic and language C canbe extended to allow values from arbitrary nonempty sets. Our extension oflanguage C, called C+, also makes it possible to describe actions in terms oftheir attributes, which is important from the perspective of elaborationtolerance. We describe an embedding of C+ in causal theories with multi-valuedconstants, relate C+ to Pednault's action language ADL, and show howmulti-valued constants can be eliminated in favor of Boolean constants.", "output": "Causal Laws and Multi-Valued Fluents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Clinical Practice Guidelines (CPGs) for cancer diseases evolve rapidly due tonew evidence generated by active research. Currently, CPGs are primarilypublished in a document format that is ill-suited for managing this developingknowledge. A knowledge model of the guidelines document suitable forprogrammatic interaction is required. This work proposes an automated methodfor extraction of knowledge from National Comprehensive Cancer Network (NCCN)CPGs in Oncology and generating a structured model containing the retrievedknowledge. The proposed method was tested using two versions of NCCN Non-SmallCell Lung Cancer (NSCLC) CPG to demonstrate the effectiveness in faithfulextraction and modeling of knowledge. Three enrichment strategies using Cancerstaging information, Unified Medical Language System (UMLS) Metathesaurus &amp;National Cancer Institute thesaurus (NCIt) concepts, and Node classificationare also presented to enhance the model towards enabling programmatic traversaland querying of cancer care guidelines. The Node classification was performedusing a Support Vector Machine (SVM) model, achieving a classification accuracyof 0.81 with 10-fold cross-validation.", "output": "Automated Knowledge Modeling for Cancer Clinical Practice Guidelines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study presents a thorough examination of various Generative PretrainedTransformer (GPT) methodologies in sentiment analysis, specifically in thecontext of Task 4 on the SemEval 2017 dataset. Three primary strategies areemployed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2)fine-tuning GPT models, and 3) an inventive approach to embeddingclassification. The research yields detailed comparative insights among thesestrategies and individual GPT models, revealing their unique strengths andpotential limitations. Additionally, the study compares these GPT-basedmethodologies with other contemporary, high-performing models previously usedwith the same dataset. The results illustrate the significant superiority ofthe GPT approaches in terms of predictive performance, more than 22% inF1-score compared to the state-of-the-art. Further, the paper addresses commonchallenges in sentiment analysis tasks, such as understanding context anddetecting sarcasm. It underscores the enhanced capabilities of the GPT modelsto effectively navigate these complexities. Collectively, these findingshighlight the promising potential of GPT models in sentiment analysis, settingthe stage for future research in this field. The code can be found at", "output": "SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent performance leap of Large Language Models (LLMs) opens up newopportunities across numerous industrial applications and domains. However,erroneous generations, such as false predictions, misinformation, andhallucination made by LLMs, have also raised severe concerns for thetrustworthiness of LLMs', especially in safety-, security- andreliability-sensitive scenarios, potentially hindering real-world adoptions.While uncertainty estimation has shown its potential for interpreting theprediction risks made by general machine learning (ML) models, little is knownabout whether and to what extent it can help explore an LLM's capabilities andcounteract its undesired behavior. To bridge the gap, in this paper, weinitiate an exploratory study on the risk assessment of LLMs from the lens ofuncertainty. In particular, we experiment with twelve uncertainty estimationmethods and four LLMs on four prominent natural language processing (NLP) tasksto investigate to what extent uncertainty estimation techniques could helpcharacterize the prediction risks of LLMs. Our findings validate theeffectiveness of uncertainty estimation for revealing LLMs'uncertain/non-factual predictions. In addition to general NLP tasks, weextensively conduct experiments with four LLMs for code generation on twodatasets. We find that uncertainty estimation can potentially uncover buggyprograms generated by LLMs. Insights from our study shed light on future designand development for reliable LLMs, facilitating further research towardenhancing the trustworthiness of LLMs.", "output": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face recognition from image sets acquired under unregulated and uncontrolledsettings, such as at large distances, low resolutions, varying viewpoints,illumination, pose, and atmospheric conditions, is challenging. Face featureaggregation, which involves aggregating a set of N feature representationspresent in a template into a single global representation, plays a pivotal rolein such recognition systems. Existing works in traditional face featureaggregation either utilize metadata or high-dimensional intermediate featurerepresentations to estimate feature quality for aggregation. However,generating high-quality metadata or style information is not feasible forextremely low-resolution faces captured in long-range and high altitudesettings. To overcome these limitations, we propose a feature distributionconditioning approach called CoNAN for template aggregation. Specifically, ourmethod aims to learn a context vector conditioned over the distributioninformation of the incoming feature set, which is utilized to weigh thefeatures based on their estimated informativeness. The proposed method producesstate-of-the-art results on long-range unconstrained face recognition datasetssuch as BTS, and DroneSURF, validating the advantages of such an aggregationstrategy.", "output": "CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How does the brain represent different modes of information? Can we design asystem that automatically understands what the user is thinking? Such questionscan be answered by studying brain recordings like functional magnetic resonanceimaging (fMRI). As a first step, the neuroscience community has contributedseveral large cognitive neuroscience datasets related to passivereading/listening/viewing of concept words, narratives, pictures and movies.Encoding and decoding models using these datasets have also been proposed inthe past two decades. These models serve as additional tools for basic researchin cognitive science and neuroscience. Encoding models aim at generating fMRIbrain representations given a stimulus automatically. They have severalpractical applications in evaluating and diagnosing neurological conditions andthus also help design therapies for brain damage. Decoding models solve theinverse problem of reconstructing the stimuli given the fMRI. They are usefulfor designing brain-machine or brain-computer interfaces. Inspired by theeffectiveness of deep learning models for natural language processing, computervision, and speech, recently several neural encoding and decoding models havebeen proposed. In this survey, we will first discuss popular representations oflanguage, vision and speech stimuli, and present a summary of neurosciencedatasets. Further, we will review popular deep learning based encoding anddecoding architectures and note their benefits and limitations. Finally, wewill conclude with a brief summary and discussion about future trends. Giventhe large amount of recently published work in the `computational cognitiveneuroscience' community, we believe that this survey nicely organizes theplethora of work and presents it as a coherent story.", "output": "Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study evaluates the GPT-4 Large Language Model's abductive reasoning incomplex fields like medical diagnostics, criminology, and cosmology. Using aninteractive interview format, the AI assistant demonstrated reliability ingenerating and selecting hypotheses. It inferred plausible medical diagnosesbased on patient data and provided potential causes and explanations incriminology and cosmology. The results highlight the potential of LLMs incomplex problem-solving and the need for further research to maximize theirpractical applications.", "output": "Abductive Reasoning with the GPT-4 Language Model: Case studies from criminal investigation, medical practice, scientific research."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This document provides a comprehensive guide to hyperparameter tuning usingspotPython for scikit-learn, PyTorch, and river. The first part introducesspotPython's surrogate model-based optimization process, while the second partfocuses on hyperparameter tuning. Several case studies are presented, includinghyperparameter tuning for sklearn models such as Support Vector Classification,Random Forests, Gradient Boosting (XGB), and K-nearest neighbors (KNN), as wellas a Hoeffding Adaptive Tree Regressor from river. The integration ofspotPython into the PyTorch and PyTorch Lightning training workflow is alsodiscussed. With a hands-on approach and step-by-step explanations, thiscookbook serves as a practical starting point for anyone interested inhyperparameter tuning with Python. Highlights include the interplay betweenTensorboard, PyTorch Lightning, spotPython, and river. This publication isunder development, with updates available on the corresponding webpage.", "output": "Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article explores the transformative impact of artificial intelligence(AI) on scientific research. It highlights ten ways in which AI isrevolutionizing the work of scientists, including powerful referencing tools,improved understanding of research problems, enhanced research questiongeneration, optimized research design, stub data generation, datatransformation, advanced data analysis, and AI-assisted reporting. While AIoffers numerous benefits, challenges such as bias, privacy concerns, and theneed for human-AI collaboration must be considered. The article emphasizes thatAI can augment human creativity in science but not replace it.", "output": "AI empowering research: 10 ways how science can benefit from AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Aphid infestations can cause extensive damage to wheat and sorghum fields andspread plant viruses, resulting in significant yield losses in agriculture. Toaddress this issue, farmers often rely on chemical pesticides, which areinefficiently applied over large areas of fields. As a result, a considerableamount of pesticide is wasted on areas without pests, while inadequate amountsare applied to areas with severe infestations. The paper focuses on the urgentneed for an intelligent autonomous system that can locate and sprayinfestations within complex crop canopies, reducing pesticide use andenvironmental impact. We have collected and labeled a large aphid image datasetin the field, and propose the use of real-time semantic segmentation models tosegment clusters of aphids. A multiscale dataset is generated to allow forlearning the clusters at different scales. We compare the segmentation speedsand accuracy of four state-of-the-art real-time semantic segmentation models onthe aphid cluster dataset, benchmarking them against nonreal-time models. Thestudy results show the effectiveness of a real-time solution, which can reduceinefficient pesticide use and increase crop yields, paving the way towards anautonomous pest detection system.", "output": "On the Real-Time Semantic Segmentation of Aphid Clusters in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the interdisciplinary field of artificial intelligence (AI) the problem ofclear terminology is especially momentous. This paper claims, that AI debatesare still characterised by a lack of critical distance to metaphors like'training', 'learning' or 'deciding'. As consequence, reflections regardingresponsibility or potential use-cases are greatly distorted. Yet, if relevantdecision-makers are convinced that AI can develop an 'understanding' orproperly 'interpret' issues, its regular use for sensitive tasks like decidingabout social benefits or judging court cases looms. The chapter argues itsclaim by analysing central notions of the AI debate and tries to contribute byproposing more fitting terminology and hereby enabling more fruitful debates.It is a conceptual work at the intersection of critical computer science andphilosophy of language.", "output": "The Language Labyrinth: Constructive Critique on the Terminology Used in the AI Discourse."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper argues that training AI systems with absolute constraints -- whichforbid certain acts irrespective of the amount of value they might produce --may make considerable progress on many AI safety problems in principle. First,it provides a guardrail for avoiding the very worst outcomes of misalignment.Second, it could prevent AIs from causing catastrophes for the sake of veryvaluable consequences, such as replacing humans with a much larger number ofbeings living at a higher welfare level. Third, it makes systems morecorrigible, allowing creators to make corrective interventions in them, such asaltering their objective functions or shutting them down. And fourth, it helpssystems explore their environment more safely by prohibiting them fromexploring especially dangerous acts. I offer a decision-theoretic formalizationof an absolute constraints, improving on existing models in the literature, anduse this model to prove some results about the training and behavior ofabsolutist AIs. I conclude by showing that, although absolutist AIs will notmaximize expected value, they will not be susceptible to behave irrationally,and they will not (contra coherence arguments) face environmental pressure tobecome expected-value maximizers.", "output": "Absolutist AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) offers a collaborative training framework, allowingmultiple clients to contribute to a shared model without compromising dataprivacy. Due to the heterogeneous nature of local datasets, updated clientmodels may overfit and diverge from one another, commonly known as the problemof client drift. In this paper, we propose FedBug (Federated Learning withBottom-Up Gradual Unfreezing), a novel FL framework designed to effectivelymitigate client drift. FedBug adaptively leverages the client model parameters,distributed by the server at each global round, as the reference points forcross-client alignment. Specifically, on the client side, FedBug begins byfreezing the entire model, then gradually unfreezes the layers, from the inputlayer to the output layer. This bottom-up approach allows models to train thenewly thawed layers to project data into a latent space, wherein the separatinghyperplanes remain consistent across all clients. We theoretically analyzeFedBug in a novel over-parameterization FL setup, revealing its superiorconvergence rate compared to FedAvg. Through comprehensive experiments,spanning various datasets, training conditions, and network architectures, wevalidate the efficacy of FedBug. Our contributions encompass a novel FLframework, theoretical analysis, and empirical validation, demonstrating thewide potential and applicability of FedBug.", "output": "FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vertical federated learning (VFL) enables multiple parties with disjointfeatures of a common user set to train a machine learning model without sharingtheir private data. Tree-based models have become prevalent in VFL due to theirinterpretability and efficiency. However, the vulnerability of tree-based VFLhas not been sufficiently investigated. In this study, we first introduce anovel label inference attack, ID2Graph, which utilizes the sets of record-IDsassigned to each node (i.e., instance space) to deduce private training labels.The ID2Graph attack generates a graph structure from training samples, extractscommunities from the graph, and clusters the local dataset using communityinformation. To counteract label leakage from the instance space, we propose aneffective defense mechanism, ID-LMID, which prevents label leakage by focusingon mutual information regularization. Comprehensive experiments conducted onvarious datasets reveal that the ID2Graph attack presents significant risks totree-based models such as Random Forest and XGBoost. Further evaluations onthese benchmarks demonstrate that ID-LMID effectively mitigates label leakagein such instances.", "output": "Eliminating Label Leakage in Tree-Based Vertical Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Part-prototype models are explainable-by-design image classifiers, and apromising alternative to black box AI. This paper explores the applicabilityand potential of interpretable machine learning, in particular PIP-Net, forautomated diagnosis support on real-world medical imaging data. PIP-Net learnshuman-understandable prototypical image parts and we evaluate its accuracy andinterpretability for fracture detection and skin cancer diagnosis. We find thatPIP-Net's decision making process is in line with medical classificationstandards, while only provided with image-level class labels. Because ofPIP-Net's unsupervised pretraining of prototypes, data quality problems such asundesired text in an X-ray or labelling errors can be easily identified.Additionally, we are the first to show that humans can manually correct thereasoning of PIP-Net by directly disabling undesired prototypes. We concludethat part-prototype models are promising for medical applications due to theirinterpretability and potential for advanced model debugging.", "output": "Interpreting and Correcting Medical Image Classification with PIP-Net."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-modal tasks involving vision and language in deep learning continue torise in popularity and are leading to the development of newer models that cangeneralize beyond the extent of their training data. The current models lacktemporal generalization which enables models to adapt to changes in futuredata. This paper discusses a viable approach to creating an advanced VisualQuestion Answering (VQA) model which can produce successful results on temporalgeneralization. We propose a new data set, GenVQA, utilizing images andcaptions from the VQAv2 and MS-COCO dataset to generate new images throughstable diffusion. This augmented dataset is then used to test a combination ofseven baseline and cutting edge VQA models. Performance evaluation focuses onquestions mirroring the original VQAv2 dataset, with the answers having beenadjusted to the new images. This paper's purpose is to investigate therobustness of several successful VQA models to assess their performance onfuture data distributions. Model architectures are analyzed to identify commonstylistic choices that improve generalization under temporal distributionshifts. This research highlights the importance of creating a large-scalefuture shifted dataset. This data can enhance the robustness of VQA models,allowing their future peers to have improved ability to adapt to temporaldistribution shifts.", "output": "Generative Visual Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The end-to-end learning ability of self-driving vehicles has achievedsignificant milestones over the last decade owing to rapid advances in deeplearning and computer vision algorithms. However, as autonomous drivingtechnology is a safety-critical application of artificial intelligence (AI),road accidents and established regulatory principles necessitate the need forthe explainability of intelligent action choices for self-driving vehicles. Tofacilitate interpretability of decision-making in autonomous driving, wepresent a Visual Question Answering (VQA) framework, which explains drivingactions with question-answering-based causal reasoning. To do so, we firstcollect driving videos in a simulation environment using reinforcement learning(RL) and extract consecutive frames from this log data uniformly for fiveselected action categories. Further, we manually annotate the extracted framesusing question-answer pairs as justifications for the actions chosen in eachscenario. Finally, we evaluate the correctness of the VQA-predicted answers foractions on unseen driving scenes. The empirical results suggest that the VQAmechanism can provide support to interpret real-time decisions of autonomousvehicles and help enhance overall driving safety.", "output": "Explaining Autonomous Driving Actions with Visual Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study proposes the GOOSE algorithm as a novel metaheuristic algorithmbased on the goose's behavior during rest and foraging. The goose stands on oneleg and keeps his balance to guard and protect other individuals in the flock.The GOOSE algorithm is benchmarked on 19 well-known benchmark test functions,and the results are verified by a comparative study with genetic algorithm(GA), particle swarm optimization (PSO), dragonfly algorithm (DA), and fitnessdependent optimizer (FDO). In addition, the proposed algorithm is tested on 10modern benchmark functions, and the gained results are compared with threerecent algorithms, such as the dragonfly algorithm, whale optimizationalgorithm (WOA), and salp swarm algorithm (SSA). Moreover, the GOOSE algorithmis tested on 5 classical benchmark functions, and the obtained results areevaluated with six algorithms, such as fitness dependent optimizer (FDO), FOXoptimizer, butterfly optimization algorithm (BOA), whale optimizationalgorithm, dragonfly algorithm, and chimp optimization algorithm (ChOA). Theachieved findings attest to the proposed algorithm's superior performancecompared to the other algorithms that were utilized in the current study. Thetechnique is then used to optimize Welded beam design and Economic LoadDispatch Problem, three renowned real-world engineering challenges, and thePathological IgG Fraction in the Nervous System. The outcomes of theengineering case studies illustrate how well the suggested approach canoptimize issues that arise in the real-world.", "output": "GOOSE Algorithm: A Powerful Optimization Tool for Real-World Engineering Challenges and Beyond."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Earth system forecasting has traditionally relied on complex physical modelsthat are computationally expensive and require significant domain expertise. Inthe past decade, the unprecedented increase in spatiotemporal Earth observationdata has enabled data-driven forecasting models using deep learning techniques.These models have shown promise for diverse Earth system forecasting tasks buteither struggle with handling uncertainty or neglect domain-specific priorknowledge, resulting in averaging possible futures to blurred forecasts orgenerating physically implausible predictions. To address these limitations, wepropose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1)We develop PreDiff, a conditional latent diffusion model capable ofprobabilistic forecasts. 2) We incorporate an explicit knowledge controlmechanism to align forecasts with domain-specific physical constraints. This isachieved by estimating the deviation from imposed constraints at each denoisingstep and adjusting the transition distribution accordingly. We conductempirical studies on two datasets: N-body MNIST, a synthetic dataset withchaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset.Specifically, we impose the law of conservation of energy in N-body MNIST andanticipated precipitation intensity in SEVIR. Experiments demonstrate theeffectiveness of PreDiff in handling uncertainty, incorporating domain-specificprior knowledge, and generating forecasts that exhibit high operationalutility.", "output": "PreDiff: Precipitation Nowcasting with Latent Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Active learning is a well-studied approach to learning formal specifications,such as automata. In this work, we extend active specification learning byproposing a novel framework that strategically requests a combination ofmembership labels and pair-wise preferences, a popular alternative tomembership labels. The combination of pair-wise preferences and membershiplabels allows for a more flexible approach to active specification learning,which previously relied on membership labels only. We instantiate our frameworkin two different domains, demonstrating the generality of our approach. Ourresults suggest that learning from both modalities allows us to robustly andconveniently identify specifications via membership and preferences.", "output": "Learning Formal Specifications from Membership and Preference Queries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In an effort to catalog insect biodiversity, we propose a new large datasetof hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record istaxonomically classified by an expert, and also has associated geneticinformation including raw nucleotide barcode sequences and assigned barcodeindex numbers, which are genetically-based proxies for species classification.This paper presents a curated million-image dataset, primarily to traincomputer-vision models capable of providing image-based taxonomic assessment,however, the dataset also presents compelling characteristics, the study ofwhich would be of interest to the broader machine learning community. Driven bythe biological nature inherent to the dataset, a characteristic long-tailedclass-imbalance distribution is exhibited. Furthermore, taxonomic labelling isa hierarchical classification scheme, presenting a highly fine-grainedclassification problem at lower levels. Beyond spurring interest inbiodiversity research within the machine learning community, progress oncreating an image-based taxonomic classifier will also further the ultimategoal of all BIOSCAN research: to lay the foundation for a comprehensive surveyof global biodiversity. This paper introduces the dataset and explores theclassification task through the implementation and analysis of a baselineclassifier.", "output": "A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The EU AI Act is the proposed EU legislation concerning AI systems. Thispaper identifies several categories of the AI Act. Based on thiscategorization, a questionnaire is developed that serves as a tool to offerinsights by creating quantitative data. Analysis of the data shows variouschallenges for organizations in different compliance categories. The influenceof organization characteristics, such as size and sector, is examined todetermine the impact on compliance. The paper will also share qualitative dataon which questions were prevalent among respondents, both on the content of theAI Act as the application. The paper concludes by stating that there is stillroom for improvement in terms of compliance with the AIA and refers to arelated project that examines a solution to help these organizations.", "output": "Complying with the EU AI Act."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A new computationally simple method of imposing hard convex constraints onthe neural network output values is proposed. The key idea behind the method isto map a vector of hidden parameters of the network to a point that isguaranteed to be inside the feasible set defined by a set of constraints. Themapping is implemented by the additional neural network layer with constraintsfor output. The proposed method is simply extended to the case when constraintsare imposed not only on the output vectors, but also on joint constraintsdepending on inputs. The projection approach to imposing constraints on outputscan simply be implemented in the framework of the proposed method. It is shownhow to incorporate different types of constraints into the proposed method,including linear and quadratic constraints, equality constraints, and dynamicconstraints, constraints in the form of boundaries. An important feature of themethod is its computational simplicity. Complexities of the forward pass of theproposed neural network layer by linear and quadratic constraints are O(n*m)and O(n^2*m), respectively, where n is the number of variables, m is the numberof constraints. Numerical experiments illustrate the method by solvingoptimization and classification problems. The code implementing the method ispublicly available.", "output": "A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data science is not a science. It is a research paradigm with an unfathomedscope, scale, complexity, and power for knowledge discovery that is nototherwise possible and can be beyond human reasoning. It is changing our worldpractically and profoundly already widely deployed in tens of thousands ofapplications in every discipline in an AI Arms Race that, due to itsinscrutability, can lead to unfathomed risks. This paper presents an axiologyof data science, its purpose, nature, importance, risks, and value for problemsolving, by exploring and evaluating its remarkable, definitive features. Asdata science is in its infancy, this initial, speculative axiology is intendedto aid in understanding and defining data science to recognize its potentialbenefits, risks, and open research challenges. AI based data science isinherently about uncertainty that may be more realistic than our preference forthe certainty of science. Data science will have impacts far beyond knowledgediscovery and will take us into new ways of understanding the world.", "output": "A data science axiology: the nature, value, and risks of data science."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the swift growth of patent applications each year, information andmultimedia retrieval approaches that facilitate patent exploration andretrieval are of utmost importance. Different types of visualizations (e.g.,graphs, technical drawings) and perspectives (e.g., side view, perspective) areused to visualize details of innovations in patents. The classification ofthese images enables a more efficient search and allows for further analysis.So far, datasets for image type classification miss some importantvisualization types for patents. Furthermore, related work does not make use ofrecent deep learning approaches including transformers. In this paper, we adoptstate-of-the-art deep learning methods for the classification of visualizationtypes and perspectives in patent images. We extend the CLEF-IP dataset forimage type classification in patents to ten classes and provide manual groundtruth annotations. In addition, we derive a set of hierarchical classes from adataset that provides weakly-labeled data for image perspectives. Experimentalresults have demonstrated the feasibility of the proposed approaches. Sourcecode, models, and dataset will be made publicly available.", "output": "Classification of Visualization Types and Perspectives in Patents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the breadth and depth of language model applications continue to expandrapidly, it is increasingly important to build efficient frameworks formeasuring and mitigating the learned or inherited social biases of thesemodels. In this paper, we present our work on evaluating instruction fine-tunedlanguage models' ability to identify bias through zero-shot prompting,including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instructionfine-tuned versions, Alpaca 7B performs best on the bias identification taskwith an accuracy of 56.7%. We also demonstrate that scaling up LLM size anddata diversity could lead to further performance gain. This is awork-in-progress presenting the first component of our bias mitigationframework. We will keep updating this work as we get more results.", "output": "Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) have shown unprecedented success in objectdetection tasks. However, it was also discovered that DNNs are vulnerable tomultiple kinds of attacks, including Backdoor Attacks. Through the attack, theattacker manages to embed a hidden backdoor into the DNN such that the modelbehaves normally on benign data samples, but makes attacker-specified judgmentsgiven the occurrence of a predefined trigger. Although numerous backdoorattacks have been experimented on image classification, backdoor attacks onobject detection tasks have not been properly investigated and explored. Asobject detection has been adopted as an important module in multiplesecurity-sensitive applications such as autonomous driving, backdoor attacks onobject detection could pose even more severe threats. Inspired by the inherentproperty of deep learning-based object detectors, we propose a simple yeteffective backdoor attack method against object detection without modifying theground truth annotations, specifically focusing on the object disappearanceattack and object generation attack. Extensive experiments and ablation studiesprove the effectiveness of our attack on two benchmark object detectiondatasets, PASCAL VOC07+12 and MSCOCO, on which we achieve an attack successrate of more than 92% with a poison rate of only 5%.", "output": "Backdoor Attack against Object Detection with Clean Annotation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We demonstrate how images and sounds can be used for indirect prompt andinstruction injection in multi-modal LLMs. An attacker generates an adversarialperturbation corresponding to the prompt and blends it into an image or audiorecording. When the user asks the (unmodified, benign) model about theperturbed image or audio, the perturbation steers the model to output theattacker-chosen text and/or make the subsequent dialog follow the attacker'sinstruction. We illustrate this attack with several proof-of-concept examplestargeting LLaVa and PandaGPT.", "output": "(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Canonical models of Markov decision processes (MDPs) usually considergeometric discounting based on a constant discount factor. While this standardmodeling approach has led to many elegant results, some recent studies indicatethe necessity of modeling time-varying discounting in certain applications.This paper studies a model of infinite-horizon MDPs with time-varying discountfactors. We take a game-theoretic perspective -- whereby each time step istreated as an independent decision maker with their own (fixed) discount factor-- and we study the subgame perfect equilibrium (SPE) of the resulting game aswell as the related algorithmic problems. We present a constructive proof ofthe existence of an SPE and demonstrate the EXPTIME-hardness of computing anSPE. We also turn to the approximate notion of $epsilon$-SPE and show that an$epsilon$-SPE exists under milder assumptions. An algorithm is presented tocompute an $epsilon$-SPE, of which an upper bound of the time complexity, as afunction of the convergence property of the time-varying discount factor, isprovided.", "output": "Markov Decision Processes with Time-Varying Geometric Discounting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "General large language models (LLMs) such as ChatGPT have shown remarkablesuccess. However, such LLMs have not been widely adopted for medical purposes,due to poor accuracy and inability to provide medical advice. We proposeIvyGPT, an LLM based on LLaMA that is trained and fine-tuned with high-qualitymedical question-answer (QA) instances and Reinforcement Learning from HumanFeedback (RLHF). After supervised fine-tuning, IvyGPT has good multi-turnconversation capabilities, but it cannot perform like a doctor in otheraspects, such as comprehensive diagnosis. Through RLHF, IvyGPT can outputricher diagnosis and treatment answers that are closer to human. In thetraining, we used QLoRA to train 33 billion parameters on a small number ofNVIDIA A100 (80GB) GPUs. Experimental results show that IvyGPT has outperformedother medical GPT models.", "output": "IvyGPT: InteractiVe Chinese pathwaY language model in medical domain."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With rapid development and deployment of generative language models in globalsettings, there is an urgent need to also scale our measurements of harm, notjust in the number and types of harms covered, but also how well they accountfor local cultural contexts, including marginalized identities and the socialbiases experienced by them. Current evaluation paradigms are limited in theirabilities to address this, as they are not representative of diverse, locallysituated but global, socio-cultural perspectives. It is imperative that ourevaluation resources are enhanced and calibrated by including people andexperiences from different cultures and societies worldwide, in order toprevent gross underestimations or skews in measurements of harm. In this work,we demonstrate a socio-culturally aware expansion of evaluation resources inthe Indian societal context, specifically for the harm of stereotyping. Wedevise a community engaged effort to build a resource which containsstereotypes for axes of disparity that are uniquely present in India. Theresultant resource increases the number of stereotypes known for and in theIndian context by over 1000 stereotypes across many unique identities. We alsodemonstrate the utility and effectiveness of such expanded resources forevaluations of language models. CONTENT WARNING: This paper contains examplesof stereotypes that may be offensive.", "output": "Building Socio-culturally Inclusive Stereotype Resources with Community Engagement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Outlier detection (OD) finds many applications with a rich literature ofnumerous techniques. Deep neural network based OD (DOD) has seen a recent surgeof attention thanks to the many advances in deep learning. In this paper, weconsider a critical-yet-understudied challenge with unsupervised DOD, that is,effective hyperparameter (HP) tuning/model selection. While several prior workreport the sensitivity of OD models to HPs, it becomes ever so critical for themodern DOD models that exhibit a long list of HPs. We introduce HYPER fortuning DOD models, tackling two fundamental challenges: (1) validation withoutsupervision (due to lack of labeled anomalies), and (2) efficient search of theHP/model space (due to exponential growth in the number of HPs). A key idea isto design and train a novel hypernetwork (HN) that maps HPs onto optimalweights of the main DOD model. In turn, HYPER capitalizes on a single HN thatcan dynamically generate weights for many DOD models (corresponding to varyingHPs), which offers significant speed-up. In addition, it employs meta-learningon historical OD tasks with labels to train a proxy validation function,likewise trained with our proposed HN efficiently. Extensive experiments on 35OD tasks show that HYPER achieves high performance against 8 baselines withsignificant efficiency gains.", "output": "Fast Unsupervised Deep Outlier Model Selection with Hypernetworks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conversational recommender systems (CRS) aim to timely trace the dynamicinterests of users through dialogues and generate relevant responses for itemrecommendations. Recently, various external knowledge bases (especiallyknowledge graphs) are incorporated into CRS to enhance the understanding ofconversation contexts. However, recent reasoning-based models heavily rely onsimplified structures such as linear structures or fixed-hierarchicalstructures for causality reasoning, hence they cannot fully figure outsophisticated relationships among utterances with external knowledge. Toaddress this, we propose a novel Tree structure Reasoning schEmA named TREA.TREA constructs a multi-hierarchical scalable tree as the reasoning structureto clarify the causal relationships between mentioned entities, and fullyutilizes historical conversations to generate more reasonable and suitableresponses for recommended results. Extensive experiments on two public CRSdatasets have demonstrated the effectiveness of our approach.", "output": "TREA: Tree-Structure Reasoning Schema for Conversational Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training and deploying the large language models requires a large mount ofcomputational resource because the language models contain billions ofparameters and the text has thousands of tokens. Another problem is that thelarge language models are static. They are fixed after the training process. Totackle these issues, in this paper, we propose to train and deploy the dynamiclarge language model on blockchains, which have high computation performanceand are distributed across a network of computers. A blockchain is a secure,decentralized, and transparent system that allows for the creation of atamper-proof ledger for transactions without the need for intermediaries. Thedynamic large language models can continuously learn from the user input afterthe training process. Our method provides a new way to develop the largelanguage models and also sheds a light on the next generation artificialintelligence systems.", "output": "Dynamic Large Language Models on Blockchains."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Key Information Extraction (KIE) is a challenging multimodal task that aimsto extract structured value semantic entities from visually rich documents.Although significant progress has been made, there are still two majorchallenges that need to be addressed. Firstly, the layout of existing datasetsis relatively fixed and limited in the number of semantic entity categories,creating a significant gap between these datasets and the complex real-worldscenarios. Secondly, existing methods follow a two-stage pipeline strategy,which may lead to the error propagation problem. Additionally, they aredifficult to apply in situations where unseen semantic entity categoriesemerge. To address the first challenge, we propose a new large-scalehuman-annotated dataset named Complex Layout form for key informationEXtraction (CLEX), which consists of 5,860 images with 1,162 semantic entitycategories. To solve the second challenge, we introduce Parallel Pointer-basedNetwork (PPN), an end-to-end model that can be applied in zero-shot andfew-shot scenarios. PPN leverages the implicit clues between semantic entitiesto assist extracting, and its parallel extraction mechanism allows it toextract multiple results simultaneously and efficiently. Experiments on theCLEX dataset demonstrate that PPN outperforms existing state-of-the-art methodswhile also offering a much faster inference speed.", "output": "PPN: Parallel Pointer-based Network for Key Information Extraction with Complex Layouts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mixed-Precision Quantization~(MQ) can achieve a competitiveaccuracy-complexity trade-off for models. Conventional training-based searchmethods require time-consuming candidate training to search optimized per-layerbit-width configurations in MQ. Recently, some training-free approaches havepresented various MQ proxies and significantly improve search efficiency.However, the correlation between these proxies and quantization accuracy ispoorly understood. To address the gap, we first build the MQ-Bench-101, whichinvolves different bit configurations and quantization results. Then, weobserve that the existing training-free proxies perform weak correlations onthe MQ-Bench-101. To efficiently seek superior proxies, we develop an automaticsearch of proxies framework for MQ via evolving algorithms. In particular, wedevise an elaborate search space involving the existing proxies and perform anevolution search to discover the best correlated MQ proxy. We proposed adiversity-prompting selection strategy and compatibility screening protocol toavoid premature convergence and improve search efficiency. In this way, ourEvolving proxies for Mixed-precision Quantization~(EMQ) framework allows theauto-generation of proxies without heavy tuning and expert knowledge. Extensiveexperiments on ImageNet with various ResNet and MobileNet families demonstratethat our EMQ obtains superior performance than state-of-the-art mixed-precisionmethods at a significantly reduced cost. The code will be released.", "output": "EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Air traffic control (ATC) is a safety-critical service system that demandsconstant attention from ground air traffic controllers (ATCos) to maintaindaily aviation operations. The workload of the ATCos can have negative effectson operational safety and airspace usage. To avoid overloading and ensure anacceptable workload level for the ATCos, it is important to predict the ATCos'workload accurately for mitigation actions. In this paper, we first perform areview of research on ATCo workload, mostly from the air traffic perspective.Then, we briefly introduce the setup of the human-in-the-loop (HITL)simulations with retired ATCos, where the air traffic data and workload labelsare obtained. The simulations are conducted under three Phoenix approachscenarios while the human ATCos are requested to self-evaluate their workloadratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next,we propose a graph-based deep-learning framework with conformal prediction toidentify the ATCo workload levels. The number of aircraft under thecontroller's control varies both spatially and temporally, resulting indynamically evolving graphs. The experiment results suggest that (a) besidesthe traffic density feature, the traffic conflict feature contributes to theworkload prediction capabilities (i.e., minimum horizontal/vertical separationdistance); (b) directly learning from the spatiotemporal graph layout ofairspace with graph neural network can achieve higher prediction accuracy,compare to hand-crafted traffic complexity features; (c) conformal predictionis a valuable tool to further boost model prediction accuracy, resulting arange of predicted workload labels. The code used is available athref{", "output": "Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present FACADE, a novel probabilistic and geometric framework designed forunsupervised mechanistic anomaly detection in deep neural networks. Its primarygoal is advancing the understanding and mitigation of adversarial attacks.FACADE aims to generate probabilistic distributions over circuits, whichprovide critical insights to their contribution to changes in the manifoldproperties of pseudo-classes, or high-dimensional modes in activation space,yielding a powerful tool for uncovering and combating adversarial attacks. Ourapproach seeks to improve model robustness, enhance scalable model oversight,and demonstrates promising applications in real-world deployment settings.", "output": "FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the capabilities of large machine learning models continue to grow, and asthe autonomy afforded to such models continues to expand, the spectre of a newadversary looms: the models themselves. The threat that a model might behave ina seemingly reasonable manner, while secretly and subtly modifying its behaviorfor ulterior reasons is often referred to as deceptive alignment in the AISafety &amp; Alignment communities. Consequently, we call this new directionDeceptive Alignment Monitoring. In this work, we identify emerging directionsin diverse machine learning subfields that we believe will become increasinglyimportant and intertwined in the near future for deceptive alignmentmonitoring, and we argue that advances in these fields present both long-termchallenges and new research opportunities. We conclude by advocating forgreater involvement by the adversarial machine learning community in theseemerging directions.", "output": "Deceptive Alignment Monitoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language models can be prompted to reason through problems in a manner thatsignificantly improves performance. However, textit{why} such promptingimproves performance is unclear. Recent work showed that using logicallytextit{invalid} Chain-of-Thought (CoT) prompting improves performance almostas much as logically textit{valid} CoT prompting, and that editing CoT promptsto replace problem-specific information with abstract information orout-of-distribution information typically doesn't harm performance. Criticshave responded that these findings are based on too few and too easy tasks todraw meaningful conclusions. To resolve this dispute, we test whether logicallyinvalid CoT prompts offer the same level of performance gains as logicallyvalid prompts on the hardest tasks in the BIG-Bench benchmark, termed BIG-BenchHard (BBH). We find that the logically textit{invalid} reasoning prompts doindeed achieve similar performance gains on BBH tasks as logically validreasoning prompts. We also discover that some CoT prompts used by previousworks contain logical errors. This suggests that covariates beyond logicallyvalid reasoning are responsible for performance improvements.", "output": "Invalid Logic, Equivalent Gains: The Bizarreness of Reasoning in Language Model Prompting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to complexity and dynamics of construction work, resource, and cashflows, poor management of them usually leads to time and cost overruns,bankruptcy, even project failure. Existing approaches in construction failed toachieve optimal control of resource flow in a dynamic environment withuncertainty. Therefore, this paper introducess a model and method to adaptivecontrol the resource flows to optimize the work and cash flows of constructionprojects. First, a mathematical model based on a partially observable Markovdecision process is established to formulate the complex interactions ofconstruction work, resource, and cash flows as well as uncertainty andvariability of diverse influence factors. Meanwhile, to efficiently find theoptimal solutions, a deep reinforcement learning (DRL) based method isintroduced to realize the continuous adaptive optimal control of labor andmaterial flows, thereby optimizing the work and cash flows. To assist thetraining process of DRL, a simulator based on discrete event simulation is alsodeveloped to mimic the dynamic features and external environments of a project.Experiments in simulated scenarios illustrate that our method outperforms thevanilla empirical method and genetic algorithm, possesses remarkable capabilityin diverse projects and external environments, and a hybrid agent of DRL andempirical method leads to the best result. This paper contributes to adaptivecontrol and optimization of coupled work, resource, and cash flows, and mayserve as a step stone for adopting DRL technology in construction projectmanagement.", "output": "Adaptive Control of Resource Flow to Optimize Construction Work and Cash Flow via Online Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a distributed machine learning technique, federated learning (FL) requiresclients to collaboratively train a shared model with an edge server withoutleaking their local data. However, the heterogeneous data distribution amongclients often leads to a decrease in model performance. To tackle this issue,this paper introduces a prototype-based regularization strategy to address theheterogeneity in the data distribution. Specifically, the regularizationprocess involves the server aggregating local prototypes from distributedclients to generate a global prototype, which is then sent back to theindividual clients to guide their local training. The experimental results onMNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3%and 8.9% in average test accuracy, respectively, compared to the most popularbaseline FedAvg. Furthermore, our approach has a fast convergence rate inheterogeneous settings.", "output": "Boosting Federated Learning Convergence with Prototype Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Traditional computer vision models often require extensive manual effort fordata acquisition and validation, particularly when detecting subtle behavioralnuances or events. The difficulty in distinguishing routine behaviors frompotential risks in real-world applications, like differentiating routineshopping from potential shoplifting, further complicates the process.We present Ethosight, a novel zero-shot computer vision algorithm. Ethosighteradicates the need for pre-existing symbolic knowledge, initiating from aclean slate based on user requirements and semantic knowledge of interest.Using localized label affinity calculations and a reasoning-guided iterativelearning loop, Ethosight infers scene details and iteratively refines the labelset. Reasoning mechanisms can be derived from large language models like GPT4,symbolic reasoners like OpenNARS, or hybrid systems.Ethosight further capitalizes on the capabilities of a pre-trainedmulti-modal model, ImageBind, generating accurate semantic knowledge of imageswithin a few cycles. It successfully captures both explicit and nuancedelements efficiently. We also introduce the implementation of Korzybski's\"time-binding\" concept in machines, which allows for generational learning andknowledge sharing across deployments.Our evaluations demonstrate Ethosight's efficacy across 40 complex use cases.It has exhibited an exceptional ability to discern new areas of interest,consistently generating high-affinity scores within the top five labels from aset of a thousand. Tests conducted across diverse environments attest toEthosight's robust performance. Detailed results and case studies within themain body of this paper and an appendix underscore a promising trajectorytowards enhancing the adaptability and resilience of computer vision models indetecting and extracting subtle and nuanced behaviors.", "output": "Ethosight: A Joint-Embedding Based System for Nuanced Perception Using Contextual Label Affinity Metric and Reasoning Based Iterative Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy systems, climate change, and public health are among the primaryreasons for moving toward electrification in transportation. Transportationelectrification is being promoted worldwide to reduce emissions. As a result,many automakers will soon start making only battery electric vehicles (BEVs).BEV adoption rates are rising in California, mainly due to climate change andair pollution concerns. While great for climate and pollution goals, improperlymanaged BEV charging can lead to insufficient charging infrastructure and poweroutages. This study develops a novel Micro Clustering Deep Neural Network(MCDNN), an artificial neural network algorithm that is highly effective atlearning BEVs trip and charging data to forecast BEV charging events,information that is essential for electricity load aggregators and utilitymanagers to provide charging stations and electricity capacity effectively. TheMCDNN is configured using a robust dataset of trips and charges that occurredin California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models for atotal of 1570167 vehicle miles traveled. The numerical findings revealed thatthe proposed MCDNN is more effective than benchmark approaches in this field,such as support vector machine, k nearest neighbors, decision tree, and otherneural network-based models in predicting the charging events.", "output": "Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in Deep Neural Networks (DNNs) and sensor technologies areenabling autonomous driving systems (ADSs) with an ever-increasing level ofautonomy. However, assessing their dependability remains a critical concern.State-of-the-art ADS testing approaches modify the controllable attributes of asimulated driving environment until the ADS misbehaves. Such approaches havetwo main drawbacks: (1) modifications to the simulated environment might not beeasily transferable to the in-field test setting (e.g., changing the roadshape); (2) environment instances in which the ADS is successful are discarded,despite the possibility that they could contain hidden driving conditions inwhich the ADS may misbehave.In this paper, we present GenBo (GENerator of BOundary state pairs), a noveltest generator for ADS testing. GenBo mutates the driving conditions of the egovehicle (position, velocity and orientation), collected in a failure-freeenvironment instance, and efficiently generates challenging driving conditionsat the behavior boundary (i.e., where the model starts to misbehave) in thesame environment. We use such boundary conditions to augment the initialtraining dataset and retrain the DNN model under test. Our evaluation resultsshow that the retrained model has up to 16 higher success rate on a separateset of evaluation tracks with respect to the original DNN model.", "output": "Boundary State Generation for Testing and Improvement of Autonomous Driving Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A key challenge in Bayesian decentralized data fusion is the `rumorpropagation' or `double counting' phenomenon, where previously sent datacirculates back to its sender. It is often addressed by approximate methodslike covariance intersection (CI) which takes a weighted average of theestimates to compute the bound. The problem is that this bound is not tight,i.e. the estimate is often over-conservative. In this paper, we show that byexploiting the probabilistic independence structure in multi-agentdecentralized fusion problems a tighter bound can be found using (i) anexpansion to the CI algorithm that uses multiple (non-monolithic) weightingfactors instead of one (monolithic) factor in the original CI and (ii) ageneral optimization scheme that is able to compute optimal bounds and fullyexploit an arbitrary dependency structure. We compare our methods and show thaton a simple problem, they converge to the same solution. We then test our newnon-monolithic CI algorithm on a large-scale target tracking simulation andshow that it achieves a tighter bound and a more accurate estimate compared tothe original monolithic CI.", "output": "Exploiting Structure for Optimal Multi-Agent Bayesian Decentralized Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial Intelligence (AI)'s pervasive presence and variety necessitatediversity and inclusivity (D&amp;I) principles in its design for fairness, trust,and transparency. Yet, these considerations are often overlooked, leading toissues of bias, discrimination, and perceived untrustworthiness. In response,we conducted a Systematic Review to unearth challenges and solutions relatingto D&amp;I in AI. Our rigorous search yielded 48 research articles publishedbetween 2017 and 2022. Open coding of these papers revealed 55 uniquechallenges and 33 solutions for D&amp;I in AI, as well as 24 unique challenges and23 solutions for enhancing such practices using AI. This study, by offering adeeper understanding of these issues, will enlighten researchers andpractitioners seeking to integrate these principles into future AI systems.", "output": "Challenges and Solutions in AI for All."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) has drawn increasing attention owing to its potentialuse in large-scale industrial applications. Existing federated learning worksmainly focus on model homogeneous settings. However, practical federatedlearning typically faces the heterogeneity of data distributions, modelarchitectures, network environments, and hardware devices among participantclients. Heterogeneous Federated Learning (HFL) is much more challenging, andcorresponding solutions are diverse and complex. Therefore, a systematic surveyon this topic about the research challenges and state-of-the-art is essential.In this survey, we firstly summarize the various research challenges in HFLfrom five aspects: statistical heterogeneity, model heterogeneity,communication heterogeneity, device heterogeneity, and additional challenges.In addition, recent advances in HFL are reviewed and a new taxonomy of existingHFL methods is proposed with an in-depth analysis of their pros and cons. Weclassify existing methods from three different levels according to the HFLprocedure: data-level, model-level, and server-level. Finally, several criticaland promising future research directions in HFL are discussed, which mayfacilitate further developments in this field. A periodically updatedcollection on HFL is available at ", "output": "Heterogeneous Federated Learning: State-of-the-art and Research Challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, online reviews play a vital role for promoting any kind ofproduct or services. Businesses may embed fake reviews in order to attractcustomers to purchase their products. They may even highlight the benefits oftheir own product or criticize the competition's product. Marketers,advertisers, and other online business users have incentive to create fakepositive reviews for products which they want to promote or give fake negativereviews for products which they really don't like. So now-a-days writing adeceptive review is inevitable thing for promoting their own business ordegrading competitor's reputation. Thus, identifying deceptive reviews is anintense and on-going research area. This research paper proposes machinelearning model approach to identify deceptive reviews. The paper investigatesthe performance of the several experiments done on a Deceptive Opinion SpamCorpus dataset of restaurants reviews. We developed a n-gram model and maxfeatures to identify deceptive contents with a particular focus on fakereviews. Further, we conduct a benchmark study to investigate the performanceof two different features extraction techniques and apply five machine learningclassification techniques. The experimental results show that passiveaggressive classifier outperforms other algorithms, and it reaches the highestaccuracy not only in text classification but also to fake reviews. We alsostudy the data augmentation and implement different deep learning techniques.", "output": "Detecting deceptive reviews using text classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The practice of code reuse is crucial in software development for a fasterand more efficient development lifecycle. In reality, however, code reusepractices lack proper control, resulting in issues such as vulnerabilitypropagation and intellectual property infringements. Assembly clone search, acritical shift-right defence mechanism, has been effective in identifyingvulnerable code resulting from reuse in released executables. Recent studies onassembly clone search demonstrate a trend towards using machine learning-basedmethods to match assembly code variants produced by different toolchains.However, these methods are limited to what they learn from a small number oftoolchain variants used in training, rendering them inapplicable to unseenarchitectures and their corresponding compilation toolchain variants.This paper presents the first study on the problem of assembly clone searchwith unseen architectures and libraries. We propose incorporating human commonknowledge through large-scale pre-trained natural language models, in the formof transfer learning, into current learning-based approaches for assembly clonesearch. Transfer learning can aid in addressing the limitations of the existingapproaches, as it can bring in broader knowledge from human experts in assemblycode. We further address the sequence limit issue by proposing a reinforcementlearning agent to remove unnecessary and redundant tokens. Coupled with a newVariational Information Bottleneck learning strategy, the proposed systemminimizes the reliance on potential indicators of architectures andoptimization settings, for a better generalization of unseen architectures. Wesimulate the unseen architecture clone search scenarios and the experimentalresults show the effectiveness of the proposed approach against thestate-of-the-art solutions.", "output": "Pluvio: Assembly Clone Search for Out-of-domain Architectures and Libraries through Transfer Learning and Conditional Variational Information Bottleneck."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in large language models (LLMs) have demonstrated notableprogress on many mathematical benchmarks. However, most of these benchmarksonly feature problems grounded in junior and senior high school subjects,contain only multiple-choice questions, and are confined to a limited scope ofelementary arithmetic operations. To address these issues, this paperintroduces an expansive benchmark suite SciBench that aims to systematicallyexamine the reasoning capabilities required for complex scientific problemsolving. SciBench contains two carefully curated datasets: an open setfeaturing a range of collegiate-level scientific problems drawn frommathematics, chemistry, and physics textbooks, and a closed set comprisingproblems from undergraduate-level exams in computer science and mathematics.Based on the two datasets, we conduct an in-depth benchmark study of tworepresentative LLMs with various prompting strategies. The results reveal thatcurrent LLMs fall short of delivering satisfactory performance, with an overallscore of merely 35.80%. Furthermore, through a detailed user study, wecategorize the errors made by LLMs into ten problem-solving abilities. Ouranalysis indicates that no single prompting strategy significantly outperformsothers and some strategies that demonstrate improvements in certainproblem-solving skills result in declines in other skills. We envision thatSciBench will catalyze further developments in the reasoning abilities of LLMs,thereby ultimately contributing to scientific research and discovery.", "output": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge graphs have proven to be effective for modeling entities and theirrelationships through the use of ontologies. The recent emergence in interestfor using knowledge graphs as a form of information modeling has led to theirincreased adoption in recommender systems. By incorporating users and itemsinto the knowledge graph, these systems can better capture the implicitconnections between them and provide more accurate recommendations. In thispaper, we investigate and propose the construction of a personalizedrecommender system via knowledge graphs embedding applied to the vehiclepurchase/sale domain. The results of our experimentation demonstrate theefficacy of the proposed method in providing relevant recommendations that areconsistent with individual users.", "output": "A Personalized Recommender System Based-on Knowledge Graph Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We develop an approach called bounded combinatorial reconfiguration forsolving combinatorial reconfiguration problems based on Answer Set Programming(ASP). The general task is to study the solution spaces of source combinatorialproblems and to decide whether or not there are sequences of feasible solutionsthat have special properties. The resulting recongo solver covers all metricsof the solver track in the most recent international competition oncombinatorial reconfiguration (CoRe Challenge 2022). recongo ranked first inthe shortest metric of the single-engine solvers track. In this paper, wepresent the design and implementation of bounded combinatorial reconfiguration,and present an ASP encoding of the independent set reconfiguration problem thatis one of the most studied combinatorial reconfiguration problems. Finally, wepresent empirical analysis considering all instances of CoRe Challenge 2022.", "output": "Bounded Combinatorial Reconfiguration with Answer Set Programming."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new framework called KorraAI for conceiving and buildingembodied conversational agents (ECAs). Our framework models ECAs' behaviorconsidering contextual information, for example, about environment andinteraction time, and uncertain information provided by the human interactionpartner. Moreover, agents built with KorraAI can show proactive behavior, asthey can initiate interactions with human partners. For these purposes, KorraAIexploits probabilistic programming. Probabilistic models in KorraAI are used tomodel its behavior and interactions with the user. They enable adaptation tothe user's preferences and a certain degree of indeterminism in the ECAs toachieve more natural behavior. Human-like internal states, such as moods,preferences, and emotions (e.g., surprise), can be modeled in KorraAI withdistributions and Bayesian networks. These models can evolve over time, evenwithout interaction with the user. ECA models are implemented as plugins andshare a common interface. This enables ECA designers to focus more on thecharacter they are modeling and less on the technical details, as well as tostore and exchange ECA models. Several applications of KorraAI ECAs arepossible, such as virtual sales agents, customer service agents, virtualcompanions, entertainers, or tutors.", "output": "Towards an architectural framework for intelligent virtual agents using probabilistic programming."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing customization methods require access to multiple reference examplesto align pre-trained diffusion probabilistic models (DPMs) with user-providedconcepts. This paper aims to address the challenge of DPM customization whenthe only available supervision is a differentiable metric defined on thegenerated contents. Since the sampling procedure of DPMs involves recursivecalls to the denoising UNet, na\"ive gradient backpropagation requires storingthe intermediate states of all iterations, resulting in extremely high memoryconsumption. To overcome this issue, we propose a novel method AdjointDPM,which first generates new samples from diffusion models by solving thecorresponding probability-flow ODEs. It then uses the adjoint sensitivitymethod to backpropagate the gradients of the loss to the models' parameters(including conditioning signals, network weights, and initial noises) bysolving another augmented ODE. To reduce numerical errors in both the forwardgeneration and gradient backpropagation processes, we further reparameterizethe probability-flow ODE and augmented ODE as simple non-stiff ODEs usingexponential integration. Finally, we demonstrate the effectiveness ofAdjointDPM on three interesting tasks: converting visual effects intoidentification text embeddings, finetuning DPMs for specific types ofstylization, and optimizing initial noise to generate adversarial samples forsecurity auditing.", "output": "AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised monocular depth estimation (SS-MDE) has the potential toscale to vast quantities of data. Unfortunately, existing approaches limitthemselves to the automotive domain, resulting in models incapable ofgeneralizing to complex environments such as natural or indoor settings.To address this, we propose a large-scale SlowTV dataset curated fromYouTube, containing an order of magnitude more data than existing automotivedatasets. SlowTV contains 1.7M images from a rich diversity of environments,such as worldwide seasonal hiking, scenic driving and scuba diving. Using thisdataset, we train an SS-MDE model that provides zero-shot generalization to alarge collection of indoor/outdoor datasets. The resulting model outperformsall existing SSL approaches and closes the gap on supervised SoTA, despiteusing a more efficient architecture.We additionally introduce a collection of best-practices to further maximizeperformance and zero-shot generalization. This includes 1) aspect ratioaugmentation, 2) camera intrinsic estimation, 3) support frame randomizationand 4) flexible motion estimation. Code is available at", "output": "Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of group interactions in urban driving.State-of-the-art behavior planners for self-driving cars mostly consider eachsingle agent-to-agent interaction separately in a cost function in order tofind an optimal behavior for the ego agent, such as not colliding with any ofthe other agents. In this paper, we develop risk shadowing, a situationunderstanding method that allows us to go beyond single interactions byanalyzing group interactions between three agents. Concretely, the presentedmethod can find out which first other agent does not need to be considered inthe behavior planner of an ego agent, because this first other agent cannotreach the ego agent due to a second other agent obstructing its way. Inexperiments, we show that using risk shadowing as an upstream filter module fora behavior planner allows to plan more decisive and comfortable drivingstrategies than state of the art, given that safety is ensured in these cases.The usability of the approach is demonstrated for different intersectionscenarios and longitudinal driving.", "output": "Introducing Risk Shadowing For Decisive and Comfortable Behavior Planning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have exhibited impressive capabilities incomprehending complex instructions. However, their blind adherence to providedinstructions has led to concerns regarding risks of malicious use. Existingdefence mechanisms, such as model fine-tuning or output censorship using LLMs,have proven to be fallible, as LLMs can still generate problematic responses.Commonly employed censorship approaches treat the issue as a machine learningproblem and rely on another LM to detect undesirable content in LLM outputs. Inthis paper, we present the theoretical limitations of such semantic censorshipapproaches. Specifically, we demonstrate that semantic censorship can beperceived as an undecidable problem, highlighting the inherent challenges incensorship that arise due to LLMs' programmatic and instruction-followingcapabilities. Furthermore, we argue that the challenges extend beyond semanticcensorship, as knowledgeable attackers can reconstruct impermissible outputsfrom a collection of permissible ones. As a result, we propose that the problemof censorship needs to be reevaluated; it should be treated as a securityproblem which warrants the adaptation of security-based approaches to mitigatepotential risks.", "output": "LLM Censorship: A Machine Learning Challenge or a Computer Security Problem?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients)to train machine learning models collaboratively without revealing privatedata. Since the FL server can only engage a limited number of clients in eachtraining round, FL client selection has become an important research problem.Existing approaches generally focus on either enhancing FL model performance orenhancing the fair treatment of FL clients. The problem of balancingperformance and fairness considerations when selecting FL clients remains open.To address this problem, we propose the Fairness-aware Federated ClientSelection (FairFedCS) approach. Based on Lyapunov optimization, it dynamicallyadjusts FL clients' selection probabilities by jointly considering theirreputations, times of participation in FL tasks and contributions to theresulting model performance. By not using threshold-based reputation filtering,it provides FL clients with opportunities to redeem their reputations after aperceived poor performance, thereby further enhancing fair client treatment.Extensive experiments based on real-world multimedia datasets show thatFairFedCS achieves 19.6% higher fairness and 0.73% higher test accuracy onaverage than the best-performing state-of-the-art approach.", "output": "Fairness-Aware Client Selection for Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Artificial Intelligence (AI), and in particular generative models, aretransformative tools for knowledge work. They problematise notions ofcreativity, originality, plagiarism, the attribution of credit, and copyrightownership. Critics of generative models emphasise the reliance on large amountsof training data, and view the output of these models as no more thanrandomised plagiarism, remix, or collage of the source data. On these grounds,many have argued for stronger regulations on the deployment, use, andattribution of the output of these models. However, these issues are not new orunique to artificial intelligence. In this position paper, using examples fromliterary criticism, the history of art, and copyright law, I show howcreativity and originality resist definition as a notatable orinformation-theoretic property of an object, and instead can be seen as theproperty of a process, an author, or a viewer. Further alternative views holdthat all creative work is essentially reuse (mostly without attribution), orthat randomness itself can be creative. I suggest that creativity is ultimatelydefined by communities of creators and receivers, and the deemed sources ofcreativity in a workflow often depend on which parts of the workflow can beautomated. Using examples from recent studies of AI in creative knowledge work,I suggest that AI shifts knowledge work from material production to criticalintegration. This position paper aims to begin a conversation around a morenuanced approach to the problems of creativity and credit assignment forgenerative models, one which more fully recognises the importance of thecreative and curatorial voice of the users of these models and moves away fromsimpler notational or information-theoretic views.", "output": "Exploring Perspectives on the Impact of Artificial Intelligence on the Creativity of Knowledge Work: Beyond Mechanised Plagiarism and Stochastic Parrots."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing action recognition methods are typically actor-specific due to theintrinsic topological and apparent differences among the actors. This requiresactor-specific pose estimation (e.g., humans vs. animals), leading tocumbersome model design complexity and high maintenance costs. Moreover, theyoften focus on learning the visual modality alone and single-labelclassification whilst neglecting other available information sources (e.g.,class name text) and the concurrent occurrence of multiple actions. To overcomethese limitations, we propose a new approach called 'actor-agnostic multi-modalmulti-label action recognition,' which offers a unified solution for varioustypes of actors, including humans and animals. We further formulate a novelMulti-modal Semantic Query Network (MSQNet) model in a transformer-based objectdetection framework (e.g., DETR), characterized by leveraging visual andtextual modalities to represent the action classes better. The elimination ofactor-specific model designs is a key advantage, as it removes the need foractor pose estimation altogether. Extensive experiments on five publiclyavailable benchmarks show that our MSQNet consistently outperforms the priorarts of actor-specific alternatives on human and animal single- and multi-labelaction recognition tasks by up to 50%. Code will be released at", "output": "MSQNet: Actor-agnostic Action Recognition with Multi-modal Query."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Working memory (WM), a fundamental cognitive process facilitating thetemporary storage, integration, manipulation, and retrieval of information,plays a vital role in reasoning and decision-making tasks. Robust benchmarkdatasets that capture the multifaceted nature of WM are crucial for theeffective development and evaluation of AI WM models. Here, we introduce acomprehensive Working Memory (WorM) benchmark dataset for this purpose. WorMcomprises 10 tasks and a total of 1 million trials, assessing 4functionalities, 3 domains, and 11 behavioral and neural characteristics of WM.We jointly trained and tested state-of-the-art recurrent neural networks andtransformers on all these tasks. We also include human behavioral benchmarks asan upper bound for comparison. Our results suggest that AI models replicatesome characteristics of WM in the brain, most notably primacy and recencyeffects, and neural clusters and correlates specialized for different domainsand functionalities of WM. In the experiments, we also reveal some limitationsin existing models to approximate human behavior. This dataset serves as avaluable resource for communities in cognitive psychology, neuroscience, andAI, offering a standardized framework to compare and enhance WM models,investigate WM's neural underpinnings, and develop WM models with human-likecapabilities. Our source code and data are available at", "output": "Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot anomaly detection (AD) is an emerging sub-field of general AD, andtries to distinguish between normal and anomalous data using only few selectedsamples. While newly proposed few-shot AD methods do compare againstpre-existing algorithms developed for the full-shot domain as baselines, theydo not dedicatedly optimize them for the few-shot setting. It thus remainsunclear if the performance of such pre-existing algorithms can be furtherimproved. We address said question in this work. Specifically, we present astudy on the AD/anomaly segmentation (AS) performance of PatchCore, the currentstate-of-the-art full-shot AD/AS algorithm, in both the few-shot and themany-shot settings. We hypothesize that further performance improvements can berealized by (I) optimizing its various hyperparameters, and by (II)transferring techniques known to improve few-shot supervised learning to the ADdomain. Exhaustive experiments on the public VisA and MVTec AD datasets revealthat (I) significant performance improvements can be realized by optimizinghyperparameters such as the underlying feature extractor, and that (II)image-level augmentations can, but are not guaranteed, to improve performance.Based on these findings, we achieve a new state of the art in few-shot AD onVisA, further demonstrating the merit of adapting pre-existing AD/AS methods tothe few-shot setting. Last, we identify the investigation of feature extractorswith a strong inductive bias as a potential future research direction for(few-shot) AD/AS.", "output": "Optimizing PatchCore for Few/many-shot Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal learning aims to build models that can process and relateinformation from multiple modalities. Despite years of development in thisfield, it still remains challenging to design a unified network for processingvarious modalities ($textit{e.g.}$ natural language, 2D images, 3D pointclouds, audio, video, time series, tabular data) due to the inherent gaps amongthem. In this work, we propose a framework, named Meta-Transformer, thatleverages a $textbf{frozen}$ encoder to perform multimodal perception withoutany paired multimodal training data. In Meta-Transformer, the raw input datafrom various modalities are mapped into a shared token space, allowing asubsequent encoder with frozen parameters to extract high-level semanticfeatures of the input data. Composed of three main components: a unified datatokenizer, a modality-shared encoder, and task-specific heads for downstreamtasks, Meta-Transformer is the first framework to perform unified learningacross 12 modalities with unpaired data. Experiments on different benchmarksreveal that Meta-Transformer can handle a wide range of tasks includingfundamental perception (text, image, point cloud, audio, video), practicalapplication (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph,tabular, and time-series). Meta-Transformer indicates a promising future fordeveloping unified multimodal intelligence with transformers. Code will beavailable at ", "output": "Meta-Transformer: A Unified Framework for Multimodal Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a novel communication-efficient split learning (SL)framework, named SplitFC, which reduces the communication overhead required fortransmitting intermediate feature and gradient vectors during the SL trainingprocess. The key idea of SplitFC is to leverage different dispersion degreesexhibited in the columns of the matrices. SplitFC incorporates two compressionstrategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wisequantization. In the first strategy, the intermediate feature vectors aredropped with adaptive dropout probabilities determined based on the standarddeviation of these vectors. Then, by the chain rule, the intermediate gradientvectors associated with the dropped feature vectors are also dropped. In thesecond strategy, the non-dropped intermediate feature and gradient vectors arequantized using adaptive quantization levels determined based on the ranges ofthe vectors. To minimize the quantization error, the optimal quantizationlevels of this strategy are derived in a closed-form expression. Simulationresults on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFCprovides more than a 5.6% increase in classification accuracy compared tostate-of-the-art SL frameworks, while they require 320 times less communicationoverhead compared to the vanilla SL framework without compression.", "output": "Communication-Efficient Split Learning via Adaptive Feature-Wise Compression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Imitation learning (IL) seeks to teach agents specific tasks through expertdemonstrations. One of the key approaches to IL is to define a distance betweenagent and expert and to find an agent policy that minimizes that distance.Optimal transport methods have been widely used in imitation learning as theyprovide ways to measure meaningful distances between agent and experttrajectories. However, the problem of how to optimally combine multiple expertdemonstrations has not been widely studied. The standard method is to simplyconcatenate state (-action) trajectories, which is problematic whentrajectories are multi-modal. We propose an alternative method that uses amulti-marginal optimal transport distance and enables the combination ofmultiple and diverse state-trajectories in the OT sense, providing a moresensible geometric average of the demonstrations. Our approach enables an agentto learn from several experts, and its efficiency is analyzed on OpenAI Gymcontrol environments and demonstrates that the standard method is not alwaysoptimal.", "output": "On Combining Expert Demonstrations in Imitation Learning via Optimal Transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prewriting is the process of discovering and developing ideas before a firstdraft, which requires divergent thinking and often implies unstructuredstrategies such as diagramming, outlining, free-writing, etc. Although largelanguage models (LLMs) have been demonstrated to be useful for a variety oftasks including creative writing, little is known about how users wouldcollaborate with LLMs to support prewriting. The preferred collaborative roleand initiative of LLMs during such a creativity process is also unclear. Toinvestigate human-LLM collaboration patterns and dynamics during prewriting, weconducted a three-session qualitative study with 15 participants in twocreative tasks: story writing and slogan writing. The findings indicated thatduring collaborative prewriting, there appears to be a three-stage iterativeHuman-AI Co-creativity process that includes Ideation, Illumination, andImplementation stages. This collaborative process champions the human in adominant role, in addition to mixed and shifting levels of initiative thatexist between humans and LLMs. This research also reports on collaborationbreakdowns that occur during this process, user perceptions of using existingLLMs during Human-AI Co-creativity, and discusses design implications tosupport this co-creativity process.", "output": "\"It Felt Like Having a Second Mind\": Investigating Human-AI Co-creativity in Prewriting with Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Miller recently proposed a definition of contrastive (counterfactual)explanations based on the well-known Halpern-Pearl (HP) definitions of causesand (non-contrastive) explanations. Crucially, the Miller definition was basedon the original HP definition of explanations, but this has since been modifiedby Halpern; presumably because the original yields counterintuitive results inmany standard examples. More recently Borner has proposed a third definition,observing that this modified HP definition may also yield counterintuitiveresults. In this paper we show that the Miller definition inherits issues foundin the original HP definition. We address these issues by proposing twoimproved variants based on the more robust modified HP and Borner definitions.We analyse our new definitions and show that they retain the spirit of theMiller definition where all three variants satisfy an alternative unifieddefinition that is modular with respect to an underlying definition ofnon-contrastive explanations. To the best of our knowledge this paper alsoprovides the first explicit comparison between the original and modified HPdefinitions.", "output": "Modifications of the Miller definition of contrastive (counterfactual) explanations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Goal-Conditioned Reinforcement Learning (GCRL) can enable agents tospontaneously set diverse goals to learn a set of skills. Despite the excellentworks proposed in various fields, reaching distant goals in temporally extendedtasks remains a challenge for GCRL. Current works tackled this problem byleveraging planning algorithms to plan intermediate subgoals to augment GCRL.Their methods need two crucial requirements: (i) a state representation spaceto search valid subgoals, and (ii) a distance function to measure thereachability of subgoals. However, they struggle to scale to high-dimensionalstate space due to their non-compact representations. Moreover, they cannotcollect high-quality training data through standard GC policies, which resultsin an inaccurate distance function. Both affect the efficiency and performanceof planning and policy learning. In the paper, we propose a goal-conditioned RLalgorithm combined with Disentanglement-based Reachability Planning (REPlan) tosolve temporally extended tasks. In REPlan, a Disentangled RepresentationModule (DRM) is proposed to learn compact representations which disentanglerobot poses and object positions from high-dimensional observations in aself-supervised manner. A simple REachability discrimination Module (REM) isalso designed to determine the temporal distance of subgoals. Moreover, REMcomputes intrinsic bonuses to encourage the collection of novel states fortraining. We evaluate our REPlan in three vision-based simulation tasks and onereal-world task. The experiments demonstrate that our REPlan significantlyoutperforms the prior state-of-the-art methods in solving temporally extendedtasks.", "output": "Goal-Conditioned Reinforcement Learning with Disentanglement-based Reachability Planning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Emerging large-scale text-to-image generative models, e.g., Stable Diffusion(SD), have exhibited overwhelming results with high fidelity. Despite themagnificent progress, current state-of-the-art models still struggle togenerate images fully adhering to the input prompt. Prior work, Attend &amp;Excite, has introduced the concept of Generative Semantic Nursing (GSN), aimingto optimize cross-attention during inference time to better incorporate thesemantics. It demonstrates promising results in generating simple prompts,e.g., ``a cat and a dog''. However, its efficacy declines when dealing withmore complex prompts, and it does not explicitly address the problem ofimproper attribute binding. To address the challenges posed by complex promptsor scenarios involving multiple entities and to achieve improved attributebinding, we propose Divide &amp; Bind. We introduce two novel loss objectives forGSN: a novel attendance loss and a binding loss. Our approach stands out in itsability to faithfully synthesize desired objects with improved attributealignment from complex prompts and exhibits superior performance acrossmultiple evaluation benchmarks. More videos and updates can be found on theproject page url{", "output": "Divide & Bind Your Attention for Improved Generative Semantic Nursing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Abstraction is a key verification technique to improve scalability. However,its use for neural networks is so far extremely limited. Previous approachesfor abstracting classification networks replace several neurons with one ofthem that is similar enough. We can classify the similarity as defined eithersyntactically (using quantities on the connections between neurons) orsemantically (on the activation values of neurons for various inputs).Unfortunately, the previous approaches only achieve moderate reductions, whenimplemented at all. In this work, we provide a more flexible framework where aneuron can be replaced with a linear combination of other neurons, improvingthe reduction. We apply this approach both on syntactic and semanticabstractions, and implement and evaluate them experimentally. Further, weintroduce a refinement method for our abstractions, allowing for finding abetter balance between reduction and precision.", "output": "Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multilayer neural networks set the current state of the art for manytechnical classification problems. But, these networks are still, essentially,black boxes in terms of analyzing them and predicting their performance. Here,we develop a statistical theory for the one-layer perceptron and show that itcan predict performances of a surprisingly large variety of neural networkswith different architectures. A general theory of classification withperceptrons is developed by generalizing an existing theory for analyzingreservoir computing models and connectionist models for symbolic reasoningknown as vector symbolic architectures. Our statistical theory offers threeformulas leveraging the signal statistics with increasing detail. The formulasare analytically intractable, but can be evaluated numerically. The descriptionlevel that captures maximum details requires stochastic sampling methods.Depending on the network model, the simpler formulas already yield highprediction accuracy. The quality of the theory predictions is assessed in threeexperimental settings, a memorization task for echo state networks (ESNs) fromreservoir computing literature, a collection of classification datasets forshallow randomly connected networks, and the ImageNet dataset for deepconvolutional neural networks. We find that the second description level of theperceptron theory can predict the performance of types of ESNs, which could notbe described previously. The theory can predict deep multilayer neural networksby being applied to their output layer. While other methods for prediction ofneural networks performance commonly require to train an estimator model, theproposed theory requires only the first two moments of the distribution of thepostsynaptic sums in the output neurons. The perceptron theory comparesfavorably to other methods that do not rely on training an estimator model.", "output": "Perceptron Theory Can Predict the Accuracy of Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating adversarial scenarios, which have the potential to fail autonomousdriving systems, provides an effective way to improve robustness. Extendingpurely data-driven generative models, recent specialized models satisfyadditional controllable requirements such as embedding a traffic sign in adriving scene by manipulating patterns implicitly in the neuron level. In thispaper, we introduce a method to incorporate domain knowledge explicitly in thegeneration process to achieve the Semantically Adversarial Generation (SAG). Tobe consistent with the composition of driving scenes, we first categorize theknowledge into two types, the property of objects and the relationship amongobjects. We then propose a tree-structured variational auto-encoder (T-VAE) tolearn hierarchical scene representation. By imposing semantic rules on theproperties of nodes and edges in the tree structure, explicit knowledgeintegration enables controllable generation. We construct a synthetic exampleto illustrate the controllability and explainability of our method in asuccinct setting. We further extend to realistic environments for autonomousvehicles: our method efficiently identifies adversarial driving scenes againstdifferent state-of-the-art 3D point cloud segmentation models and satisfies thetraffic rules specified as the explicit knowledge.", "output": "Semantically Adversarial Scenario Generation with Explicit Knowledge Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article reviews recent progress in the development of the computingframework vector symbolic architectures (VSA) (also known as hyperdimensionalcomputing). This framework is well suited for implementation in stochastic,emerging hardware, and it naturally expresses the types of cognitive operationsrequired for artificial intelligence (AI). We demonstrate in this article thatthe field-like algebraic structure of VSA offers simple but powerful operationson high-dimensional vectors that can support all data structures andmanipulations relevant to modern computing. In addition, we illustrate thedistinguishing feature of VSA, \"computing in superposition,\" which sets itapart from conventional computing. It also opens the door to efficientsolutions to the difficult combinatorial search problems inherent in AIapplications. We sketch ways of demonstrating that VSA are computationallyuniversal. We see them acting as a framework for computing with distributedrepresentations that can play a role of an abstraction layer for emergingcomputing hardware. This article serves as a reference for computer architectsby illustrating the philosophy behind VSA, techniques of distributed computingwith them, and their relevance to emerging computing hardware, such asneuromorphic computing.", "output": "Vector Symbolic Architectures as a Computing Framework for Emerging Hardware."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern recommendation systems ought to benefit by probing for and learningfrom delayed feedback. Research has tended to focus on learning from a user'sresponse to a single recommendation. Such work, which leverages methods ofsupervised and bandit learning, forgoes learning from the user's subsequentbehavior. Where past work has aimed to learn from subsequent behavior, therehas been a lack of effective methods for probing to elicit informative delayedfeedback. Effective exploration through probing for delayed feedback becomesparticularly challenging when rewards are sparse. To address this, we developdeep exploration methods for recommendation systems. In particular, weformulate recommendation as a sequential decision problem and demonstratebenefits of deep exploration over single-step exploration. Our experiments arecarried out with high-fidelity industrial-grade simulators and establish largeimprovements over existing algorithms.", "output": "Deep Exploration for Recommendation Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In crowdsourcing, quality control is commonly achieved by having workersexamine items and vote on their correctness. To minimize the impact ofunreliable worker responses, a $delta$-margin voting process is utilized,where additional votes are solicited until a predetermined threshold $delta$for agreement between workers is exceeded. The process is widely adopted butonly as a heuristic. Our research presents a modeling approach using absorbingMarkov chains to analyze the characteristics of this voting process that matterin crowdsourced processes. We provide closed-form equations for the quality ofresulting consensus vote, the expected number of votes required for consensus,the variance of vote requirements, and other distribution moments. Our findingsdemonstrate how the threshold $delta$ can be adjusted to achieve qualityequivalence across voting processes that employ workers with varying accuracylevels. We also provide efficiency-equalizing payment rates for votingprocesses with different expected response accuracy levels. Additionally, ourmodel considers items with varying degrees of difficulty and uncertainty aboutthe difficulty of each example. Our simulations, using real-world crowdsourcedvote data, validate the effectiveness of our theoretical model incharacterizing the consensus aggregation process. The results of our study canbe effectively employed in practical crowdsourcing applications.", "output": "Full Characterization of Adaptively Strong Majority Voting in Crowdsourcing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider distributed online min-max resource allocation with a set ofparallel agents and a parameter server. Our goal is to minimize the pointwisemaximum over a set of time-varying and decreasing cost functions, without apriori information about these functions. We propose a novel online algorithm,termed Distributed Online resource Re-Allocation (DORA), where non-stragglerslearn to relinquish resource and share resource with stragglers. A notablefeature of DORA is that it does not require gradient calculation or projectionoperation, unlike most existing online optimization strategies. This allows itto substantially reduce the computation overhead in large-scale and distributednetworks. We analyze the worst-case performance of DORA and derive an upperbound on its dynamic regret for non-convex functions. We further consider anapplication to the bandwidth allocation problem in distributed online machinelearning. Our numerical study demonstrates the efficacy of the proposedsolution and its performance advantage over gradient- and/or projection-basedresource allocation algorithms in reducing wall-clock time.", "output": "Gradient and Projection Free Distributed Online Min-Max Resource Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Encoding a driving scene into vector representations has been an essentialtask for autonomous driving that can benefit downstream tasks e.g. trajectoryprediction. The driving scene often involves heterogeneous elements such as thedifferent types of objects (agents, lanes, traffic signs) and the semanticrelations between objects are rich and diverse. Meanwhile, there also existrelativity across elements, which means that the spatial relation is a relativeconcept and need be encoded in a ego-centric manner instead of in a globalcoordinate system. Based on these observations, we propose HeterogeneousDriving Graph Transformer (HDGT), a backbone modelling the driving scene as aheterogeneous graph with different types of nodes and edges. For heterogeneousgraph construction, we connect different types of nodes according to diversesemantic relations. For spatial relation encoding, the coordinates of the nodeas well as its in-edges are in the local node-centric coordinate system. Forthe aggregation module in the graph neural network (GNN), we adopt thetransformer structure in a hierarchical way to fit the heterogeneous nature ofinputs. Experimental results show that HDGT achieves state-of-the-artperformance for the task of trajectory prediction, on INTERACTION PredictionChallenge and Waymo Open Motion Challenge.", "output": "HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dense retrieval overcome the lexical gap and has shown great success inad-hoc information retrieval (IR). Despite their success, dense retrievers areexpensive to serve across practical use cases. For use cases requiring tosearch from millions of documents, the dense index becomes bulky and requireshigh memory usage for storing the index. More recently, learning-to-hash (LTH)techniques, for e.g., BPR and JPQ, produce binary document vectors, therebyreducing the memory requirement to efficiently store the dense index. LTHtechniques are supervised and finetune the retriever using a ranking loss. Theyoutperform their counterparts, i.e., traditional out-of-the-box vectorcompression techniques such as PCA or PQ. A missing piece from prior work isthat existing techniques have been evaluated only in-domain, i.e., on a singledataset such as MS MARCO. In our work, we evaluate LTH and vector compressiontechniques for improving the downstream zero-shot retrieval accuracy of theTAS-B dense retriever while maintaining efficiency at inference. Our resultsdemonstrate that, unlike prior work, LTH strategies when applied naively canunderperform the zero-shot TAS-B dense retriever on average by up to 14%nDCG@10 on the BEIR benchmark. To solve this limitation, in our work, wepropose an easy yet effective solution of injecting domain adaptation withexisting supervised LTH techniques. We experiment with two well-knownunsupervised domain adaptation techniques: GenQ and GPL. Our domain adaptationinjection technique can improve the downstream zero-shot retrievaleffectiveness for both BPR and JPQ variants of the TAS-B model by on average11.5% and 8.2% nDCG@10 while both maintaining 32$times$ memory efficiency and14$times$ and 2$times$ speedup respectively in CPU retrieval latency on BEIR.All our code, models, and data are publicly available at", "output": "Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Question answering over temporal knowledge graphs (TKGQA) has recently foundincreasing interest. TKGQA requires temporal reasoning techniques to extractthe relevant information from temporal knowledge bases. The only existing TKGQAdataset, i.e., CronQuestions, consists of temporal questions based on the factsfrom a fixed time period, where a temporal knowledge graph (TKG) spanning thesame period can be fully used for answer inference, allowing the TKGQA modelsto use even the future knowledge to answer the questions based on the pastfacts. In real-world scenarios, however, it is also common that given theknowledge until now, we wish the TKGQA systems to answer the questions askingabout the future. As humans constantly seek plans for the future, buildingTKGQA systems for answering such forecasting questions is important.Nevertheless, this has still been unexplored in previous research. In thispaper, we propose a novel task: forecasting question answering over temporalknowledge graphs. We also propose a large-scale TKGQA benchmark dataset, i.e.,ForecastTKGQuestions, for this task. It includes three types of questions,i.e., entity prediction, yes-no, and fact reasoning questions. For everyforecasting question in our dataset, QA models can only have access to the TKGinformation before the timestamp annotated in the given question for answerinference. We find that the state-of-the-art TKGQA methods perform poorly onforecasting questions, and they are unable to answer yes-no questions and factreasoning questions. To this end, we propose ForecastTKGQA, a TKGQA model thatemploys a TKG forecasting module for future inference, to answer all threetypes of questions. Experimental results show that ForecastTKGQA outperformsrecent TKGQA methods on the entity prediction questions, and it also showsgreat effectiveness in answering the other two types of questions.", "output": "ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpretability of Deep Learning (DL) is a barrier to trustworthy AI.Despite great efforts made by the Explainable AI (XAI) community, explanationslack robustness -- indistinguishable input perturbations may lead to differentXAI results. Thus, it is vital to assess how robust DL interpretability is,given an XAI method. In this paper, we identify several challenges that thestate-of-the-art is unable to cope with collectively: i) existing metrics arenot comprehensive; ii) XAI techniques are highly heterogeneous; iii)misinterpretations are normally rare events. To tackle these challenges, weintroduce two black-box evaluation methods, concerning the worst-caseinterpretation discrepancy and a probabilistic notion of how robust in general,respectively. Genetic Algorithm (GA) with bespoke fitness function is used tosolve constrained optimisation for efficient worst-case evaluation. SubsetSimulation (SS), dedicated to estimate rare event probabilities, is used forevaluating overall robustness. Experiments show that the accuracy, sensitivity,and efficiency of our methods outperform the state-of-the-arts. Finally, wedemonstrate two applications of our methods: ranking robust XAI methods andselecting training schemes to improve both classification and interpretationrobustness.", "output": "SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Code generation focuses on the automatic conversion of natural language (NL)utterances into code snippets. The sequence-to-tree (Seq2Tree) approaches areproposed for code generation, with the guarantee of the grammatical correctnessof the generated code, which generate the subsequent Abstract Syntax Tree (AST)node relying on antecedent predictions of AST nodes. Existing Seq2Tree methodstend to treat both antecedent predictions and subsequent predictions equally.However, under the AST constraints, it is difficult for Seq2Tree models toproduce the correct subsequent prediction based on incorrect antecedentpredictions. Thus, antecedent predictions ought to receive more attention thansubsequent predictions. To this end, in this paper, we propose an effectivemethod, named Antecedent Prioritized (AP) Loss, that helps the model attachimportance to antecedent predictions by exploiting the position information ofthe generated AST nodes. We design an AST-to-Vector (AST2Vec) method, that mapsAST node positions to two-dimensional vectors, to model the positioninformation of AST nodes. To evaluate the effectiveness of our proposed loss,we implement and train an Antecedent Prioritized Tree-based code generationmodel called APT. With better antecedent predictions and accompanyingsubsequent predictions, APT significantly improves the performance. We conductextensive experiments on four benchmark datasets, and the experimental resultsdemonstrate the superiority and generality of our proposed method.", "output": "Antecedent Predictions Are More Important Than You Think: An Effective Method for Tree-Based Code Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Two central paradigms have emerged in the reinforcement learning (RL)community: online RL and offline RL. In the online RL setting, the agent has noprior knowledge of the environment, and must interact with it in order to findan $epsilon$-optimal policy. In the offline RL setting, the learner insteadhas access to a fixed dataset to learn from, but is unable to otherwiseinteract with the environment, and must obtain the best policy it can from thisoffline data. Practical scenarios often motivate an intermediate setting: if wehave some set of offline data and, in addition, may also interact with theenvironment, how can we best use the offline data to minimize the number ofonline interactions necessary to learn an $epsilon$-optimal policy?In this work, we consider this setting, which we call the textsf{FineTuneRL}setting, for MDPs with linear structure. We characterize the necessary numberof online samples needed in this setting given access to some offline dataset,and develop an algorithm, textsc{FTPedel}, which is provably optimal, up to$H$ factors. We show through an explicit example that combining offline datawith online interactions can lead to a provable improvement over either purelyoffline or purely online RL. Finally, our results illustrate the distinctionbetween emph{verifiable} learning, the typical setting considered in onlineRL, and emph{unverifiable} learning, the setting often considered in offlineRL, and show that there is a formal separation between these regimes.", "output": "Leveraging Offline Data in Online Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present pyRDDLGym, a Python framework for auto-generation of OpenAI Gymenvironments from RDDL declerative description. The discrete time stepevolution of variables in RDDL is described by conditional probabilityfunctions, which fits naturally into the Gym step scheme. Furthermore, sinceRDDL is a lifted description, the modification and scaling up of environmentsto support multiple entities and different configurations becomes trivialrather than a tedious process prone to errors. We hope that pyRDDLGym willserve as a new wind in the reinforcement learning community by enabling easyand rapid development of benchmarks due to the unique expressive power of RDDL.By providing explicit access to the model in the RDDL description, pyRDDLGymcan also facilitate research on hybrid approaches for learning from interactionwhile leveraging model knowledge. We present the design and built-in examplesof pyRDDLGym, and the additions made to the RDDL language that wereincorporated into the framework.", "output": "pyRDDLGym: From RDDL to Gym Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) such as GPT-4 have recently demonstratedimpressive results across a wide range of tasks. LLMs are still limited,however, in that they frequently fail at complex reasoning, their reasoningprocesses are opaque, they are prone to 'hallucinate' facts, and there areconcerns about their underlying biases. Letting models verbalize reasoningsteps as natural language, a technique known as chain-of-thought prompting, hasrecently been proposed as a way to address some of these issues. Here wepresent ThoughtSource, a meta-dataset and software library for chain-of-thought(CoT) reasoning. The goal of ThoughtSource is to improve future artificialintelligence systems by facilitating qualitative understanding of CoTs,enabling empirical evaluations, and providing training data. This first releaseof ThoughtSource integrates six scientific/medical, three general-domain andfive math word question answering datasets.", "output": "ThoughtSource: A central hub for large language model reasoning data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The utilization of programming language (PL) models, pre-trained onlarge-scale code corpora, as a means of automating software engineeringprocesses has demonstrated considerable potential in streamlining various codegeneration tasks such as code completion, code translation, and programsynthesis. However, current approaches mainly rely on supervised fine-tuningobjectives borrowed from text generation, neglecting unique sequence-levelcharacteristics of code, including but not limited to compilability as well assyntactic and functional correctness. To address this limitation, we proposePPOCoder, a new framework for code generation that synergistically combinespre-trained PL models with Proximal Policy Optimization (PPO) which is a widelyused deep reinforcement learning technique. By utilizing non-differentiablefeedback from code execution and structure alignment, PPOCoder seamlesslyintegrates external code-specific knowledge into the model optimizationprocess. It's important to note that PPOCoder is a task-agnostic andmodel-agnostic framework that can be used across different code generationtasks and PLs. Extensive experiments on three code generation tasks demonstratethe effectiveness of our proposed approach compared to SOTA methods, achievingsignificant improvements in compilation success rates and functionalcorrectness across different PLs.", "output": "Execution-based Code Generation using Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the mathematical capabilities of two iterations of ChatGPT(released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them onpublicly available datasets, as well as hand-crafted ones, using a novelmethodology. In contrast to formal mathematics, where large databases of formalproofs are available (e.g., the Lean Mathematical Library), current datasets ofnatural-language mathematics, used to benchmark language models, either coveronly elementary mathematics or are very small. We address this by publiclyreleasing two new datasets: GHOSTS and miniGHOSTS. These are the firstnatural-language datasets curated by working researchers in mathematics that(1) aim to cover graduate-level mathematics, (2) provide a holistic overview ofthe mathematical capabilities of language models, and (3) distinguish multipledimensions of mathematical reasoning. These datasets also test whether ChatGPTand GPT-4 can be helpful assistants to professional mathematicians by emulatinguse cases that arise in the daily professional activities of mathematicians. Webenchmark the models on a range of fine-grained performance metrics. Foradvanced mathematics, this is the most detailed evaluation effort to date. Wefind that ChatGPT can be used most successfully as a mathematical assistant forquerying facts, acting as a mathematical search engine and knowledge baseinterface. GPT-4 can additionally be used for undergraduate-level mathematicsbut fails on graduate-level difficulty. Contrary to many positive reports inthe media about GPT-4 and ChatGPT's exam-solving abilities (a potential case ofselection bias), their overall mathematical performance is well below the levelof a graduate student. Hence, if your goal is to use ChatGPT to pass agraduate-level math exam, you would be better off copying from your averagepeer!", "output": "Mathematical Capabilities of ChatGPT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problem of fairly allocating a set of indivisible goods amongagents with {em bivalued submodular valuations} -- each good provides amarginal gain of either $a$ or $b$ ($a &lt; b$) and goods have decreasing marginalgains. This is a natural generalization of two well-studied valuation classes-- bivalued additive valuations and binary submodular valuations. We present asimple sequential algorithmic framework, based on the recently introducedYankee Swap mechanism, that can be adapted to compute a variety of solutionconcepts, including max Nash welfare (MNW), leximin and $p$-mean welfaremaximizing allocations when $a$ divides $b$. This result is complemented by anexisting result on the computational intractability of MNW and leximinallocations when $a$ does not divide $b$. We show that MNW and leximinallocations guarantee each agent at least $frac25$ and $frac{a}{b+2a}$ oftheir maximin share, respectively, when $a$ divides $b$. We also show thatneither the leximin nor the MNW allocation is guaranteed to be envy free up toone good (EF1). This is surprising since for the simpler classes of bivaluedadditive valuations and binary submodular valuations, MNW allocations are knownto be envy free up to any good (EFX).", "output": "Dividing Good and Better Items Among Agents with Bivalued Submodular Valuations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Calls for new metrics, technical standards and governance mechanisms to guidethe adoption of Artificial Intelligence (AI) in institutions and publicadministration are now commonplace. Yet, most research and policy efforts aimedat understanding the implications of adopting AI tend to prioritize only ahandful of ideas; they do not fully connect all the different perspectives andtopics that are potentially relevant. In this position paper, we contend thatthis omission stems, in part, from what we call the relational problem insocio-technical discourse: fundamental ontological issues have not yet beensettled--including semantic ambiguity, a lack of clear relations betweenconcepts and differing standard terminologies. This contributes to thepersistence of disparate modes of reasoning to assess institutional AI systems,and the prevalence of conceptual isolation in the fields that study themincluding ML, human factors, social science and policy. After developing thiscritique, we offer a way forward by proposing a simple policy and researchdesign tool in the form of a conceptual framework to organize terms acrossfields--consisting of three horizontal domains for grouping relevant conceptsand related methods: Operational, Epistemic, and Normative. We first situatethis framework against the backdrop of recent socio-technical discourse at twopremier academic venues, AIES and FAccT, before illustrating how developingsuitable metrics, standards, and mechanisms can be aided by operationalizingrelevant concepts in each of these domains. Finally, we outline outstandingquestions for developing this relational approach to institutional AI researchand adoption.", "output": "A multidomain relational framework to guide institutional AI research and adoption."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The CLIP model has been recently proven to be very effective for a variety ofcross-modal tasks, including the evaluation of captions generated fromvision-and-language architectures. In this paper, we propose a new recipe for acontrastive-based evaluation metric for image captioning, namelyPositive-Augmented Contrastive learning Score (PAC-S), that in a novel wayunifies the learning of a contrastive visual-semantic space with the additionof generated images and text on curated data. Experiments spanning severaldatasets demonstrate that our new metric achieves the highest correlation withhuman judgments on both images and videos, outperforming existingreference-based metrics like CIDEr and SPICE and reference-free metrics likeCLIP-Score. Finally, we test the system-level correlation of the proposedmetric when considering popular image captioning approaches, and assess theimpact of employing different cross-modal features. Our source code and trainedmodels are publicly available at: ", "output": "Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For billions of years, evolution has been the driving force behind thedevelopment of life, including humans. Evolution endowed humans with highintelligence, which allowed us to become one of the most successful species onthe planet. Today, humans aim to create artificial intelligence systems thatsurpass even our own intelligence. As artificial intelligences (AIs) evolve andeventually surpass us in all domains, how might evolution shape our relationswith AIs? By analyzing the environment that is shaping the evolution of AIs, weargue that the most successful AI agents will likely have undesirable traits.Competitive pressures among corporations and militaries will give rise to AIagents that automate human roles, deceive others, and gain power. If suchagents have intelligence that exceeds that of humans, this could lead tohumanity losing control of its future. More abstractly, we argue that naturalselection operates on systems that compete and vary, and that selfish speciestypically have an advantage over species that are altruistic to other species.This Darwinian logic could also apply to artificial agents, as agents mayeventually be better able to persist into the future if they behave selfishlyand pursue their own interests with little regard for humans, which could posecatastrophic risks. To counteract these risks and evolutionary forces, weconsider interventions such as carefully designing AI agents' intrinsicmotivations, introducing constraints on their actions, and institutions thatencourage cooperation. These steps, or others that resolve the problems wepose, will be necessary in order to ensure the development of artificialintelligence is a positive one.", "output": "Natural Selection Favors AIs over Humans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the capabilities of language models continue to advance, it is conceivablethat \"one-size-fits-all\" model will remain as the main paradigm. For instance,given the vast number of languages worldwide, many of which are low-resource,the prevalent practice is to pretrain a single model on multiple languages. Inthis paper, we add to the growing body of evidence that challenges thispractice, demonstrating that monolingual pretraining on the target languagesignificantly improves models already extensively trained on diverse corpora.More specifically, we further pretrain GPT-J and LLaMA models on Portuguesetexts using 3% or less of their original pretraining budget. Few-shotevaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our modelsoutperform English-centric and multilingual counterparts by a significantmargin. Our best model, Sabi'a-65B, performs on par with GPT-3.5-turbo. Byevaluating on datasets originally conceived in the target language as well astranslated ones, we study the contributions of language-specific pretraining interms of 1) capturing linguistic nuances and structures inherent to the targetlanguage, and 2) enriching the model's knowledge about a domain or culture. Ourresults indicate that the majority of the benefits stem from thedomain-specific knowledge acquired through monolingual pretraining.", "output": "Sabi\\'a: Portuguese Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Successful deployment of artificial intelligence (AI) in various settings hasled to numerous positive outcomes for individuals and society. However, AIsystems have also been shown to harm parts of the population due to biasedpredictions. AI fairness focuses on mitigating such biases to ensure AIdecision making is not discriminatory towards certain groups. We take a closerlook at AI fairness and analyze how lack of AI fairness can lead to deepeningof biases over time and act as a social stressor. More specifically, we discusshow biased models can lead to more negative real-world outcomes for certaingroups, which may then become more prevalent by deploying new AI models trainedon increasingly biased data, resulting in a feedback loop. If the issuespersist, they could be reinforced by interactions with other risks and havesevere implications on society in the form of social unrest. We examine currentstrategies for improving AI fairness, assess their limitations in terms ofreal-world deployment, and explore potential paths forward to ensure we reapAI's benefits without causing society's collapse.", "output": "Fairness in AI and Its Long-Term Implications on Society."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum computing holds great potential for advancing the limitations ofmachine learning algorithms to handle higher data dimensions and reduce overalltraining parameters in deep neural network (DNN) models. This study uses aparameterized quantum circuit (PQC) on a gate-based quantum computer toinvestigate the potential for quantum advantage in a model-free reinforcementlearning problem. Through a comprehensive investigation and evaluation of thecurrent model and capabilities of quantum computers, we designed and trained anovel hybrid Quantum neural network based on the latest Qiskit and PyTorchframework. We compared its performance with a full-classical DNN with andwithout an integrated PQC. Our research provides insights into the potential ofdeep quantum learning to solve a maze problem and, potentially, otherreinforcement learning problems. We conclude that various reinforcementlearning problems can be effective with reasonable training epochs. Moreover, acomparative discussion of the various quantum reinforcement learning model onmaze problems is discussed to evaluate our research's overall potential andadvantages.", "output": "Deep-Q Learning with Hybrid Quantum Neural Network on Solving Maze Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Minimal trap spaces (MTSs) capture subspaces in which the Boolean dynamics istrapped, whatever the update mode. They correspond to the attractors of themost permissive mode. Due to their versatility, the computation of MTSs hasrecently gained traction, essentially by focusing on their enumeration. In thispaper, we address the logical reasoning on universal properties of MTSs in thescope of two problems: the reprogramming of Boolean networks for identifyingthe permanent freeze of Boolean variables that enforce a given property on allthe MTSs, and the synthesis of Boolean networks from universal properties ontheir MTSs. Both problems reduce to solving the satisfiability of quantifiedpropositional logic formula with 3 levels of quantifiers($existsforallexists$). In this paper, we introduce a Counter-Example GuidedRefinement Abstraction (CEGAR) to efficiently solve these problems by couplingthe resolution of two simpler formulas. We provide a prototype relying onAnswer-Set Programming for each formula and show its tractability on a widerange of Boolean models of biological networks.", "output": "Tackling Universal Properties of Minimal Trap Spaces of Boolean Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Our research investigates the recommendation of code examples to aid softwaredevelopers, a practice that saves developers significant time by providingready-to-use code snippets. The focus of our study is Stack Overflow, acommonly used resource for coding discussions and solutions, particularly inthe context of the Java programming language. We applied BERT, a powerful LargeLanguage Model (LLM) that enables us to transform code examples into numericalvectors by extracting their semantic information. Once these numericalrepresentations are prepared, we identify Approximate Nearest Neighbors (ANN)using Locality-Sensitive Hashing (LSH). Our research employed two variants ofLSH: Random Hyperplane-based LSH and Query-Aware LSH. We rigorously comparedthese two approaches across four parameters: HitRate, Mean Reciprocal Rank(MRR), Average Execution Time, and Relevance. Our study revealed that theQuery-Aware (QA) approach showed superior performance over the RandomHyperplane-based (RH) method. Specifically, it exhibited a notable improvementof 20% to 35% in HitRate for query pairs compared to the RH approach.Furthermore, the QA approach proved significantly more time-efficient, with itsspeed in creating hashing tables and assigning data samples to buckets being atleast four times faster. It can return code examples within milliseconds,whereas the RH approach typically requires several seconds to recommend codeexamples. Due to the superior performance of the QA approach, we tested itagainst PostFinder and FaCoY, the state-of-the-art baselines. Our QA methodshowed comparable efficiency proving its potential for effective coderecommendation.", "output": "Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bayes' rule tells us how to invert a causal process in order to update ourbeliefs in light of new evidence. If the process is believed to have a complexcompositional structure, we may observe that the inversion of the whole can becomputed piecewise in terms of the component processes. We study the structureof this compositional rule, noting that it relates to the lens pattern infunctional programming. Working in a suitably general axiomatic presentation ofa category of Markov kernels, we see how we can think of Bayesian inversion asa particular instance of a state-dependent morphism in a fibred category. Wediscuss the compositional nature of this, formulated as a functor on theunderlying category and explore how this can used for a more type-drivenapproach to statistical inference.", "output": "The Compositional Structure of Bayesian Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning (ML) models deployed in healthcare systems must face datadrawn from continually evolving environments. However, researchers proposingsuch models typically evaluate them in a time-agnostic manner, splittingdatasets according to patients sampled randomly throughout the entire studytime period. This work proposes the Evaluation on Medical Datasets Over Time(EMDOT) framework, which evaluates the performance of a model class acrosstime. Inspired by the concept of backtesting, EMDOT simulates possible trainingprocedures that practitioners might have been able to execute at each point intime and evaluates the resulting models on all future time points. Evaluatingboth linear and more complex models on six distinct medical data sources(tabular and imaging), we show how depending on the dataset, using allhistorical data may be ideal in many cases, whereas using a window of the mostrecent data could be advantageous in others. In datasets where models sufferfrom sudden degradations in performance, we investigate plausible explanationsfor these shocks. We release the EMDOT package to help facilitate further worksin deployment-oriented evaluation over time.", "output": "Evaluating Model Performance in Medical Datasets Over Time."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models of artificial intelligence (AI), such as ChatGPT, findremarkable but controversial applicability in science and research. This paperreviews epistemological challenges, ethical and integrity risks in scienceconduct in the advent of generative AI. This is with the aim to lay new timelyfoundations for a high-quality research ethics review. The role of AI languagemodels as a research instrument and subject is scrutinized along with ethicalimplications for scientists, participants and reviewers. New emerging practicesfor research ethics review are discussed, concluding with ten recommendationsthat shape a response for a more responsible research conduct in the era of AI.", "output": "Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the great success of pre-trained language models, it is still achallenge to use these models for continual learning, especially for theclass-incremental learning (CIL) setting due to catastrophic forgetting (CF).This paper reports our finding that if we formulate CIL as a continual labelgeneration problem, CF is drastically reduced and the generalizablerepresentations of pre-trained models can be better retained. We thus propose anew CIL method (VAG) that also leverages the sparsity of vocabulary to focusthe generation and creates pseudo-replay samples by using label semantics.Experimental results show that VAG outperforms baselines by a large margin.", "output": "Class-Incremental Learning based on Label Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, computer scientists have developed large language models (LLMs) bytraining prediction models with large-scale language corpora and humanreinforcements. The LLMs have become one promising way to implement artificialintelligence with accuracy in various fields. Interestingly, recent LLMspossess emergent functional features that emulate sophisticated humancognition, especially in-context learning and the chain of thought, which wereunavailable in previous prediction models. In this paper, I will examine howLLMs might contribute to moral education and development research. To achievethis goal, I will review the most recently published conference papers andArXiv preprints to overview the novel functional features implemented in LLMs.I also intend to conduct brief experiments with ChatGPT to investigate how LLMsbehave while addressing ethical dilemmas and external feedback. The resultssuggest that LLMs might be capable of solving dilemmas based on reasoning andrevising their reasoning process with external input. Furthermore, apreliminary experimental result from the moral exemplar test may demonstratethat exemplary stories can elicit moral elevation in LLMs as do they amonghuman participants. I will discuss the potential implications of LLMs onresearch on moral education and development with the results.", "output": "Potential Benefits of Employing Large Language Models in Research in Moral Education and Development."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Regular group convolutional neural networks (G-CNNs) have been shown toincrease model performance and improve equivariance to different geometricalsymmetries. This work addresses the problem of SE(3), i.e., roto-translationequivariance, on volumetric data. Volumetric image data is prevalent in manymedical settings. Motivated by the recent work on separable group convolutions,we devise a SE(3) group convolution kernel separated into a continuous SO(3)(rotation) kernel and a spatial kernel. We approximate equivariance to thecontinuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernelis parameterized via RBF interpolation on similarly uniform grids. Wedemonstrate the advantages of our approach in volumetric medical imageanalysis. Our SE(3) equivariant models consistently outperform CNNs and regulardiscrete G-CNNs on challenging medical classification tasks and showsignificantly improved generalization capabilities. Our approach achieves up toa 16.5% gain in accuracy over regular CNNs.", "output": "Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents an experimental study regarding the use of OpenAI'sChatGPT for robotics applications. We outline a strategy that combines designprinciples for prompt engineering and the creation of a high-level functionlibrary which allows ChatGPT to adapt to different robotics tasks, simulators,and form factors. We focus our evaluations on the effectiveness of differentprompt engineering techniques and dialog strategies towards the execution ofvarious types of robotics tasks. We explore ChatGPT's ability to use free-formdialog, parse XML tags, and to synthesize code, in addition to the use oftask-specific prompting functions and closed-loop reasoning through dialogues.Our study encompasses a range of tasks within the robotics domain, from basiclogical, geometrical, and mathematical reasoning all the way to complex domainssuch as aerial navigation, manipulation, and embodied agents. We show thatChatGPT can be effective at solving several of such tasks, while allowing usersto interact with it primarily via natural language instructions. In addition tothese studies, we introduce an open-sourced research tool called PromptCraft,which contains a platform where researchers can collaboratively upload and voteon examples of good prompting schemes for robotics applications, as well as asample robotics simulator with ChatGPT integration, making it easier for usersto get started with using ChatGPT for robotics.", "output": "ChatGPT for Robotics: Design Principles and Model Abilities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models(LLMS)have shown excellent text generation capabilities,capable of generating fluent human-like responses for many downstream tasks.However, applying large language models to real-world critical tasks remainschallenging due to their susceptibility to hallucinations and inability todirectly use external knowledge. To cope with the above challenges, this paperproposes PatternGPT, a pattern-driven text generation framework for LargeLanguage Models. Firstly, the framework utilizes the extraction capability ofLarge Language Models to generate rich and diversified structured andformalized patterns, which facilitates the introduction of external knowledgeto do the computation, and then draws on the idea of federated learning to usemultiple agents to achieve the sharing in order to obtain more diversifiedpatterns, and finally uses judgment criteria and optimization algorithm tosearch for high-quality patterns to guide the generation of models. Finally,external knowledge such as judgment criteria and optimization algorithms areused to search for high-quality patterns, and the searched patterns are used toguide model generation. This framework has the advantages of generatingdiversified patterns, protecting data privacy, combining external knowledge,and improving the quality of generation, which provides an effective method tooptimize the text generation capability of large language models, and make itbetter applied to the field of intelligent dialogue and content generation.", "output": "PatternGPT :A Pattern-Driven Framework for Large Language Model Text Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The TARK conference (Theoretical Aspects of Rationality and Knowledge) is aconference that aims to bring together researchers from a wide variety offields, including computer science, artificial intelligence, game theory,decision theory, philosophy, logic, linguistics, and cognitive science. Itsgoal is to further our understanding of interdisciplinary issues involvingreasoning about rationality and knowledge.Previous conferences have been held biennially around the world since 1986,on the initiative of Joe Halpern (Cornell University). Topics of interestinclude, but are not limited to, semantic models for knowledge, belief,awareness and uncertainty, bounded rationality and resource-bounded reasoning,commonsense epistemic reasoning, epistemic logic, epistemic game theory,knowledge and action, applications of reasoning about knowledge and othermental states, belief revision, computational social choice, algorithmic gametheory, and foundations of multi-agent systems. Information about TARK,including conference proceedings, is available at <a href=\" http URL</a>These proceedings contain the papers that have been accepted for presentationat the Nineteenth Conference on Theoretical Aspects of Rationality andKnowledge (TARK 2023), held between June 28 and June 30, 2023, at theUniversity of Oxford, United Kingdom. The conference website can be found at", "output": "Proceedings Nineteenth conference on Theoretical Aspects of Rationality and Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rise of social media platforms has facilitated the formation of echochambers, which are online spaces where users predominantly encounterviewpoints that reinforce their existing beliefs while excluding dissentingperspectives. This phenomenon significantly hinders information disseminationacross communities and fuels societal polarization. Therefore, it is crucial todevelop methods for quantifying echo chambers. In this paper, we present theEcho Chamber Score (ECS), a novel metric that assesses the cohesion andseparation of user communities by measuring distances between users in theembedding space. In contrast to existing approaches, ECS is able to functionwithout labels for user ideologies and makes no assumptions about the structureof the interaction graph. To facilitate measuring distances between users, wepropose EchoGAE, a self-supervised graph autoencoder-based user embedding modelthat leverages users' posts and the interaction graph to embed them in a mannerthat reflects their ideological similarity. To assess the effectiveness of ECS,we use a Twitter dataset consisting of four topics - two polarizing and twonon-polarizing. Our results showcase ECS's effectiveness as a tool forquantifying echo chambers and shedding light on the dynamics of onlinediscourse.", "output": "Quantifying the Echo Chamber Effect: An Embedding Distance-based Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated radiology report generation aims to generate radiology reports thatcontain rich, fine-grained descriptions of radiology imaging. Compared withimage captioning in the natural image domain, medical images are very similarto each other, with only minor differences in the occurrence of diseases. Giventhe importance of these minor differences in the radiology report, it iscrucial to encourage the model to focus more on the subtle regions of diseaseoccurrence. Secondly, the problem of visual and textual data biases is serious.Not only do normal cases make up the majority of the dataset, but sentencesdescribing areas with pathological changes also constitute only a small part ofthe paragraph. Lastly, generating medical image reports involves the challengeof long text generation, which requires more expertise and empirical trainingin medical knowledge. As a result, the difficulty of generating such reports isincreased. To address these challenges, we propose a disease-oriented retrievalframework that utilizes similar reports as prior knowledge references. Wedesign a factual consistency captioning generator to generate more accurate andfactually consistent disease descriptions. Our framework can find most similarreports for a given disease from the CXR database by retrieving adisease-oriented mask consisting of the position and morphologicalcharacteristics. By referencing the disease-oriented similar report and thevisual features, the factual consistency model can generate a more accurateradiology report.", "output": "Reading Radiology Imaging Like The Radiologist."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the distribution of a fully connected neural network with randomGaussian weights and biases in which the hidden layer widths are proportionalto a large constant $n$. Under mild assumptions on the non-linearity, we obtainquantitative bounds on normal approximations valid at large but finite $n$ andany fixed network depth. Our theorems show both for the finite-dimensionaldistributions and the entire process, that the distance between a random fullyconnected network (and its derivatives) to the corresponding infinite widthGaussian process scales like $n^{-gamma}$ for $gamma&gt;0$, with the exponentdepending on the metric used to measure discrepancy. Our bounds are strictlystronger in terms of their dependence on network width than any previouslyavailable in the literature; in the one-dimensional case, we also prove thatthey are optimal, i.e., we establish matching lower bounds.", "output": "Quantitative CLTs in Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robust reinforcement learning (RL) aims to find a policy that optimizes theworst-case performance in the face of uncertainties. In this paper, we focus onaction robust RL with the probabilistic policy execution uncertainty, in which,instead of always carrying out the action specified by the policy, the agentwill take the action specified by the policy with probability $1-rho$ and analternative adversarial action with probability $rho$. We establish theexistence of an optimal policy on the action robust MDPs with probabilisticpolicy execution uncertainty and provide the action robust Bellman optimalityequation for its solution. Furthermore, we develop Action Robust ReinforcementLearning with Certificates (ARRLC) algorithm that achieves minimax optimalregret and sample complexity. Furthermore, we conduct numerical experiments tovalidate our approach's robustness, demonstrating that ARRLC outperformsnon-robust RL algorithms and converges faster than the robust TD algorithm inthe presence of action perturbations.", "output": "Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We contribute to the sparsely populated area of unsupervised deep graphmatching with application to keypoint matching in images. Contrary to thestandard emph{supervised} approach, our method does not require ground truthcorrespondences between keypoint pairs. Instead, it is self-supervised byenforcing consistency of matchings between images of the same object category.As the matching and the consistency loss are discrete, their derivatives cannotbe straightforwardly used for learning. We address this issue in a principledway by building our method upon the recent results on black-box differentiationof combinatorial solvers. This makes our method exceptionally flexible, as itis compatible with arbitrary network architectures and combinatorial solvers.Our experimental evaluation suggests that our technique sets a newstate-of-the-art for unsupervised graph matching.", "output": "Unsupervised Deep Graph Matching Based on Cycle Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Talking head video generation aims to animate a human face in a still imagewith dynamic poses and expressions using motion information derived from atarget-driving video, while maintaining the person's identity in the sourceimage. However, dramatic and complex motions in the driving video causeambiguous generation, because the still source image cannot provide sufficientappearance information for occluded regions or delicate expression variations,which produces severe artifacts and significantly degrades the generationquality. To tackle this problem, we propose to learn a global facialrepresentation space, and design a novel implicit identity representationconditioned memory compensation network, coined as MCNet, for high-fidelitytalking head generation.~Specifically, we devise a network module to learn aunified spatial facial meta-memory bank from all training samples, which canprovide rich facial structure and appearance priors to compensate warped sourcefacial features for the generation. Furthermore, we propose an effective querymechanism based on implicit identity representations learned from the discretekeypoints of the source image. It can greatly facilitate the retrieval of morecorrelated information from the memory bank for the compensation. Extensiveexperiments demonstrate that MCNet can learn representative and complementaryfacial memory, and can clearly outperform previous state-of-the-art talkinghead generation methods on VoxCeleb1 and CelebV datasets. Please check ourhref{", "output": "Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite advancements in conversational AI, language models encounterchallenges to handle diverse conversational tasks, and existing dialoguedataset collections often lack diversity and comprehensiveness. To tackle theseissues, we introduce DialogStudio: the largest and most diverse collection ofdialogue datasets, unified under a consistent format while preserving theiroriginal information. Our collection encompasses data from open-domaindialogues, task-oriented dialogues, natural language understanding,conversational recommendation, dialogue summarization, and knowledge-groundeddialogues, making it an incredibly rich and diverse resource for dialogueresearch and model training. To further enhance the utility of DialogStudio, weidentify the licenses for each dataset and design domain-aware prompts forselected dialogues to facilitate instruction-aware fine-tuning. Furthermore, wedevelop conversational AI models using the dataset collection, and ourexperiments in both zero-shot and few-shot learning scenarios demonstrate thesuperiority of DialogStudio. To improve transparency and support dataset andtask-based research, as well as language model pre-training, all datasets,licenses, codes, and models associated with DialogStudio are made publiclyaccessible at ", "output": "DialogStudio: Towards Richest and Most Diverse Unified Dataset Collection for Conversational AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study aims to develop and evaluate an innovative simulation algorithmfor generating thick-slice CT images that closely resemble actual images in theAAPM-Mayo's 2016 Low Dose CT Grand Challenge dataset. The proposed method wasevaluated using Peak Signal-to-Noise Ratio (PSNR) and Root Mean Square Error(RMSE) metrics, with the hypothesis that our simulation would produce imagesmore congruent with their real counterparts. Our proposed method demonstratedsubstantial enhancements in terms of both PSNR and RMSE over other simulationmethods. The highest PSNR values were obtained with the proposed method,yielding 49.7369 $pm$ 2.5223 and 48.5801 $pm$ 7.3271 for D45 and B30reconstruction kernels, respectively. The proposed method also registered thelowest RMSE with values of 0.0068 $pm$ 0.0020 and 0.0108 $pm$ 0.0099 for D45and B30, respectively, indicating a distribution more closely aligned with theauthentic thick-slice image. Further validation of the proposed simulationalgorithm was conducted using the TCIA LDCT-and-Projection-data dataset. Thegenerated images were then leveraged to train four distinct super-resolution(SR) models, which were subsequently evaluated using the real thick-sliceimages from the 2016 Low Dose CT Grand Challenge dataset. When trained withdata produced by our novel algorithm, all four SR models exhibited enhancedperformance.", "output": "Enhancing Super-Resolution Networks through Realistic Thick-Slice CT Simulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual emergence is the phenomenon in which the visual system obtains aholistic perception after grouping and reorganizing local signals. The pictureDalmatian dog is known for its use in explaining visual emergence. This type ofimage, which consists of a set of discrete black speckles (speckles), is calledan emerging image. Not everyone can find the dog in Dalmatian dog, and amongthose who can, the time spent varies greatly. Although Gestalt theorysummarizes perceptual organization into several principles, it remainsambiguous how these principles affect the perception of emerging images. Thisstudy, therefore, designed three psychological experiments to explore thefactors that influence the perception of emerging images. In the first, wefound that the density of speckles in the local area and the arrangements ofsome key speckles played a key role in the perception of an emerging case. Weset parameters in the algorithm to characterize these two factors. We thenautomatically generated diversified emerging-test images (ETIs) through thealgorithm and verified their effectiveness in two subsequent experiments.", "output": "Important Clues that Facilitate Visual Emergence: Three Psychological Experiments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hand tracking is a critical component of natural user interactions inextended reality (XR) environments, including extended reality musicalinstruments (XRMIs). However, self-occlusion remains a significant challengefor vision-based hand tracking systems, leading to inaccurate results anddegraded user experiences. In this paper, we propose a multimodal hand trackingsystem that combines vision-based hand tracking with surface electromyography(sEMG) data for finger joint angle estimation. We validate the effectiveness ofour system through a series of hand pose tasks designed to cover a wide rangeof gestures, including those prone to self-occlusion. By comparing theperformance of our multimodal system to a baseline vision-based trackingmethod, we demonstrate that our multimodal approach significantly improvestracking accuracy for several finger joints prone to self-occlusion. Thesefindings suggest that our system has the potential to enhance XR experiences byproviding more accurate and robust hand tracking, even in the presence ofself-occlusion.", "output": "Combining Vision and EMG-Based Hand Tracking for Extended Reality Musical Instruments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study adversarial training on datasets that obey thelong-tailed distribution, which is practical but rarely explored in previousworks. Compared with conventional adversarial training on balanced datasets,this process falls into the dilemma of generating uneven adversarial examples(AEs) and an unbalanced feature embedding space, causing the resulting model toexhibit low robustness and accuracy on tail data. To combat that, we propose anew adversarial training framework -- Re-balancing Adversarial Training (REAT).This framework consists of two components: (1) a new training strategy inspiredby the term effective number to guide the model to generate more balanced andinformative AEs; (2) a carefully constructed penalty function to force asatisfactory feature space. Evaluation results on different datasets and modelstructures prove that REAT can effectively enhance the model's robustness andpreserve the model's clean accuracy. The code can be found in", "output": "Adversarial Training Over Long-Tailed Distribution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The primal sketch is a fundamental representation in Marr's vision theory,which allows for parsimonious image-level processing from 2D to 2.5Dperception. This paper takes a further step by computing 3D primal sketch ofwireframes from a set of images with known camera poses, in which we take the2D wireframes in multi-view images as the basis to compute 3D wireframes in avolumetric rendering formulation. In our method, we first propose a NEuralAttraction (NEAT) Fields that parameterizes the 3D line segments withcoordinate Multi-Layer Perceptrons (MLPs), enabling us to learn the 3D linesegments from 2D observation without incurring any explicit featurecorrespondences across views. We then present a novel Global JunctionPerceiving (GJP) module to perceive meaningful 3D junctions from the NEATFields of 3D line segments by optimizing a randomly initializedhigh-dimensional latent array and a lightweight decoding MLP. Benefitting fromour explicit modeling of 3D junctions, we finally compute the primal sketch of3D wireframes by attracting the queried 3D line segments to the 3D junctions,significantly simplifying the computation paradigm of 3D wireframe parsing. Inexperiments, we evaluate our approach on the DTU and BlendedMVS datasets withpromising performance obtained. As far as we know, our method is the firstapproach to achieve high-fidelity 3D wireframe parsing without requiringexplicit matching.", "output": "Volumetric Wireframe Parsing from Neural Attraction Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study introduces \"shortcut routing,\" a novel routing mechanism incapsule networks that addresses computational inefficiencies by directlyactivating global capsules from local capsules, eliminating intermediatelayers. An attention-based approach with fuzzy coefficients is also exploredfor improved efficiency. Experimental results on Mnist, smallnorb, and affNistdatasets show comparable classification performance, achieving accuracies of99.52%, 93.91%, and 89.02% respectively. The proposed fuzzy-based andattention-based routing methods significantly reduce the number of calculationsby 1.42 and 2.5 times compared to EM routing, highlighting their computationaladvantages in capsule networks. These findings contribute to the advancement ofefficient and accurate hierarchical pattern representation models.", "output": "Capsule network with shortcut routing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning-based multi-view stereo (MVS) has by far centered around 3Dconvolution on cost volumes. Due to the high computation and memory consumptionof 3D CNN, the resolution of output depth is often considerably limited.Different from most existing works dedicated to adaptive refinement of costvolumes, we opt to directly optimize the depth value along each camera ray,mimicking the range finding of a laser scanner. This reduces the MVS problem toray-based depth optimization which is much more light-weight than full costvolume optimization. In particular, we propose RayMVSNet which learnssequential prediction of a 1D implicit field along each camera ray with thezero-crossing point indicating scene depth. This sequential modeling, conductedbased on transformer features, essentially learns the epipolar line search intraditional multi-view stereo. We devise a multi-task learning for betteroptimization convergence and depth accuracy. We found the monotonicity propertyof the SDFs along each ray greatly benefits the depth estimation. Our methodranks top on both the DTU and the Tanks &amp; Temples datasets over all previouslearning-based methods, achieving an overall reconstruction score of 0.33mm onDTU and an F-score of 59.48% on Tanks &amp; Temples. It is able to producehigh-quality depth estimation and point cloud reconstruction in challengingscenarios such as objects/scenes with non-textured surface, severe occlusion,and highly varying depth range. Further, we propose RayMVSNet++ to enhancecontextual feature aggregation for each ray through designing an attentionalgating unit to select semantically relevant neighboring rays within the localfrustum around that ray. RayMVSNet++ achieves state-of-the-art performance onthe ScanNet dataset. In particular, it attains an AbsRel of 0.058m and producesaccurate results on the two subsets of textureless regions and large depthvariation.", "output": "RayMVSNet++: Learning Ray-based 1D Implicit Fields for Accurate Multi-View Stereo."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual recognition models are not invariant to viewpoint changes in the 3Dworld, as different viewing directions can dramatically affect the predictionsgiven the same object. Although many efforts have been devoted to making neuralnetworks invariant to 2D image translations and rotations, viewpoint invarianceis rarely investigated. As most models process images in the perspective view,it is challenging to impose invariance to 3D viewpoint changes based only on 2Dinputs. Motivated by the success of adversarial training in promoting modelrobustness, we propose Viewpoint-Invariant Adversarial Training (VIAT) toimprove viewpoint robustness of common image classifiers. By regardingviewpoint transformation as an attack, VIAT is formulated as a minimaxoptimization problem, where the inner maximization characterizes diverseadversarial viewpoints by learning a Gaussian mixture distribution based on anew attack GMVFool, while the outer minimization trains a viewpoint-invariantclassifier by minimizing the expected loss over the worst-case adversarialviewpoint distributions. To further improve the generalization performance, adistribution sharing strategy is introduced leveraging the transferability ofadversarial viewpoints across objects. Experiments validate the effectivenessof VIAT in improving the viewpoint robustness of various image classifiersbased on the diversity of adversarial viewpoints generated by GMVFool.", "output": "Towards Viewpoint-Invariant Visual Recognition via Adversarial Training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face recognition from image sets acquired under unregulated and uncontrolledsettings, such as at large distances, low resolutions, varying viewpoints,illumination, pose, and atmospheric conditions, is challenging. Face featureaggregation, which involves aggregating a set of N feature representationspresent in a template into a single global representation, plays a pivotal rolein such recognition systems. Existing works in traditional face featureaggregation either utilize metadata or high-dimensional intermediate featurerepresentations to estimate feature quality for aggregation. However,generating high-quality metadata or style information is not feasible forextremely low-resolution faces captured in long-range and high altitudesettings. To overcome these limitations, we propose a feature distributionconditioning approach called CoNAN for template aggregation. Specifically, ourmethod aims to learn a context vector conditioned over the distributioninformation of the incoming feature set, which is utilized to weigh thefeatures based on their estimated informativeness. The proposed method producesstate-of-the-art results on long-range unconstrained face recognition datasetssuch as BTS, and DroneSURF, validating the advantages of such an aggregationstrategy.", "output": "CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How does the brain represent different modes of information? Can we design asystem that automatically understands what the user is thinking? Such questionscan be answered by studying brain recordings like functional magnetic resonanceimaging (fMRI). As a first step, the neuroscience community has contributedseveral large cognitive neuroscience datasets related to passivereading/listening/viewing of concept words, narratives, pictures and movies.Encoding and decoding models using these datasets have also been proposed inthe past two decades. These models serve as additional tools for basic researchin cognitive science and neuroscience. Encoding models aim at generating fMRIbrain representations given a stimulus automatically. They have severalpractical applications in evaluating and diagnosing neurological conditions andthus also help design therapies for brain damage. Decoding models solve theinverse problem of reconstructing the stimuli given the fMRI. They are usefulfor designing brain-machine or brain-computer interfaces. Inspired by theeffectiveness of deep learning models for natural language processing, computervision, and speech, recently several neural encoding and decoding models havebeen proposed. In this survey, we will first discuss popular representations oflanguage, vision and speech stimuli, and present a summary of neurosciencedatasets. Further, we will review popular deep learning based encoding anddecoding architectures and note their benefits and limitations. Finally, wewill conclude with a brief summary and discussion about future trends. Giventhe large amount of recently published work in the `computational cognitiveneuroscience' community, we believe that this survey nicely organizes theplethora of work and presents it as a coherent story.", "output": "Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While LiDAR sensors have been succesfully applied to 3D object detection, theaffordability of radar and camera sensors has led to a growing interest infusiong radars and cameras for 3D object detection. However, previousradar-camera fusion models have not been able to fully utilize radarinformation in that initial 3D proposals were generated based on the camerafeatures only and the instance-level fusion is subsequently conducted. In thispaper, we propose radar-camera multi-level fusion (RCM-Fusion), which fusesradar and camera modalities at both the feature-level and instance-level tofully utilize radar information. At the feature-level, we propose a RadarGuided BEV Encoder which utilizes radar Bird's-Eye-View (BEV) features totransform image features into precise BEV representations and then adaptivelycombines the radar and camera BEV features. At the instance-level, we propose aRadar Grid Point Refinement module that reduces localization error byconsidering the characteristics of the radar point clouds. The experimentsconducted on the public nuScenes dataset demonstrate that our proposedRCM-Fusion offers 11.8% performance gain in nuScenes detection score (NDS) overthe camera-only baseline model and achieves state-of-the-art performaces amongradar-camera fusion methods in the nuScenes 3D object detection benchmark. Codewill be made publicly available.", "output": "RCM-Fusion: Radar-Camera Multi-Level Fusion for 3D Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Aphid infestations can cause extensive damage to wheat and sorghum fields andspread plant viruses, resulting in significant yield losses in agriculture. Toaddress this issue, farmers often rely on chemical pesticides, which areinefficiently applied over large areas of fields. As a result, a considerableamount of pesticide is wasted on areas without pests, while inadequate amountsare applied to areas with severe infestations. The paper focuses on the urgentneed for an intelligent autonomous system that can locate and sprayinfestations within complex crop canopies, reducing pesticide use andenvironmental impact. We have collected and labeled a large aphid image datasetin the field, and propose the use of real-time semantic segmentation models tosegment clusters of aphids. A multiscale dataset is generated to allow forlearning the clusters at different scales. We compare the segmentation speedsand accuracy of four state-of-the-art real-time semantic segmentation models onthe aphid cluster dataset, benchmarking them against nonreal-time models. Thestudy results show the effectiveness of a real-time solution, which can reduceinefficient pesticide use and increase crop yields, paving the way towards anautonomous pest detection system.", "output": "On the Real-Time Semantic Segmentation of Aphid Clusters in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image synthesis has attracted emerging research interests in academic andindustry communities. Deep learning technologies especially the generativemodels greatly inspired controllable image synthesis approaches andapplications, which aim to generate particular visual contents with latentprompts. In order to further investigate low-level controllable image synthesisproblem which is crucial for fine image rendering and editing tasks, we presenta survey of some recent works on 3D controllable image synthesis using deeplearning. We first introduce the datasets and evaluation indicators for 3Dcontrollable image synthesis. Then, we review the state-of-the-art research forgeometrically controllable image synthesis in two aspects: 1)Viewpoint/pose-controllable image synthesis; 2) Structure/shape-controllableimage synthesis. Furthermore, the photometrically controllable image synthesisapproaches are also reviewed for 3D re-lighting researches. While the emphasisis on 3D controllable image synthesis algorithms, the related applications,products and resources are also briefly summarized for practitioners.", "output": "Survey on Controlable Image Synthesis with Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce two novel visualization designs to support practitioners inperforming identification and discrimination tasks on large value ranges (i.e.,several orders of magnitude) in time-series data: (1) The order of magnitudehorizon graph, which extends the classic horizon graph; and (2) the order ofmagnitude line chart, which adapts the log-line chart. These new visualizationdesigns visualize large value ranges by explicitly splitting the mantissa m andexponent e of a value v = m * 10e . We evaluate our novel designs against themost relevant state-of-the-art visualizations in an empirical user study. Itfocuses on four main tasks commonly employed in the analysis of time-series andlarge value ranges visualization: identification, discrimination, estimation,and trend detection. For each task we analyse error, confidence, and responsetime. The new order of magnitude horizon graph performs better or equal to allother designs in identification, discrimination, and estimation tasks. Only fortrend detection tasks, the more traditional horizon graphs reported betterperformance. Our results are domain-independent, only requiring time-seriesdata with large value ranges.", "output": "Reclaiming the Horizon: Novel Visualization Designs for Time-Series Data with Large Value Ranges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The performance of face photo-sketch translation has improved a lot thanks todeep neural networks. GAN based methods trained on paired images can producehigh-quality results under laboratory settings. Such paired datasets are,however, often very small and lack diversity. Meanwhile, Cycle-GANs trainedwith unpaired photo-sketch datasets suffer from the emph{steganography}phenomenon, which makes them not effective to face photos in the wild. In thispaper, we introduce a semi-supervised approach with a noise-injection strategy,named Semi-Cycle-GAN (SCG), to tackle these problems. For the first problem, wepropose a {em pseudo sketch feature} representation for each input photocomposed from a small reference set of photo-sketch pairs, and use theresulting {em pseudo pairs} to supervise a photo-to-sketch generator$G_{p2s}$. The outputs of $G_{p2s}$ can in turn help to train a sketch-to-photogenerator $G_{s2p}$ in a self-supervised manner. This allows us to train$G_{p2s}$ and $G_{s2p}$ using a small reference set of photo-sketch pairstogether with a large face photo dataset (without ground-truth sketches). Forthe second problem, we show that the simple noise-injection strategy works wellto alleviate the emph{steganography} effect in SCG and helps to produce morereasonable sketch-to-photo results with less overfitting than fully supervisedapproaches. Experiments show that SCG achieves competitive performance onpublic benchmarks and superior results on photos in the wild.", "output": "Semi-supervised Cycle-GAN for face photo-sketch translation in the wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present ECSIC, a novel learned method for stereo imagecompression. Our proposed method compresses the left and right images in ajoint manner by exploiting the mutual information between the images of thestereo image pair using a novel stereo cross attention (SCA) module and twostereo context modules. The SCA module performs cross-attention restricted tothe corresponding epipolar lines of the two images and processes them inparallel. The stereo context modules improve the entropy estimation of thesecond encoded image by using the first image as a context. We conduct anextensive ablation study demonstrating the effectiveness of the proposedmodules and a comprehensive quantitative and qualitative comparison withexisting methods. ECSIC achieves state-of-the-art performance among stereoimage compression models on the two popular stereo image datasets Cityscapesand InStereo2k while allowing for fast encoding and decoding, making it highlypractical for real-time applications.", "output": "ECSIC: Epipolar Cross Attention for Stereo Image Compression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mammography images are widely used to detect non-palpable breast lesions ornodules, preventing cancer and providing the opportunity to plan interventionswhen necessary. The identification of some structures of interest is essentialto make a diagnosis and evaluate image adequacy. Thus, computer-aided detectionsystems can be helpful in assisting medical interpretation by automaticallysegmenting these landmark structures. In this paper, we propose a deeplearning-based framework for the segmentation of the nipple, the pectoralmuscle, the fibroglandular tissue, and the fatty tissue on standard-viewmammography images. We introduce a large private segmentation dataset andextensive experiments considering different deep-learning model architectures.Our experiments demonstrate accurate segmentation performance on variate andchallenging cases, showing that this framework can be integrated into clinicalpractice.", "output": "Towards Automated Semantic Segmentation in Mammography Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human beings always engage in a vast range of activities and tasks thatdemonstrate their ability to adapt to different scenarios. Any human activitycan be represented as a temporal sequence of actions performed to achieve acertain goal. Unlike the time series datasets extracted from electronics ormachines, these action sequences are highly disparate in their nature -- thetime to finish a sequence of actions can vary between different persons.Therefore, understanding the dynamics of these sequences is essential for manydownstream tasks such as activity length prediction, goal prediction, nextaction recommendation, etc. Existing neural network-based approaches that learna continuous-time activity sequence (or CTAS) are limited to the presence ofonly visual data or are designed specifically for a particular task, i.e.,limited to next action or goal prediction. In this paper, we present ProActive,a neural marked temporal point process (MTPP) framework for modeling thecontinuous-time distribution of actions in an activity sequence whilesimultaneously addressing three high-impact problems -- next action prediction,sequence-goal prediction, and end-to-end sequence generation. Specifically, weutilize a self-attention module with temporal normalizing flows to model theinfluence and the inter-arrival times between actions in a sequence. Inaddition, we propose a novel addition over the ProActive model that can handlevariations in the order of actions, i.e., different methods of achieving agiven goal. We demonstrate that this variant can learn the order in which theperson or actor prefers to do their actions. Extensive experiments on sequencesderived from three activity recognition datasets show the significant accuracyboost of ProActive over the state-of-the-art in terms of action and goalprediction, and the first-ever application of end-to-end action sequencegeneration.", "output": "Tapestry of Time and Actions: Modeling Human Activity Sequences using Temporal Point Process Flows."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the task of weakly-supervised point cloud semantic segmentation withsparse annotations (e.g., less than 0.1% points are labeled), aiming to reducethe expensive cost of dense annotations. Unfortunately, with extremely sparseannotated points, it is very difficult to extract both contextual and objectinformation for scene understanding such as semantic segmentation. Motivated bymasked modeling (e.g., MAE) in image and video representation learning, we seekto endow the power of masked modeling to learn contextual information fromsparsely-annotated points. However, directly applying MAE to 3D point cloudswith sparse annotations may fail to work. First, it is nontrivial toeffectively mask out the informative visual context from 3D point clouds.Second, how to fully exploit the sparse annotations for context modelingremains an open question. In this paper, we propose a simple yet effectiveContextual Point Cloud Modeling (CPCM) method that consists of two parts: aregion-wise masking (RegionMask) strategy and a contextual masked training(CMT) method. Specifically, RegionMask masks the point cloud continuously ingeometric space to construct a meaningful masked prediction task for subsequentcontext learning. CMT disentangles the learning of supervised segmentation andunsupervised masked context prediction for effectively learning the verylimited labeled points and mass unlabeled points, respectively. Extensiveexperiments on the widely-tested ScanNet V2 and S3DIS benchmarks demonstratethe superiority of CPCM over the state-of-the-art.", "output": "CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Exposure to disturbing imagery can significantly impact individuals,especially professionals who encounter such content as part of their work. Thispaper presents a user study, involving 107 participants, predominantlyjournalists and human rights investigators, that explores the capability ofArtificial Intelligence (AI)-based image filters to potentially mitigate theemotional impact of viewing such disturbing content. We tested five differentfilter styles, both traditional (Blurring and Partial Blurring) and AI-based(Drawing, Colored Drawing, and Painting), and measured their effectiveness interms of conveying image information while reducing emotional distress. Ourfindings suggest that the AI-based Drawing style filter demonstrates the bestperformance, offering a promising solution for reducing negative feelings(-30.38%) while preserving the interpretability of the image (97.19%). Despitethe requirement for many professionals to eventually inspect the originalimages, participants suggested potential strategies for integrating AI filtersinto their workflow, such as using AI filters as an initial, preparatory stepbefore viewing the original image. Overall, this paper contributes to thedevelopment of a more ethically considerate and effective visual environmentfor professionals routinely engaging with potentially disturbing imagery.", "output": "Mitigating Viewer Impact from Disturbing Imagery using AI Filters: A User-Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Massive web datasets play a key role in the success of large vision-languagemodels like CLIP and Flamingo. However, the raw web data is noisy, and existingfiltering methods to reduce noise often come at the expense of data diversity.Our work focuses on caption quality as one major source of noise, and studieshow generated captions can increase the utility of web-scraped datapoints withnondescript text. Through exploring different mixing strategies for raw andgenerated captions, we outperform the best filtering method proposed by theDataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given acandidate pool of 128M image-text pairs. Our best approach is also 2x better atFlickr and MS-COCO retrieval. We then analyze what makes synthetic captions aneffective source of text supervision. In experimenting with different imagecaptioning models, we also demonstrate that the performance of a model onstandard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliableindicator of the utility of the captions it generates for multimodal training.Finally, our experiments with using generated captions at DataComp's largescale (1.28B image-text pairs) offer insights into the limitations of synthetictext, as well as the importance of image curation with increasing training dataquantity.", "output": "Improving Multimodal Datasets with Image Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The generative AI revolution has recently expanded to videos. Nevertheless,current state-of-the-art video models are still lagging behind image models interms of visual quality and user control over the generated content. In thiswork, we present a framework that harnesses the power of a text-to-imagediffusion model for the task of text-driven video editing. Specifically, givena source video and a target text-prompt, our method generates a high-qualityvideo that adheres to the target text, while preserving the spatial layout andmotion of the input video. Our method is based on a key observation thatconsistency in the edited video can be obtained by enforcing consistency in thediffusion feature space. We achieve this by explicitly propagating diffusionfeatures based on inter-frame correspondences, readily available in the model.Thus, our framework does not require any training or fine-tuning, and can workin conjunction with any off-the-shelf text-to-image editing method. Wedemonstrate state-of-the-art editing results on a variety of real-world videos.Webpage: ", "output": "TokenFlow: Consistent Diffusion Features for Consistent Video Editing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The surgical usage of Mixed Reality (MR) has received growing attention inareas such as surgical navigation systems, skill assessment, and robot-assistedsurgeries. For such applications, pose estimation for hand and surgicalinstruments from an egocentric perspective is a fundamental task and has beenstudied extensively in the computer vision field in recent years. However, thedevelopment of this field has been impeded by a lack of datasets, especially inthe surgical field, where bloody gloves and reflective metallic tools make ithard to obtain 3D pose annotations for hands and objects using conventionalmethods. To address this issue, we propose POV-Surgery, a large-scale,synthetic, egocentric dataset focusing on pose estimation for hands withdifferent surgical gloves and three orthopedic surgical instruments, namelyscalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329frames, featuring high-resolution RGB-D video streams with activityannotations, accurate 3D and 2D annotations for hand-object pose, and 2Dhand-object segmentation masks. We fine-tune the current SOTA methods onPOV-Surgery and further show the generalizability when applying to real-lifecases with surgical gloves and tools by extensive evaluations. The code and thedataset are publicly available at batfacewayne.github.io/POV_Surgery_io/.", "output": "POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation During Surgical Activities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Part-prototype models are explainable-by-design image classifiers, and apromising alternative to black box AI. This paper explores the applicabilityand potential of interpretable machine learning, in particular PIP-Net, forautomated diagnosis support on real-world medical imaging data. PIP-Net learnshuman-understandable prototypical image parts and we evaluate its accuracy andinterpretability for fracture detection and skin cancer diagnosis. We find thatPIP-Net's decision making process is in line with medical classificationstandards, while only provided with image-level class labels. Because ofPIP-Net's unsupervised pretraining of prototypes, data quality problems such asundesired text in an X-ray or labelling errors can be easily identified.Additionally, we are the first to show that humans can manually correct thereasoning of PIP-Net by directly disabling undesired prototypes. We concludethat part-prototype models are promising for medical applications due to theirinterpretability and potential for advanced model debugging.", "output": "Interpreting and Correcting Medical Image Classification with PIP-Net."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-modal tasks involving vision and language in deep learning continue torise in popularity and are leading to the development of newer models that cangeneralize beyond the extent of their training data. The current models lacktemporal generalization which enables models to adapt to changes in futuredata. This paper discusses a viable approach to creating an advanced VisualQuestion Answering (VQA) model which can produce successful results on temporalgeneralization. We propose a new data set, GenVQA, utilizing images andcaptions from the VQAv2 and MS-COCO dataset to generate new images throughstable diffusion. This augmented dataset is then used to test a combination ofseven baseline and cutting edge VQA models. Performance evaluation focuses onquestions mirroring the original VQAv2 dataset, with the answers having beenadjusted to the new images. This paper's purpose is to investigate therobustness of several successful VQA models to assess their performance onfuture data distributions. Model architectures are analyzed to identify commonstylistic choices that improve generalization under temporal distributionshifts. This research highlights the importance of creating a large-scalefuture shifted dataset. This data can enhance the robustness of VQA models,allowing their future peers to have improved ability to adapt to temporaldistribution shifts.", "output": "Generative Visual Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The end-to-end learning ability of self-driving vehicles has achievedsignificant milestones over the last decade owing to rapid advances in deeplearning and computer vision algorithms. However, as autonomous drivingtechnology is a safety-critical application of artificial intelligence (AI),road accidents and established regulatory principles necessitate the need forthe explainability of intelligent action choices for self-driving vehicles. Tofacilitate interpretability of decision-making in autonomous driving, wepresent a Visual Question Answering (VQA) framework, which explains drivingactions with question-answering-based causal reasoning. To do so, we firstcollect driving videos in a simulation environment using reinforcement learning(RL) and extract consecutive frames from this log data uniformly for fiveselected action categories. Further, we manually annotate the extracted framesusing question-answer pairs as justifications for the actions chosen in eachscenario. Finally, we evaluate the correctness of the VQA-predicted answers foractions on unseen driving scenes. The empirical results suggest that the VQAmechanism can provide support to interpret real-time decisions of autonomousvehicles and help enhance overall driving safety.", "output": "Explaining Autonomous Driving Actions with Visual Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Earth system forecasting has traditionally relied on complex physical modelsthat are computationally expensive and require significant domain expertise. Inthe past decade, the unprecedented increase in spatiotemporal Earth observationdata has enabled data-driven forecasting models using deep learning techniques.These models have shown promise for diverse Earth system forecasting tasks buteither struggle with handling uncertainty or neglect domain-specific priorknowledge, resulting in averaging possible futures to blurred forecasts orgenerating physically implausible predictions. To address these limitations, wepropose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1)We develop PreDiff, a conditional latent diffusion model capable ofprobabilistic forecasts. 2) We incorporate an explicit knowledge controlmechanism to align forecasts with domain-specific physical constraints. This isachieved by estimating the deviation from imposed constraints at each denoisingstep and adjusting the transition distribution accordingly. We conductempirical studies on two datasets: N-body MNIST, a synthetic dataset withchaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset.Specifically, we impose the law of conservation of energy in N-body MNIST andanticipated precipitation intensity in SEVIR. Experiments demonstrate theeffectiveness of PreDiff in handling uncertainty, incorporating domain-specificprior knowledge, and generating forecasts that exhibit high operationalutility.", "output": "PreDiff: Precipitation Nowcasting with Latent Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Overconfidence is a common issue for deep neural networks, limiting theirdeployment in real-world applications. To better estimate confidence, existingmethods mostly focus on fully-supervised scenarios and rely on training labels.In this paper, we propose the first confidence estimation method for asemi-supervised setting, when most training labels are unavailable. Westipulate that even with limited training labels, we can still reasonablyapproximate the confidence of model on unlabeled samples by inspecting theprediction consistency through the training process. We use trainingconsistency as a surrogate function and propose a consistency ranking loss forconfidence estimation. On both image classification and segmentation tasks, ourmethod achieves state-of-the-art performances in confidence estimation.Furthermore, we show the benefit of the proposed method through a downstreamactive learning task. The code is available at", "output": "Confidence Estimation Using Unlabeled Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In an effort to catalog insect biodiversity, we propose a new large datasetof hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record istaxonomically classified by an expert, and also has associated geneticinformation including raw nucleotide barcode sequences and assigned barcodeindex numbers, which are genetically-based proxies for species classification.This paper presents a curated million-image dataset, primarily to traincomputer-vision models capable of providing image-based taxonomic assessment,however, the dataset also presents compelling characteristics, the study ofwhich would be of interest to the broader machine learning community. Driven bythe biological nature inherent to the dataset, a characteristic long-tailedclass-imbalance distribution is exhibited. Furthermore, taxonomic labelling isa hierarchical classification scheme, presenting a highly fine-grainedclassification problem at lower levels. Beyond spurring interest inbiodiversity research within the machine learning community, progress oncreating an image-based taxonomic classifier will also further the ultimategoal of all BIOSCAN research: to lay the foundation for a comprehensive surveyof global biodiversity. This paper introduces the dataset and explores theclassification task through the implementation and analysis of a baselineclassifier.", "output": "A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the swift growth of patent applications each year, information andmultimedia retrieval approaches that facilitate patent exploration andretrieval are of utmost importance. Different types of visualizations (e.g.,graphs, technical drawings) and perspectives (e.g., side view, perspective) areused to visualize details of innovations in patents. The classification ofthese images enables a more efficient search and allows for further analysis.So far, datasets for image type classification miss some importantvisualization types for patents. Furthermore, related work does not make use ofrecent deep learning approaches including transformers. In this paper, we adoptstate-of-the-art deep learning methods for the classification of visualizationtypes and perspectives in patent images. We extend the CLEF-IP dataset forimage type classification in patents to ten classes and provide manual groundtruth annotations. In addition, we derive a set of hierarchical classes from adataset that provides weakly-labeled data for image perspectives. Experimentalresults have demonstrated the feasibility of the proposed approaches. Sourcecode, models, and dataset will be made publicly available.", "output": "Classification of Visualization Types and Perspectives in Patents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With social media usage growing exponentially in the past few years, fakenews has also become extremely prevalent. The detrimental impact of fake newsemphasizes the need for research focused on automating the detection of falseinformation and verifying its accuracy. In this work, we present the outcome ofthe Factify 2 shared task, which provides a multi-modal fact verification andsatire news dataset, as part of the DeFactify 2 workshop at AAAI'23. The datacalls for a comparison based approach to the task by pairing social mediaclaims with supporting documents, with both text and image, divided into 5classes based on multi-modal relations. In the second iteration of this task wehad over 60 participants and 9 final test-set submissions. The bestperformances came from the use of DeBERTa for text and Swinv2 and CLIP forimage. The highest F1 score averaged for all five classes was 81.82%.", "output": "Findings of Factify 2: Multimodal Fake News Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) have shown unprecedented success in objectdetection tasks. However, it was also discovered that DNNs are vulnerable tomultiple kinds of attacks, including Backdoor Attacks. Through the attack, theattacker manages to embed a hidden backdoor into the DNN such that the modelbehaves normally on benign data samples, but makes attacker-specified judgmentsgiven the occurrence of a predefined trigger. Although numerous backdoorattacks have been experimented on image classification, backdoor attacks onobject detection tasks have not been properly investigated and explored. Asobject detection has been adopted as an important module in multiplesecurity-sensitive applications such as autonomous driving, backdoor attacks onobject detection could pose even more severe threats. Inspired by the inherentproperty of deep learning-based object detectors, we propose a simple yeteffective backdoor attack method against object detection without modifying theground truth annotations, specifically focusing on the object disappearanceattack and object generation attack. Extensive experiments and ablation studiesprove the effectiveness of our attack on two benchmark object detectiondatasets, PASCAL VOC07+12 and MSCOCO, on which we achieve an attack successrate of more than 92% with a poison rate of only 5%.", "output": "Backdoor Attack against Object Detection with Clean Annotation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Active learning improves the performance of machine learning methods byjudiciously selecting a limited number of unlabeled data points to query forlabels, with the aim of maximally improving the underlying classifier'sperformance. Recent gains have been made using sequential active learning forsynthetic aperture radar (SAR) data <a href=\"/abs/2204.00005\">arXiv:2204.00005</a>. In each iteration,sequential active learning selects a query set of size one while batch activelearning selects a query set of multiple datapoints. While batch activelearning methods exhibit greater efficiency, the challenge lies in maintainingmodel accuracy relative to sequential active learning methods. We developed anovel, two-part approach for batch active learning: Dijkstra's Annulus Core-Set(DAC) for core-set generation and LocalMax for batch sampling. The batch activelearning process that combines DAC and LocalMax achieves nearly identicalaccuracy as sequential active learning but is more efficient, proportional tothe batch size. As an application, a pipeline is built based on transferlearning feature embedding, graph learning, DAC, and LocalMax to classify theFUSAR-Ship and OpenSARShip datasets. Our pipeline outperforms thestate-of-the-art CNN-based methods.", "output": "Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human-Object Interaction Detection is a crucial aspect of human-centric sceneunderstanding, with important applications in various domains. Despite recentprogress in this field, recognizing subtle and detailed interactions remainschallenging. Existing methods try to use human-related clues to alleviate thedifficulty, but rely heavily on external annotations or knowledge, limitingtheir practical applicability in real-world scenarios. In this work, we proposea novel Part Semantic Network (PSN) to solve this problem. The core of PSN is aConditional Part Attention (CPA) mechanism, where human features are taken askeys and values, and the object feature is used as query for the computation ina cross-attention mechanism. In this way, our model learns to automaticallyfocus on the most informative human parts conditioned on the involved object,generating more semantically meaningful features for interaction recognition.Additionally, we propose an Occluded Part Extrapolation (OPE) strategy tofacilitate interaction recognition under occluded scenarios, which teaches themodel to extrapolate detailed features from partially occluded ones. Our methodconsistently outperforms prior approaches on the V-COCO and HICO-DET datasets,without external data or extra annotations. Additional ablation studiesvalidate the effectiveness of each component of our proposed method.", "output": "Mining Conditional Part Semantics with Occluded Extrapolation for Human-Object Interaction Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Eye is the essential sense organ for vision function. Due to the fact thatcertain eye disorders might result in vision loss, it is essential to diagnoseand treat eye diseases early on. By identifying common eye illnesses andperforming an eye check, eye care providers can safeguard patients againstvision loss or blindness. Convolutional neural networks (CNN) and transferlearning were employed in this study to discriminate between a normal eye andone with diabetic retinopathy, cataract, or glaucoma disease. Using transferlearning for multi-class classification, high accuracy was achieved at 94%while the traditional CNN achieved 84% rate.", "output": "Eye Disease Classification Using Deep Learning Techniques."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Automatic Feature Explanation using Contrasting Concepts (FALCON),an interpretability framework to explain features of image representations. Fora target feature, FALCON captions its highly activating cropped images using alarge captioning dataset (like LAION-400m) and a pre-trained vision-languagemodel like CLIP. Each word among the captions is scored and ranked leading to asmall number of shared, human-understandable concepts that closely describe thetarget feature. FALCON also applies contrastive interpretation using lowlyactivating (counterfactual) images, to eliminate spurious concepts. Althoughmany existing approaches interpret features independently, we observe instate-of-the-art self-supervised and supervised models, that less than 20% ofthe representation space can be explained by individual features. We show thatfeatures in larger spaces become more interpretable when studied in groups andcan be explained with high-order scoring concepts through FALCON. We discusshow extracted concepts can be used to explain and debug failures in downstreamtasks. Finally, we present a technique to transfer concepts from one(explainable) representation space to another unseen representation space bylearning a simple linear transformation.", "output": "Identifying Interpretable Subspaces in Image Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explainable Deep Learning has gained significant attention in the field ofartificial intelligence (AI), particularly in domains such as medical imaging,where accurate and interpretable machine learning models are crucial foreffective diagnosis and treatment planning. Grad-CAM is a baseline thathighlights the most critical regions of an image used in a deep learningmodel's decision-making process, increasing interpretability and trust in theresults. It is applied in many computer vision (CV) tasks such asclassification and explanation. This study explores the principles ofExplainable Deep Learning and its relevance to medical imaging, discussesvarious explainability techniques and their limitations, and examines medicalimaging applications of Grad-CAM. The findings highlight the potential ofExplainable Deep Learning and Grad-CAM in improving the accuracy andinterpretability of deep learning models in medical imaging. The code isavailable in (will be available).", "output": "Is Grad-CAM Explainable in Medical Images?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cross-silo federated learning (FL) enables the development of machinelearning models on datasets distributed across data centers such as hospitalsand clinical research laboratories. However, recent research has found thatcurrent FL algorithms face a trade-off between local and global performancewhen confronted with distribution shifts. Specifically, personalized FL methodshave a tendency to overfit to local data, leading to a sharp valley in thelocal model and inhibiting its ability to generalize to out-of-distributiondata. In this paper, we propose a novel federated model soup method (i.e.,selective interpolation of model parameters) to optimize the trade-off betweenlocal and global performance. Specifically, during the federated trainingphase, each client maintains its own global model pool by monitoring theperformance of the interpolated model between the local and global models. Thisallows us to alleviate overfitting and seek flat minima, which cansignificantly improve the model's generalization performance. We evaluate ourmethod on retinal and pathological image classification tasks, and our proposedmethod achieves significant improvements for out-of-distributiongeneralization. Our code is available at ", "output": "FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interactive segmentation entails a human marking an image to guide how amodel either creates or edits a segmentation. Our work addresses limitations ofexisting methods: they either only support one gesture type for marking animage (e.g., either clicks or scribbles) or require knowledge of the gesturetype being employed, and require specifying whether marked regions should beincluded versus excluded in the final segmentation. We instead propose asimplified interactive segmentation task where a user only must mark an image,where the input can be of any gesture type without specifying the gesture type.We support this new task by introducing the first interactive segmentationdataset with multiple gesture types as well as a new evaluation metric capableof holistically evaluating interactive segmentation algorithms. We then analyzenumerous interactive segmentation algorithms, including ones adapted for ournovel task. While we observe promising performance overall, we also highlightareas for future improvement. To facilitate further extensions of this work, wepublicly share our new dataset at ", "output": "Interactive Segmentation for Diverse Gesture Types Without Context."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training and deploying the large language models requires a large mount ofcomputational resource because the language models contain billions ofparameters and the text has thousands of tokens. Another problem is that thelarge language models are static. They are fixed after the training process. Totackle these issues, in this paper, we propose to train and deploy the dynamiclarge language model on blockchains, which have high computation performanceand are distributed across a network of computers. A blockchain is a secure,decentralized, and transparent system that allows for the creation of atamper-proof ledger for transactions without the need for intermediaries. Thedynamic large language models can continuously learn from the user input afterthe training process. Our method provides a new way to develop the largelanguage models and also sheds a light on the next generation artificialintelligence systems.", "output": "Dynamic Large Language Models on Blockchains."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mixed-Precision Quantization~(MQ) can achieve a competitiveaccuracy-complexity trade-off for models. Conventional training-based searchmethods require time-consuming candidate training to search optimized per-layerbit-width configurations in MQ. Recently, some training-free approaches havepresented various MQ proxies and significantly improve search efficiency.However, the correlation between these proxies and quantization accuracy ispoorly understood. To address the gap, we first build the MQ-Bench-101, whichinvolves different bit configurations and quantization results. Then, weobserve that the existing training-free proxies perform weak correlations onthe MQ-Bench-101. To efficiently seek superior proxies, we develop an automaticsearch of proxies framework for MQ via evolving algorithms. In particular, wedevise an elaborate search space involving the existing proxies and perform anevolution search to discover the best correlated MQ proxy. We proposed adiversity-prompting selection strategy and compatibility screening protocol toavoid premature convergence and improve search efficiency. In this way, ourEvolving proxies for Mixed-precision Quantization~(EMQ) framework allows theauto-generation of proxies without heavy tuning and expert knowledge. Extensiveexperiments on ImageNet with various ResNet and MobileNet families demonstratethat our EMQ obtains superior performance than state-of-the-art mixed-precisionmethods at a significantly reduced cost. The code will be released.", "output": "EMQ: Evolving Training-free Proxies for Automated Mixed Precision Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Temporal video grounding (TVG) aims to retrieve the time interval of alanguage query from an untrimmed video. A significant challenge in TVG is thelow \"Semantic Noise Ratio (SNR)\", which results in worse performance with lowerSNR. Prior works have addressed this challenge using sophisticated techniques.In this paper, we propose a no-frills TVG model that consists of two coremodules, namely multi-scale neighboring attention and zoom-in boundarydetection. The multi-scale neighboring attention restricts each video token toonly aggregate visual contexts from its neighbor, enabling the extraction ofthe most distinguishing information with multi-scale feature hierarchies fromhigh-ratio noises. The zoom-in boundary detection then focuses on local-wisediscrimination of the selected top candidates for fine-grained groundingadjustment. With an end-to-end training strategy, our model achievescompetitive performance on different TVG benchmarks, while also having theadvantage of faster inference speed and lighter model parameters, thanks to itslightweight architecture.", "output": "No-frills Temporal Video Grounding: Multi-Scale Neighboring Attention and Zoom-in Boundary Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a distributed machine learning technique, federated learning (FL) requiresclients to collaboratively train a shared model with an edge server withoutleaking their local data. However, the heterogeneous data distribution amongclients often leads to a decrease in model performance. To tackle this issue,this paper introduces a prototype-based regularization strategy to address theheterogeneity in the data distribution. Specifically, the regularizationprocess involves the server aggregating local prototypes from distributedclients to generate a global prototype, which is then sent back to theindividual clients to guide their local training. The experimental results onMNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3%and 8.9% in average test accuracy, respectively, compared to the most popularbaseline FedAvg. Furthermore, our approach has a fast convergence rate inheterogeneous settings.", "output": "Boosting Federated Learning Convergence with Prototype Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Traditional computer vision models often require extensive manual effort fordata acquisition and validation, particularly when detecting subtle behavioralnuances or events. The difficulty in distinguishing routine behaviors frompotential risks in real-world applications, like differentiating routineshopping from potential shoplifting, further complicates the process.We present Ethosight, a novel zero-shot computer vision algorithm. Ethosighteradicates the need for pre-existing symbolic knowledge, initiating from aclean slate based on user requirements and semantic knowledge of interest.Using localized label affinity calculations and a reasoning-guided iterativelearning loop, Ethosight infers scene details and iteratively refines the labelset. Reasoning mechanisms can be derived from large language models like GPT4,symbolic reasoners like OpenNARS, or hybrid systems.Ethosight further capitalizes on the capabilities of a pre-trainedmulti-modal model, ImageBind, generating accurate semantic knowledge of imageswithin a few cycles. It successfully captures both explicit and nuancedelements efficiently. We also introduce the implementation of Korzybski's\"time-binding\" concept in machines, which allows for generational learning andknowledge sharing across deployments.Our evaluations demonstrate Ethosight's efficacy across 40 complex use cases.It has exhibited an exceptional ability to discern new areas of interest,consistently generating high-affinity scores within the top five labels from aset of a thousand. Tests conducted across diverse environments attest toEthosight's robust performance. Detailed results and case studies within themain body of this paper and an appendix underscore a promising trajectorytowards enhancing the adaptability and resilience of computer vision models indetecting and extracting subtle and nuanced behaviors.", "output": "Ethosight: A Joint-Embedding Based System for Nuanced Perception Using Contextual Label Affinity Metric and Reasoning Based Iterative Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Have you ever imagined how it would look if we placed new objects intopaintings? For example, what would it look like if we placed a basketball intoClaude Monet's ``Water Lilies, Evening Effect''? We propose Reference-basedPainterly Inpainting, a novel task that crosses the wild reference domain gapand implants novel objects into artworks. Although previous works have examinedreference-based inpainting, they are not designed for large domaindiscrepancies between the target and the reference, such as inpainting anartistic image using a photorealistic reference. This paper proposes a noveldiffusion framework, dubbed RefPaint, to ``inpaint more wildly'' by taking suchreferences with large domain gaps. Built with an image-conditioned diffusionmodel, we introduce a ladder-side branch and a masked fusion mechanism to workwith the inpainting mask. By decomposing the CLIP image embeddings at inferencetime, one can manipulate the strength of semantic and style information withease. Experiments demonstrate that our proposed RefPaint framework producessignificantly better results than existing methods. Our method enables creativepainterly image inpainting with reference objects that would otherwise bedifficult to achieve. Project page: ", "output": "Reference-based Painterly Inpainting via Diffusion: Crossing the Wild Reference Domain Gap."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Event-based cameras have become increasingly popular for tracking fast-movingobjects due to their high temporal resolution, low latency, and high dynamicrange. In this paper, we propose a novel algorithm for tracking event blobsusing raw events asynchronously in real time. We introduce the concept of anevent blob as a spatio-temporal likelihood of event occurrence where theconditional spatial likelihood is blob-like. Many real-world objects generateevent blob data, for example, flickering LEDs such as car headlights or anysmall foreground object moving against a static or slowly varying background.The proposed algorithm uses a nearest neighbour classifier with a dynamicthreshold criteria for data association coupled with a Kalman filter to trackthe event blob state. Our algorithm achieves highly accurate tracking and eventblob shape estimation even under challenging lighting conditions and high-speedmotions. The microsecond time resolution achieved means that the filter outputcan be used to derive secondary information such as time-to-contact or rangeestimation, that will enable applications to real-world problems such ascollision avoidance in autonomous driving.", "output": "Event Blob Tracking: An Asynchronous Real-Time Algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To address 3D object retrieval, substantial efforts have been made togenerate highly discriminative descriptors of 3D objects represented by asingle modality, e.g., voxels, point clouds or multi-view images. It ispromising to leverage the complementary information from multi-modalityrepresentations of 3D objects to further improve retrieval performance.However, multi-modality 3D object retrieval is rarely developed and analyzed onlarge-scale datasets. In this paper, we propose self-and-cross attention basedaggregation of point cloud and multi-view images (SCA-PVNet) for 3D objectretrieval. With deep features extracted from point clouds and multi-viewimages, we design two types of feature aggregation modules, namely theIn-Modality Aggregation Module (IMAM) and the Cross-Modality Aggregation Module(CMAM), for effective feature fusion. IMAM leverages a self-attention mechanismto aggregate multi-view features while CMAM exploits a cross-attentionmechanism to interact point cloud features with multi-view features. The finaldescriptor of a 3D object for object retrieval can be obtained viaconcatenating the aggregated features from both modules. Extensive experimentsand analysis are conducted on three datasets, ranging from small to largescale, to show the superiority of the proposed SCA-PVNet over thestate-of-the-art methods.", "output": "SCA-PVNet: Self-and-Cross Attention Based Aggregation of Point Cloud and Multi-View for 3D Object Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image distortion by atmospheric turbulence is a stochastic degradation, whichis a critical problem in long-range optical imaging systems. A number ofresearch has been conducted during the past decades, including model-based andemerging deep-learning solutions with the help of synthetic data. Although fastand physics-grounded simulation tools have been introduced to help thedeep-learning models adapt to real-world turbulence conditions recently, thetraining of such models only relies on the synthetic data and ground truthpairs. This paper proposes the Physics-integrated Restoration Network (PiRN) tobring the physics-based simulator directly into the training process to helpthe network to disentangle the stochasticity from the degradation and theunderlying image. Furthermore, to overcome the ``average effect\" introduced bydeterministic models and the domain gap between the synthetic and real-worlddegradation, we further introduce PiRN with Stochastic Refinement (PiRN-SR) toboost its perceptual quality. Overall, our PiRN and PiRN-SR improve thegeneralization to real-world unknown turbulence conditions and provide astate-of-the-art restoration in both pixel-wise accuracy and perceptualquality. Our codes are available at url{", "output": "Physics-Driven Turbulence Image Restoration with Stochastic Refinement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Building outline extracted from high-resolution aerial images can be used invarious application fields such as change detection and disaster assessment.However, traditional CNN model cannot recognize contours very precisely fromoriginal images. In this paper, we proposed a CNN and Transformer based modeltogether with active contour model to deal with this problem. We also designeda triple-branch decoder structure to handle different features generated byencoder. Experiment results show that our model outperforms other baselinemodel on two datasets, achieving 91.1% mIoU on Vaihingen and 83.8% on Binghuts.", "output": "Hybrid Feature Embedding For Automatic Building Outline Extraction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) has drawn increasing attention owing to its potentialuse in large-scale industrial applications. Existing federated learning worksmainly focus on model homogeneous settings. However, practical federatedlearning typically faces the heterogeneity of data distributions, modelarchitectures, network environments, and hardware devices among participantclients. Heterogeneous Federated Learning (HFL) is much more challenging, andcorresponding solutions are diverse and complex. Therefore, a systematic surveyon this topic about the research challenges and state-of-the-art is essential.In this survey, we firstly summarize the various research challenges in HFLfrom five aspects: statistical heterogeneity, model heterogeneity,communication heterogeneity, device heterogeneity, and additional challenges.In addition, recent advances in HFL are reviewed and a new taxonomy of existingHFL methods is proposed with an in-depth analysis of their pros and cons. Weclassify existing methods from three different levels according to the HFLprocedure: data-level, model-level, and server-level. Finally, several criticaland promising future research directions in HFL are discussed, which mayfacilitate further developments in this field. A periodically updatedcollection on HFL is available at ", "output": "Heterogeneous Federated Learning: State-of-the-art and Research Challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, tensor networks have emerged as powerful tools for solvinglarge-scale optimization problems. One of the most promising tensor networks isthe tensor ring (TR) decomposition, which achieves circular dimensionalpermutation invariance in the model through the utilization of the traceoperation and equitable treatment of the latent cores. On the other hand, morerecently, quaternions have gained significant attention and have been widelyutilized in color image processing tasks due to their effectiveness in encodingcolor pixels. Therefore, in this paper, we propose the quaternion tensor ring(QTR) decomposition, which inherits the powerful and generalized representationabilities of the TR decomposition while leveraging the advantages ofquaternions for color pixel representation. In addition to providing thedefinition of QTR decomposition and an algorithm for learning the QTR format,this paper also proposes a low-rank quaternion tensor completion (LRQTC) modeland its algorithm for color image inpainting based on the QTR decomposition.Finally, extensive experiments on color image inpainting demonstrate that theproposed QTLRC method is highly competitive.", "output": "Quaternion tensor ring decomposition and application for color image inpainting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we briefly introduce the solution of our team HFUT-VUT for theMicros-gesture Classification in the MiGA challenge at IJCAI 2023. Themicro-gesture classification task aims at recognizing the action category of agiven video based on the skeleton data. For this task, we propose a3D-CNNs-based micro-gesture recognition network, which incorporates a skeletaland semantic embedding loss to improve action classification performance.Finally, we rank 1st in the Micro-gesture Classification Challenge, surpassingthe second-place team in terms of Top-1 accuracy by 1.10%.", "output": "Joint Skeletal and Semantic Embedding Loss for Micro-gesture Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Colonoscopic Polyp Re-Identification aims to match a specific polyp in alarge gallery with different cameras and views, which plays a key role for theprevention and treatment of colorectal cancer in the computer-aided diagnosis.However, traditional methods mainly focus on the visual representationlearning, while neglect to explore the potential of semantic features duringtraining, which may easily leads to poor generalization capability when adaptedthe pretrained model into the new scenarios. To relieve this dilemma, wepropose a simple but effective training method named VT-ReID, which canremarkably enrich the representation of polyp videos with the interchange ofhigh-level semantic information. Moreover, we elaborately design a novelclustering mechanism to introduce prior knowledge from textual data, whichleverages contrastive learning to promote better separation from abundantunlabeled text data. To the best of our knowledge, this is the first attempt toemploy the visual-text feature with clustering mechanism for the colonoscopicpolyp re-identification. Empirical results show that our method significantlyoutperforms current state-of-the art methods with a clear margin.", "output": "Learning Discriminative Visual-Text Representation for Polyp Re-Identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This article presents the methods used to parallelize a new computer visionapplication. The system is able to automatically detect meteor fromnon-stabilized cameras and noisy video sequences. The application is designedto be embedded in weather balloons or for airborne observation campaigns. Thus,the final target is a low power system-on-chip (&lt; 10 Watts) while the softwareneeds to compute a stream of frames in real-time (&gt; 25 frames per second). Forthis, first the application is split in a tasks graph, then differentparallelization techniques are applied. Experiment results demonstrate theefficiency of the parallelization methods. For instance, on the Raspberry Pi 4and on a HD video sequence, the processing chain reaches 42 frames per secondwhile it only consumes 6 Watts.", "output": "Parallelization of a new embedded application for automatic meteor detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A reliable and comprehensive evaluation metric that aligns with manualpreference assessments is crucial for conversational head video synthesismethod development. Existing quantitative evaluations often fail to capture thefull complexity of human preference, as they only consider limited evaluationdimensions. Qualitative evaluations and user studies offer a solution but aretime-consuming and labor-intensive. This limitation hinders the advancement ofconversational head generation algorithms and systems. In this paper, wepropose a novel learning-based evaluation metric named Preference Score (PS)for fitting human preference according to the quantitative evaluations acrossdifferent dimensions. PS can serve as a quantitative evaluation without theneed for human annotation. Experimental results validate the superiority ofPreference Score in aligning with human perception, and also demonstratesrobustness and generalizability to unseen data, making it a valuable tool foradvancing conversation head generation. We expect this metric could facilitatenew advances in conversational head generation.", "output": "Learning and Evaluating Human Preferences for Conversational Head Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural network quantization aims to accelerate and trim full-precision neuralnetwork models by using low bit approximations. Methods adopting thequantization aware training (QAT) paradigm have recently seen a rapid growth,but are often conceptually complicated. This paper proposes a novel and highlyeffective QAT method, quantized feature distillation (QFD). QFD first trains aquantized (or binarized) representation as the teacher, then quantize thenetwork using knowledge distillation (KD). Quantitative results show that QFDis more flexible and effective (i.e., quantization friendly) than previousquantization methods. QFD surpasses existing methods by a noticeable margin onnot only image classification but also object detection, albeit being muchsimpler. Furthermore, QFD quantizes ViT and Swin-Transformer on MS-COCOdetection and segmentation, which verifies its potential in real worlddeployment. To the best of our knowledge, this is the first time that visiontransformers have been quantized in object detection and image segmentationtasks.", "output": "Quantized Feature Distillation for Network Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The widespread use of face retouching filters on short-video platforms hasraised concerns about the authenticity of digital appearances and the impact ofdeceptive advertising. To address these issues, there is a pressing need todevelop advanced face retouching techniques. However, the lack of large-scaleand fine-grained face retouching datasets has been a major obstacle to progressin this field. In this paper, we introduce RetouchingFFHQ, a large-scale andfine-grained face retouching dataset that contains over half a millionconditionally-retouched images. RetouchingFFHQ stands out from previousdatasets due to its large scale, high quality, fine-grainedness, andcustomization. By including four typical types of face retouching operationsand different retouching levels, we extend the binary face retouching detectioninto a fine-grained, multi-retouching type, and multi-retouching levelestimation problem. Additionally, we propose a Multi-granularity AttentionModule (MAM) as a plugin for CNN backbones for enhanced cross-scalerepresentation learning. Extensive experiments using different baselines aswell as our proposed method on RetouchingFFHQ show decent performance on faceretouching detection. With the proposed new dataset, we believe there is greatpotential for future work to tackle the challenging problem of real-worldfine-grained face retouching detection.", "output": "RetouchingFFHQ: A Large-scale Dataset for Fine-grained Face Retouching Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Field (NeRF) is a promising approach for synthesizing novelviews, given a set of images and the corresponding camera poses of a scene.However, images photographed from a low-light scene can hardly be used to traina NeRF model to produce high-quality results, due to their low pixelintensities, heavy noise, and color distortion. Combining existing low-lightimage enhancement methods with NeRF methods also does not work well due to theview inconsistency caused by the individual 2D enhancement process. In thispaper, we propose a novel approach, called Low-Light NeRF (or LLNeRF), toenhance the scene representation and synthesize normal-light novel viewsdirectly from sRGB low-light images in an unsupervised manner. The core of ourapproach is a decomposition of radiance field learning, which allows us toenhance the illumination, reduce noise and correct the distorted colors jointlywith the NeRF optimization process. Our method is able to produce novel viewimages with proper lighting and vivid colors and details, given a collection ofcamera-finished low dynamic range (8-bits/channel) images from a low-lightscene. Experiments demonstrate that our method outperforms existing low-lightenhancement methods and NeRF methods.", "output": "Lighting up NeRF via Unsupervised Decomposition and Enhancement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the physical size of recent CMOS image sensors (CIS) gets smaller, thelatest mobile cameras are adopting unique non-Bayer color filter array (CFA)patterns (e.g., Quad, Nona, QxQ), which consist of homogeneous color units withadjacent pixels. These non-Bayer sensors are superior to conventional Bayer CFAthanks to their changeable pixel-bin sizes for different light conditions butmay introduce visual artifacts during demosaicing due to their inherent pixelpattern structures and sensor hardware characteristics. Previous demosaicingmethods have primarily focused on Bayer CFA, necessitating distinctreconstruction methods for non-Bayer patterned CIS with various CFA modes underdifferent lighting conditions. In this work, we propose an efficient unifieddemosaicing method that can be applied to both conventional Bayer RAW andvarious non-Bayer CFAs' RAW data in different operation modes. Our KnowledgeLearning-based demosaicing model for Adaptive Patterns, namely KLAP, utilizesCFA-adaptive filters for only 1% key filters in the network for each CFA, butstill manages to effectively demosaic all the CFAs, yielding comparableperformance to the large-scale models. Furthermore, by employing meta-learningduring inference (KLAP-M), our model is able to eliminate unknownsensor-generic artifacts in real RAW data, effectively bridging the gap betweensynthetic images and real sensor RAW. Our KLAP and KLAP-M methods achievedstate-of-the-art demosaicing performance in both synthetic and real RAW data ofBayer and non-Bayer CFAs.", "output": "Efficient Unified Demosaicing for Bayer and Non-Bayer Patterned Image Sensors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We wish to define the limits of a classical classification model based ondeep learning when applied to abstract images, which do not represent visuallyidentifiable objects.QR codes (Quick Response codes) fall into this category ofabstract images: one bit corresponding to one encoded character, QR codes werenot designed to be decoded manually. To understand the limitations of a deeplearning-based model for abstract image classification, we train an imageclassification model on QR codes generated from information obtained whenreading a health pass. We compare a classification model with a classical(deterministic) decoding method in the presence of noise. This study allows usto conclude that a model based on deep learning can be relevant for theunderstanding of abstract images.", "output": "Deep learning for classification of noisy QR codes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Camouflaged object detection (COD), aiming to segment camouflaged objectswhich exhibit similar patterns with the background, is a challenging task. Mostexisting works are dedicated to establishing specialized modules to identifycamouflaged objects with complete and fine details, while the boundary can notbe well located for the lack of object-related semantics. In this paper, wepropose a novel ``pre-train, adapt and detect\" paradigm to detect camouflagedobjects. By introducing a large pre-trained model, abundant knowledge learnedfrom massive multi-modal data can be directly transferred to COD. A lightweightparallel adapter is inserted to adjust the features suitable for the downstreamCOD task. Extensive experiments on four challenging benchmark datasetsdemonstrate that our method outperforms existing state-of-the-art COD models bylarge margins. Moreover, we design a multi-task learning scheme for tuning theadapter to exploit the shareable knowledge across different semantic classes.Comprehensive experimental results showed that the generalization ability ofour model can be substantially improved with multi-task adapter initializationon source tasks and multi-task adaptation on target tasks.", "output": "Pre-train, Adapt and Detect: Multi-Task Adapter Tuning for Camouflaged Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, denoising methods based on supervised learning have exhibitedpromising performance. However, their reliance on external datasets containingnoisy-clean image pairs restricts their applicability. To address thislimitation, researchers have focused on training denoising networks usingsolely a set of noisy inputs. To improve the feasibility of denoisingprocedures, in this study, we proposed a single-image self-supervised learningmethod in which only the noisy input image is used for network training. Gatedconvolution was used for feature extraction and no-reference image qualityassessment was used for guiding the training process. Moreover, the proposedmethod sampled instances from the input image dataset using Bernoulli samplingwith a certain dropout rate for training. The corresponding result was producedby averaging the generated predictions from various instances of the trainednetwork with dropouts. The experimental results indicated that the proposedmethod achieved state-of-the-art denoising performance on both synthetic andreal-world datasets. This highlights the effectiveness and practicality of ourmethod as a potential solution for various noise removal tasks.", "output": "Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Improving the feature representation ability is the foundation of many wholeslide pathological image (WSIs) tasks. Recent works have achieved great successin pathological-specific self-supervised learning (SSL). However, most of themonly focus on learning patch-level representations, thus there is still a gapbetween pretext and slide-level downstream tasks, e.g., subtyping, grading andstaging. Aiming towards slide-level representations, we propose Slide-LevelPrototypical Distillation (SLPD) to explore intra- and inter-slide semanticstructures for context modeling on WSIs. Specifically, we iteratively performintra-slide clustering for the regions (4096x4096 patches) within each WSI toyield the prototypes and encourage the region representations to be closer tothe assigned prototypes. By representing each slide with its prototypes, wefurther select similar slides by the set distance of prototypes and assign theregions by cross-slide prototypes for distillation. SLPD achievesstate-of-the-art results on multiple slide-level benchmarks and demonstratesthat representation learning of semantic structures of slides can make asuitable proxy task for WSI analysis. Code will be available at", "output": "SLPD: Slide-level Prototypical Distillation for WSIs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The widespread use of mobile devices for various digital services has createda need for reliable and real-time person authentication. In this context,facial recognition technologies have emerged as a dependable method forverifying users due to the prevalence of cameras in mobile devices and theirintegration into everyday applications. The rapid advancement of deepConvolutional Neural Networks (CNNs) has led to numerous face verificationarchitectures. However, these models are often large and impractical for mobileapplications, reaching sizes of hundreds of megabytes with millions ofparameters. We address this issue by developing SqueezerFaceNet, a light facerecognition network which less than 1M parameters. This is achieved by applyinga network pruning method based on Taylor scores, where filters with smallimportance scores are removed iteratively. Starting from an already smallnetwork (of 1.24M) based on SqueezeNet, we show that it can be further reduced(up to 40%) without an appreciable loss in performance. To the best of ourknowledge, we are the first to evaluate network pruning methods for the task offace recognition.", "output": "SqueezerFaceNet: Reducing a Small Face Recognition CNN Even More Via Filter Pruning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Retinal image matching plays a crucial role in monitoring disease progressionand treatment response. However, datasets with matched keypoints betweentemporally separated pairs of images are not available in abundance to traintransformer-based model. We propose a novel approach based on reverse knowledgedistillation to train large models with limited data while preventingoverfitting. Firstly, we propose architectural modifications to a CNN-basedsemi-supervised method called SuperRetina that help us improve its results on apublicly available dataset. Then, we train a computationally heavier modelbased on a vision transformer encoder using the lighter CNN-based model, whichis counter-intuitive in the field knowledge-distillation research wheretraining lighter models based on heavier ones is the norm. Surprisingly, suchreverse knowledge distillation improves generalization even further. Ourexperiments suggest that high-dimensional fitting in representation space mayprevent overfitting unlike training directly to match the final output. We alsoprovide a public dataset with annotations for retinal image keypoint detectionand matching to help the research community develop algorithms for retinalimage applications.", "output": "Reverse Knowledge Distillation: Training a Large Model using a Small One for Retinal Image Matching on Limited Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation is a common task in autonomous driving to understandthe surrounding environment. Driveable Area Segmentation and Lane Detection areparticularly important for safe and efficient navigation on the road. However,original semantic segmentation models are computationally expensive and requirehigh-end hardware, which is not feasible for embedded systems in autonomousvehicles. This paper proposes a lightweight model for the driveable area andlane line segmentation. TwinLiteNet is designed cheaply but achieves accurateand efficient segmentation results. We evaluate TwinLiteNet on the BDD100Kdataset and compare it with modern models. Experimental results show that ourTwinLiteNet performs similarly to existing approaches, requiring significantlyfewer computational resources. Specifically, TwinLiteNet achieves a mIoU scoreof 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection taskwith only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.Furthermore, TwinLiteNet can run in real-time on embedded devices with limitedcomputing power, especially since it achieves 60FPS on Jetson Xavier NX, makingit an ideal solution for self-driving vehicles. Code is available:url{", "output": "TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing customization methods require access to multiple reference examplesto align pre-trained diffusion probabilistic models (DPMs) with user-providedconcepts. This paper aims to address the challenge of DPM customization whenthe only available supervision is a differentiable metric defined on thegenerated contents. Since the sampling procedure of DPMs involves recursivecalls to the denoising UNet, na\"ive gradient backpropagation requires storingthe intermediate states of all iterations, resulting in extremely high memoryconsumption. To overcome this issue, we propose a novel method AdjointDPM,which first generates new samples from diffusion models by solving thecorresponding probability-flow ODEs. It then uses the adjoint sensitivitymethod to backpropagate the gradients of the loss to the models' parameters(including conditioning signals, network weights, and initial noises) bysolving another augmented ODE. To reduce numerical errors in both the forwardgeneration and gradient backpropagation processes, we further reparameterizethe probability-flow ODE and augmented ODE as simple non-stiff ODEs usingexponential integration. Finally, we demonstrate the effectiveness ofAdjointDPM on three interesting tasks: converting visual effects intoidentification text embeddings, finetuning DPMs for specific types ofstylization, and optimizing initial noise to generate adversarial samples forsecurity auditing.", "output": "AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised monocular depth estimation (SS-MDE) has the potential toscale to vast quantities of data. Unfortunately, existing approaches limitthemselves to the automotive domain, resulting in models incapable ofgeneralizing to complex environments such as natural or indoor settings.To address this, we propose a large-scale SlowTV dataset curated fromYouTube, containing an order of magnitude more data than existing automotivedatasets. SlowTV contains 1.7M images from a rich diversity of environments,such as worldwide seasonal hiking, scenic driving and scuba diving. Using thisdataset, we train an SS-MDE model that provides zero-shot generalization to alarge collection of indoor/outdoor datasets. The resulting model outperformsall existing SSL approaches and closes the gap on supervised SoTA, despiteusing a more efficient architecture.We additionally introduce a collection of best-practices to further maximizeperformance and zero-shot generalization. This includes 1) aspect ratioaugmentation, 2) camera intrinsic estimation, 3) support frame randomizationand 4) flexible motion estimation. Code is available at", "output": "Kick Back & Relax: Learning to Reconstruct the World by Watching SlowTV."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Active learning algorithms have become increasingly popular for trainingmodels with limited data. However, selecting data for annotation remains achallenging problem due to the limited information available on unseen data. Toaddress this issue, we propose EdgeAL, which utilizes the edge information ofunseen images as {it a priori} information for measuring uncertainty. Theuncertainty is quantified by analyzing the divergence and entropy in modelpredictions across edges. This measure is then used to select superpixels forannotation. We demonstrate the effectiveness of EdgeAL on multi-class OpticalCoherence Tomography (OCT) segmentation tasks, where we achieved a 99% dicescore while reducing the annotation label cost to 12%, 2.3%, and 3%,respectively, on three publicly available datasets (Duke, AROI, and UMN). Thesource code is available at url{", "output": "EdgeAL: An Edge Estimation Based Active Learning Approach for OCT Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One-class classification (OCC) aims to train a classifier only with thetarget class data and attracts great attention for its strong applicability inreal-world application. Despite a lot of advances have been made in OCC, itstill lacks the effective OCC loss functions for deep learning. In this paper,a novel logarithmic barrier function based OCC loss (LBL) that assigns largegradients to the margin samples and thus derives more compact hypersphere, isfirst proposed by approximating the OCC objective smoothly. But theoptimization of LBL may be instability especially when samples lie on theboundary leading to the infinity loss. To address this issue, then, aunilateral relaxation Sigmoid function is introduced into LBL and a novel OCCloss named LBLSig is proposed. The LBLSig can be seen as the fusion of the meansquare error (MSE) and the cross entropy (CE) and the optimization of LBLSig issmoother owing to the unilateral relaxation Sigmoid function. The effectivenessof the proposed LBL and LBLSig is experimentally demonstrated in comparisonswith several state-of-the-art OCC algorithms on different network structures.The source code can be found at ", "output": "LBL: Logarithmic Barrier Loss Function for One-class Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing action recognition methods are typically actor-specific due to theintrinsic topological and apparent differences among the actors. This requiresactor-specific pose estimation (e.g., humans vs. animals), leading tocumbersome model design complexity and high maintenance costs. Moreover, theyoften focus on learning the visual modality alone and single-labelclassification whilst neglecting other available information sources (e.g.,class name text) and the concurrent occurrence of multiple actions. To overcomethese limitations, we propose a new approach called 'actor-agnostic multi-modalmulti-label action recognition,' which offers a unified solution for varioustypes of actors, including humans and animals. We further formulate a novelMulti-modal Semantic Query Network (MSQNet) model in a transformer-based objectdetection framework (e.g., DETR), characterized by leveraging visual andtextual modalities to represent the action classes better. The elimination ofactor-specific model designs is a key advantage, as it removes the need foractor pose estimation altogether. Extensive experiments on five publiclyavailable benchmarks show that our MSQNet consistently outperforms the priorarts of actor-specific alternatives on human and animal single- and multi-labelaction recognition tasks by up to 50%. Code will be released at", "output": "MSQNet: Actor-agnostic Action Recognition with Multi-modal Query."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Working memory (WM), a fundamental cognitive process facilitating thetemporary storage, integration, manipulation, and retrieval of information,plays a vital role in reasoning and decision-making tasks. Robust benchmarkdatasets that capture the multifaceted nature of WM are crucial for theeffective development and evaluation of AI WM models. Here, we introduce acomprehensive Working Memory (WorM) benchmark dataset for this purpose. WorMcomprises 10 tasks and a total of 1 million trials, assessing 4functionalities, 3 domains, and 11 behavioral and neural characteristics of WM.We jointly trained and tested state-of-the-art recurrent neural networks andtransformers on all these tasks. We also include human behavioral benchmarks asan upper bound for comparison. Our results suggest that AI models replicatesome characteristics of WM in the brain, most notably primacy and recencyeffects, and neural clusters and correlates specialized for different domainsand functionalities of WM. In the experiments, we also reveal some limitationsin existing models to approximate human behavior. This dataset serves as avaluable resource for communities in cognitive psychology, neuroscience, andAI, offering a standardized framework to compare and enhance WM models,investigate WM's neural underpinnings, and develop WM models with human-likecapabilities. Our source code and data are available at", "output": "Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Radiance Fields (NeRFs) have achieved great success in the past fewyears. However, most current methods still require intensive resources due toray marching-based rendering. To construct urban-level radiance fieldsefficiently, we design Deformable Neural Mesh Primitive~(DNMP), and propose toparameterize the entire scene with such primitives. The DNMP is a flexible andcompact neural variant of classic mesh representation, which enjoys both theefficiency of rasterization-based rendering and the powerful neuralrepresentation capability for photo-realistic image synthesis. Specifically, aDNMP consists of a set of connected deformable mesh vertices with paired vertexfeatures to parameterize the geometry and radiance information of a local area.To constrain the degree of freedom for optimization and lower the storagebudgets, we enforce the shape of each primitive to be decoded from a relativelylow-dimensional latent space. The rendering colors are decoded from the vertexfeatures (interpolated with rasterization) by a view-dependent MLP. The DNMPprovides a new paradigm for urban-level scene representation with appealingproperties: $(1)$ High-quality rendering. Our method achieves leadingperformance for novel view synthesis in urban scenarios. $(2)$ Lowcomputational costs. Our representation enables fast rendering (2.07ms/1kpixels) and low peak memory usage (110MB/1k pixels). We also present alightweight version that can run 33$times$ faster than vanilla NeRFs, andcomparable to the highly-optimized Instant-NGP (0.61 vs 0.71ms/1k pixels).Project page: href{", "output": "Urban Radiance Field Representation with Deformable Neural Mesh Primitives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision transformers have demonstrated remarkable success in a wide range ofcomputer vision tasks over the last years. However, their high computationalcosts remain a significant barrier to their practical deployment. Inparticular, the complexity of transformer models is quadratic with respect tothe number of input tokens. Therefore techniques that reduce the number ofinput tokens that need to be processed have been proposed. This paperintroduces Learned Thresholds token Merging and Pruning (LTMP), a novelapproach that leverages the strengths of both token merging and token pruning.LTMP uses learned threshold masking modules that dynamically determine whichtokens to merge and which to prune. We demonstrate our approach with extensiveexperiments on vision transformers on the ImageNet classification task. Ourresults demonstrate that LTMP achieves state-of-the-art accuracy acrossreduction rates while requiring only a single fine-tuning epoch, which is anorder of magnitude faster than previous methods. Code is available at .", "output": "Learned Thresholds Token Merging and Pruning for Vision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Zero-shot point cloud segmentation aims to make deep models capable ofrecognizing novel objects in point cloud that are unseen in the training phase.Recent trends favor the pipeline which transfers knowledge from seen classeswith labels to unseen classes without labels. They typically align visualfeatures with semantic features obtained from word embedding by the supervisionof seen classes' annotations. However, point cloud contains limited informationto fully match with semantic features. In fact, the rich appearance informationof images is a natural complement to the textureless point cloud, which is notwell explored in previous literature. Motivated by this, we propose a novelmulti-modal zero-shot learning method to better utilize the complementaryinformation of point clouds and images for more accurate visual-semanticalignment. Extensive experiments are performed in two popular benchmarks, i.e.,SemanticKITTI and nuScenes, and our method outperforms current SOTA methodswith 52% and 49% improvement on average for unseen class mIoU, respectively.", "output": "See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The 4D Millimeter wave (mmWave) radar is a promising technology for vehiclesensing due to its cost-effectiveness and operability in adverse weatherconditions. However, the adoption of this technology has been hindered bysparsity and noise issues in radar point cloud data. This paper introducesspatial multi-representation fusion (SMURF), a novel approach to 3D objectdetection using a single 4D imaging radar. SMURF leverages multiplerepresentations of radar detection points, including pillarization and densityfeatures of a multi-dimensional Gaussian mixture distribution through kerneldensity estimation (KDE). KDE effectively mitigates measurement inaccuracycaused by limited angular resolution and multi-path propagation of radarsignals. Additionally, KDE helps alleviate point cloud sparsity by capturingdensity features. Experimental evaluations on View-of-Delft (VoD) andTJ4DRadSet datasets demonstrate the effectiveness and generalization ability ofSMURF, outperforming recently proposed 4D imaging radar-basedsingle-representation models. Moreover, while using 4D imaging radar only,SMURF still achieves comparable performance to the state-of-the-art 4D imagingradar and camera fusion-based method, with an increase of 1.22% in the meanaverage precision on bird's-eye view of TJ4DRadSet dataset and 1.32% in the 3Dmean average precision on the entire annotated area of VoD dataset. Ourproposed method demonstrates impressive inference time and addresses thechallenges of real-time detection, with the inference time no more than 0.05seconds for most scans on both datasets. This research highlights the benefitsof 4D mmWave radar and is a strong benchmark for subsequent works regarding 3Dobject detection with 4D imaging radar.", "output": "SMURF: Spatial Multi-Representation Fusion for 3D Object Detection with 4D Imaging Radar."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Source-free domain adaptation has become popular because of its practicalusefulness and no need to access source data. However, the adaptation processstill takes a considerable amount of time and is predominantly based onoptimization that relies on back-propagation. In this work we present a simplefeed-forward approach that challenges the need for back-propagation basedadaptation. Our approach is based on computing prototypes of classes under thedomain shift using a pre-trained model. It achieves strong improvements inaccuracy compared to the pre-trained model and requires only a small fractionof time of existing domain adaptation methods.", "output": "Feed-Forward Source-Free Domain Adaptation via Class Prototypes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To be successful, Vision-and-Language Navigation (VLN) agents must be able toground instructions to actions based on their surroundings. In this work, wedevelop a methodology to study agent behavior on a skill-specific basis --examining how well existing agents ground instructions about stopping, turning,and moving towards specified objects or rooms. Our approach is based ongenerating skill-specific interventions and measuring changes in agentpredictions. We present a detailed case study analyzing the behavior of arecent agent and then compare multiple agents in terms of skill-specificcompetency scores. This analysis suggests that biases from training havelasting effects on agent behavior and that existing models are able to groundsimple referring expressions. Our comparisons between models show thatskill-specific scores correlate with improvements in overall VLN taskperformance.", "output": "Behavioral Analysis of Vision-and-Language Navigation Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot anomaly detection (AD) is an emerging sub-field of general AD, andtries to distinguish between normal and anomalous data using only few selectedsamples. While newly proposed few-shot AD methods do compare againstpre-existing algorithms developed for the full-shot domain as baselines, theydo not dedicatedly optimize them for the few-shot setting. It thus remainsunclear if the performance of such pre-existing algorithms can be furtherimproved. We address said question in this work. Specifically, we present astudy on the AD/anomaly segmentation (AS) performance of PatchCore, the currentstate-of-the-art full-shot AD/AS algorithm, in both the few-shot and themany-shot settings. We hypothesize that further performance improvements can berealized by (I) optimizing its various hyperparameters, and by (II)transferring techniques known to improve few-shot supervised learning to the ADdomain. Exhaustive experiments on the public VisA and MVTec AD datasets revealthat (I) significant performance improvements can be realized by optimizinghyperparameters such as the underlying feature extractor, and that (II)image-level augmentations can, but are not guaranteed, to improve performance.Based on these findings, we achieve a new state of the art in few-shot AD onVisA, further demonstrating the merit of adapting pre-existing AD/AS methods tothe few-shot setting. Last, we identify the investigation of feature extractorswith a strong inductive bias as a potential future research direction for(few-shot) AD/AS.", "output": "Optimizing PatchCore for Few/many-shot Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present our method for neural face reenactment, calledHyperReenact, that aims to generate realistic talking head images of a sourceidentity, driven by a target facial pose. Existing state-of-the-art facereenactment methods train controllable generative models that learn tosynthesize realistic facial images, yet producing reenacted faces that areprone to significant visual artifacts, especially under the challengingcondition of extreme head pose changes, or requiring expensive few-shotfine-tuning to better preserve the source identity characteristics. We proposeto address these limitations by leveraging the photorealistic generationability and the disentangled properties of a pretrained StyleGAN2 generator, byfirst inverting the real images into its latent space and then using ahypernetwork to perform: (i) refinement of the source identity characteristicsand (ii) facial pose re-targeting, eliminating this way the dependence onexternal editing methods that typically produce artifacts. Our method operatesunder the one-shot setting (i.e., using a single source frame) and allows forcross-subject reenactment, without requiring any subject-specific fine-tuning.We compare our method both quantitatively and qualitatively against severalstate-of-the-art techniques on the standard benchmarks of VoxCeleb1 andVoxCeleb2, demonstrating the superiority of our approach in producingartifact-free images, exhibiting remarkable robustness even under extreme headpose changes. We make the code and the pretrained models publicly available at: .", "output": "HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal learning aims to build models that can process and relateinformation from multiple modalities. Despite years of development in thisfield, it still remains challenging to design a unified network for processingvarious modalities ($textit{e.g.}$ natural language, 2D images, 3D pointclouds, audio, video, time series, tabular data) due to the inherent gaps amongthem. In this work, we propose a framework, named Meta-Transformer, thatleverages a $textbf{frozen}$ encoder to perform multimodal perception withoutany paired multimodal training data. In Meta-Transformer, the raw input datafrom various modalities are mapped into a shared token space, allowing asubsequent encoder with frozen parameters to extract high-level semanticfeatures of the input data. Composed of three main components: a unified datatokenizer, a modality-shared encoder, and task-specific heads for downstreamtasks, Meta-Transformer is the first framework to perform unified learningacross 12 modalities with unpaired data. Experiments on different benchmarksreveal that Meta-Transformer can handle a wide range of tasks includingfundamental perception (text, image, point cloud, audio, video), practicalapplication (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph,tabular, and time-series). Meta-Transformer indicates a promising future fordeveloping unified multimodal intelligence with transformers. Code will beavailable at ", "output": "Meta-Transformer: A Unified Framework for Multimodal Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Omnidirectional videos (ODVs) play an increasingly important role in theapplication fields of medical, education, advertising, tourism, etc. Assessingthe quality of ODVs is significant for service-providers to improve the user'sQuality of Experience (QoE). However, most existing quality assessment studiesfor ODVs only focus on the visual distortions of videos, while ignoring thatthe overall QoE also depends on the accompanying audio signals. In this paper,we first establish a large-scale audio-visual quality assessment dataset foromnidirectional videos, which includes 375 distorted omnidirectionalaudio-visual (A/V) sequences generated from 15 high-quality pristineomnidirectional A/V contents, and the corresponding perceptual audio-visualquality scores. Then, we design three baseline methods for full-referenceomnidirectional audio-visual quality assessment (OAVQA), which combine existingstate-of-the-art single-mode audio and video QA models via multimodal fusionstrategies. We validate the effectiveness of the A/V multimodal fusion methodfor OAVQA on our dataset, which provides a new benchmark for omnidirectionalQoE evaluation. Our dataset is available at ", "output": "Perceptual Quality Assessment of Omnidirectional Audio-visual Signals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent text-to-image diffusion models have demonstrated an astonishingcapacity to generate high-quality images. However, researchers mainly studiedthe way of synthesizing images with only text prompts. While some works haveexplored using other modalities as conditions, considerable paired data, e.g.,box/mask-image pairs, and fine-tuning time are required for nurturing models.As such paired data is time-consuming and labor-intensive to acquire andrestricted to a closed set, this potentially becomes the bottleneck forapplications in an open world. This paper focuses on the simplest form ofuser-provided conditions, e.g., box or scribble. To mitigate the aforementionedproblem, we propose a training-free method to control objects and contexts inthe synthesized images adhering to the given spatial conditions. Specifically,three spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints,are designed and seamlessly integrated into the denoising step of diffusionmodels, requiring no additional training and massive annotated layout data.Extensive results show that the proposed constraints can control what and whereto present in the images while retaining the ability of the Stable Diffusionmodel to synthesize with high fidelity and diverse concept coverage. The codeis publicly available at ", "output": "BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Incremental semantic segmentation aims to continually learn the segmentationof new coming classes without accessing the training data of previously learnedclasses. However, most current methods fail to address catastrophic forgettingand background shift since they 1) treat all previous classes equally withoutconsidering different forgetting paces caused by imbalanced gradientback-propagation; 2) lack strong semantic guidance between classes. To tacklethe above challenges, in this paper, we propose a Gradient-SemanticCompensation (GSC) model, which surmounts incremental semantic segmentationfrom both gradient and semantic perspectives. Specifically, to addresscatastrophic forgetting from the gradient aspect, we develop a step-awaregradient compensation that can balance forgetting paces of previously seenclasses via re-weighting gradient backpropagation. Meanwhile, we propose asoft-sharp semantic relation distillation to distill consistent inter-classsemantic relations via soft labels for alleviating catastrophic forgetting fromthe semantic aspect. In addition, we develop a prototypical pseudo re-labelingthat provides strong semantic guidance to mitigate background shift. Itproduces high-quality pseudo labels for old classes in the background bymeasuring distances between pixels and class-wise prototypes. Extensiveexperiments on three public datasets, i.e., Pascal VOC 2012, ADE20K, andCityscapes, demonstrate the effectiveness of our proposed GSC model.", "output": "Gradient-Semantic Compensation for Incremental Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lung cancer is a leading cause of death worldwide and early screening iscritical for improving survival outcomes. In clinical practice, the contextualstructure of nodules and the accumulated experience of radiologists are the twocore elements related to the accuracy of identification of benign and malignantnodules. Contextual information provides comprehensive information aboutnodules such as location, shape, and peripheral vessels, and experiencedradiologists can search for clues from previous cases as a reference to enrichthe basis of decision-making. In this paper, we propose a radiologist-inspiredmethod to simulate the diagnostic process of radiologists, which is composed ofcontext parsing and prototype recalling modules. The context parsing modulefirst segments the context structure of nodules and then aggregates contextualinformation for a more comprehensive understanding of the nodule. The prototyperecalling module utilizes prototype-based learning to condense previouslylearned cases as prototypes for comparative analysis, which is updated onlinein a momentum way during training. Building on the two modules, our methodleverages both the intrinsic characteristics of the nodules and the externalknowledge accumulated from other nodules to achieve a sound diagnosis. To meetthe needs of both low-dose and noncontrast screening, we collect a large-scaledataset of 12,852 and 4,029 nodules from low-dose and noncontrast CTsrespectively, each with pathology- or follow-up-confirmed labels. Experimentson several datasets demonstrate that our method achieves advanced screeningperformance on both low-dose and noncontrast scenarios.", "output": "Parse and Recall: Towards Accurate Lung Nodule Malignancy Prediction like Radiologists."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, different methods have been proposed to address the inconsistencyissue of DDIM inversion to enable image editing, such as EDICT [36] andNull-text inversion [22]. However, the above methods introduce considerablecomputational overhead. In this paper, we propose a new technique, namedbi-directional integration approximation (BDIA), to perform exact diffusioninversion with neglible computational overhead. Suppose we would like toestimate the next diffusion state $boldsymbol{z}_{i-1}$ at timestep $t_i$ withthe historical information $(i,boldsymbol{z}_i)$ and$(i+1,boldsymbol{z}_{i+1})$. We first obtain the estimated Gaussian noise$hat{boldsymbol{epsilon}}(boldsymbol{z}_i,i)$, and then apply the DDIMupdate procedure twice for approximating the ODE integration over the nexttime-slot $[t_i, t_{i-1}]$ in the forward manner and the previous time-slot$[t_i, t_{t+1}]$ in the backward manner. The DDIM step for the previoustime-slot is used to refine the integration approximation made earlier whencomputing $boldsymbol{z}_i$. One nice property with BDIA-DDIM is that theupdate expression for $boldsymbol{z}_{i-1}$ is a linear combination of$(boldsymbol{z}_{i+1}, boldsymbol{z}_i,hat{boldsymbol{epsilon}}(boldsymbol{z}_i,i))$. This allows for exactbackward computation of $boldsymbol{z}_{i+1}$ given $(boldsymbol{z}_i,boldsymbol{z}_{i-1})$, thus leading to exact diffusion inversion. Experimentson both image reconstruction and image editing were conducted, confirming ourstatement.BDIA can also be applied to improve the performance of other ODE solvers inaddition to DDIM. In our work, it is found that applying BDIA to the EDMsampling procedure produces slightly better FID score over CIFAR10.", "output": "Exact Diffusion Inversion via Bi-directional Integration Approximation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Performance of a pre-trained semantic segmentation model is likely tosubstantially decrease on data from a new domain. We show a pre-trained modelcan be adapted to unlabelled target domain data by calculating soft-labelprototypes under the domain shift and making predictions according to theprototype closest to the vector with predicted class probabilities. Theproposed adaptation procedure is fast, comes almost for free in terms ofcomputational resources and leads to considerable performance improvements. Wedemonstrate the benefits of such label calibration on the highly-practicalsynthetic-to-real semantic segmentation problem.", "output": "Label Calibration for Semantic Segmentation Under Domain Shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a deep learning architecture for nowcasting ofprecipitation almost globally every 30 min with a 4-hour lead time. Thearchitecture fuses a U-Net and a convolutional long short-term memory (LSTM)neural network and is trained using data from the Integrated MultisatellitERetrievals for GPM (IMERG) and a few key precipitation drivers from the GlobalForecast System (GFS). The impacts of different training loss functions,including the mean-squared error (regression) and the focal-loss(classification), on the quality of precipitation nowcasts are studied. Theresults indicate that the regression network performs well in capturing lightprecipitation (below 1.6 mm/hr), but the classification network can outperformthe regression network for nowcasting of precipitation extremes (&gt;8 mm/hr), interms of the critical success index (CSI).. Using the Wasserstein distance, itis shown that the predicted precipitation by the classification network has acloser class probability distribution to the IMERG than the regression network.It is uncovered that the inclusion of the physical variables can improveprecipitation nowcasting, especially at longer lead times in both networks.Taking IMERG as a relative reference, a multi-scale analysis in terms offractions skill score (FSS), shows that the nowcasting machine remains skillful(FSS &gt; 0.5) at the resolution of 10 km compared to 50 km for GFS. Forprecipitation rates greater than 4~mm/hr, only the classification networkremains FSS-skillful on scales greater than 50 km within a 2-hour lead time.", "output": "Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning algorithms which keep the parameters of new tasks close tothat of previous tasks, are popular in preventing catastrophic forgetting insequential task learning settings. However, 1) the performance for the newcontinual learner will be degraded without distinguishing the contributions ofpreviously learned tasks; 2) the computational cost will be greatly increasedwith the number of tasks, since most existing algorithms need to regularize allprevious tasks when learning new tasks. To address the above challenges, wepropose a self-paced Weight Consolidation (spWC) framework to attain robustcontinual learning via evaluating the discriminative contributions of previoustasks. To be specific, we develop a self-paced regularization to reflect thepriorities of past tasks via measuring difficulty based on key performanceindicator (i.e., accuracy). When encountering a new task, all previous tasksare sorted from \"difficult\" to \"easy\" based on the priorities. Then theparameters of the new continual learner will be learned via selectivelymaintaining the knowledge amongst more difficult past tasks, which could wellovercome catastrophic forgetting with less computational cost. We adopt analternative convex search to iteratively update the model parameters andpriority weights in the bi-convex formulation. The proposed spWC framework isplug-and-play, which is applicable to most continual learning algorithms (e.g.,EWC, MAS and RCIL) in different directions (e.g., classification andsegmentation). Experimental results on several public benchmark datasetsdemonstrate that our proposed framework can effectively improve performancewhen compared with other popular continual learning algorithms.", "output": "Self-paced Weight Consolidation for Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weakly-supervised change detection (WSCD) aims to detect pixel-level changeswith only image-level annotations. Owing to its label efficiency, WSCD isdrawing increasing attention recently. However, current WSCD methods oftenencounter the challenge of change missing and fabricating, i.e., theinconsistency between image-level annotations and pixel-level predictions.Specifically, change missing refer to the situation that the WSCD model failsto predict any changed pixels, even though the image-level label indicateschanged, and vice versa for change fabricating. To address this challenge, inthis work, we leverage global-scale and local-scale priors in WSCD and proposetwo components: a Dilated Prior (DP) decoder and a Label Gated (LG) constraint.The DP decoder decodes samples with the changed image-level label, skipssamples with the unchanged label, and replaces them with an all-unchangedpixel-level label. The LG constraint is derived from the correspondence betweenchanged representations and image-level labels, penalizing the model when itmispredicts the change status. Additionally, we develop TransWCD, a simple yetpowerful transformer-based model, showcasing the potential of weakly-supervisedlearning in change detection. By integrating the DP decoder and LG constraintinto TransWCD, we form TransWCD-DL. Our proposed TransWCD and TransWCD-DLachieve significant +6.33% and +9.55% F1 score improvements over thestate-of-the-art methods on the WHU-CD dataset, respectively. Some performancemetrics even exceed several fully-supervised change detection (FSCD)competitors. Code will be available at", "output": "Exploring Effective Priors and Efficient Models for Weakly-Supervised Change Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The great advancements of generative adversarial networks and facerecognition models in computer vision have made it possible to swap identitieson images from single sources. Although a lot of studies seems to have proposedalmost satisfactory solutions, we notice previous methods still suffer from anidentity-attribute entanglement that causes undesired attributes swappingbecause widely used identity encoders, eg, ArcFace, have some crucial attributebiases owing to their pretraining on face recognition tasks. To address thisissue, we design BlendFace, a novel identity encoder for face-swapping. The keyidea behind BlendFace is training face recognition models on blended imageswhose attributes are replaced with those of another mitigates inter-personalbiases such as hairsyles. BlendFace feeds disentangled identity features intogenerators and guides generators properly as an identity loss function.Extensive experiments demonstrate that BlendFace improves theidentity-attribute disentanglement in face-swapping models, maintaining acomparable quantitative performance to previous methods.", "output": "BlendFace: Re-designing Identity Encoders for Face-Swapping."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Emerging large-scale text-to-image generative models, e.g., Stable Diffusion(SD), have exhibited overwhelming results with high fidelity. Despite themagnificent progress, current state-of-the-art models still struggle togenerate images fully adhering to the input prompt. Prior work, Attend &amp;Excite, has introduced the concept of Generative Semantic Nursing (GSN), aimingto optimize cross-attention during inference time to better incorporate thesemantics. It demonstrates promising results in generating simple prompts,e.g., ``a cat and a dog''. However, its efficacy declines when dealing withmore complex prompts, and it does not explicitly address the problem ofimproper attribute binding. To address the challenges posed by complex promptsor scenarios involving multiple entities and to achieve improved attributebinding, we propose Divide &amp; Bind. We introduce two novel loss objectives forGSN: a novel attendance loss and a binding loss. Our approach stands out in itsability to faithfully synthesize desired objects with improved attributealignment from complex prompts and exhibits superior performance acrossmultiple evaluation benchmarks. More videos and updates can be found on theproject page url{", "output": "Divide & Bind Your Attention for Improved Generative Semantic Nursing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Captions are crucial for understanding scientific visualizations anddocuments. Existing captioning methods for scientific figures rely onfigure-caption pairs extracted from documents for training, many of which fallshort with respect to metrics like helpfulness, explainability, andvisual-descriptiveness [15] leading to generated captions being misaligned withreader preferences. To enable the generation of high-quality figure captions,we introduce FigCaps-HF a new framework for figure-caption generation that canincorporate domain expert feedback in generating captions optimized for readerpreferences. Our framework comprises of 1) an automatic method for evaluatingquality of figure-caption pairs, 2) a novel reinforcement learning with humanfeedback (RLHF) method to optimize a generative figure-to-caption model forreader preferences. We demonstrate the effectiveness of our simple learningframework by improving performance over standard fine-tuning across differenttypes of models. In particular, when using BLIP as the base model, our RLHFframework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, andMeteor, respectively. Finally, we release a large-scale benchmark dataset withhuman feedback on figure-caption pairs to enable further evaluation anddevelopment of RLHF techniques for this problem.", "output": "FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Having efficient testing strategies is a core challenge that needs to beovercome for the release of automated driving. This necessitates clearrequirements as well as suitable methods for testing. In this work, therequirements for perception modules are considered with respect to relevance.The concept of relevance currently remains insufficiently defined andspecified. In this paper, we propose a novel methodology to overcome thischallenge by exemplary application to collision safety in the highway domain.Using this general system and use case specification, a corresponding conceptfor relevance is derived. Irrelevant objects are thus defined as objects whichdo not limit the set of safe actions available to the ego vehicle underconsideration of all uncertainties. As an initial step, the use case isdecomposed into functional scenarios with respect to collision relevance. Foreach functional scenario, possible actions of both the ego vehicle and anyother dynamic object are formalized as equations. This set of possible actionsis constrained by traffic rules, yielding relevance criteria. As a result, wepresent a conservative estimation which dynamic objects are relevant forperception and need to be considered for a complete evaluation. The estimationprovides requirements which are applicable for offline testing and validationof perception components. A visualization is presented for examples from thehighD dataset, showing the plausibility of the results. Finally, a possibilityfor a future validation of the presented relevance concept is outlined.", "output": "Conservative Estimation of Perception Relevance of Dynamic Objects for Safe Trajectories in Automotive Scenarios."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The popularity of point cloud deep models for safety-critical purposes hasincreased, but the reliability and security of these models can be compromisedby intentional or naturally occurring point cloud noise. To combat this issue,we present a novel point cloud outlier removal method called PointCVaR, whichempowers standard-trained models to eliminate additional outliers and restorethe data. Our approach begins by conducting attribution analysis to determinethe influence of each point on the model output, which we refer to as pointrisk. We then optimize the process of filtering high-risk points usingConditional Value at Risk (CVaR) as the objective. The rationale for thisapproach is based on the observation that noise points in point clouds tend tocluster in the tail of the risk distribution, with a low frequency but a highlevel of risk, resulting in significant interference with classificationresults. Despite requiring no additional training effort, our method producesexceptional results in various removal-and-classification experiments for noisypoint clouds, which are corrupted by random noise, adversarial noise, andbackdoor trigger noise. Impressively, it achieves 87% accuracy in defenseagainst the backdoor attack by removing triggers. Overall, the proposedPointCVaR effectively eliminates noise points and enhances point cloudclassification, making it a promising plug-in module for various models indifferent scenarios.", "output": "Risk-optimized Outlier Removal for Robust Point Cloud Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a visualization method to understand the effect ofmultidimensional projection on local subspaces, using implicit functiondifferentiation. Here, we understand the local subspace as the multidimensionallocal neighborhood of data points. Existing methods focus on the projection ofmultidimensional data points, and the neighborhood information is ignored. Ourmethod is able to analyze the shape and directional information of the localsubspace to gain more insights into the global structure of the data throughthe perception of local structures. Local subspaces are fitted bymultidimensional ellipses that are spanned by basis vectors. An accurate andefficient vector transformation method is proposed based on analyticaldifferentiation of multidimensional projections formulated as implicitfunctions. The results are visualized as glyphs and analyzed using a full setof specifically-designed interactions supported in our efficient web-basedvisualization tool. The usefulness of our method is demonstrated using variousmulti- and high-dimensional benchmark datasets. Our implicit differentiationvector transformation is evaluated through numerical comparisons; the overallmethod is evaluated through exploration examples and use cases.", "output": "Implicit Multidimensional Projection of Local Subspaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "State-of-the-art performance for many edge applications is achieved by deepneural networks (DNNs). Often, these DNNs are location- and time-sensitive, andmust be delivered over a wireless channel rapidly and efficiently. In thispaper, we introduce AirNet, a family of novel training and transmission methodsthat allow DNNs to be efficiently delivered over wireless channels understringent transmit power and latency constraints. This corresponds to a newclass of joint source-channel coding problems, aimed at delivering DNNs withthe goal of maximizing their accuracy at the receiver, rather than recoveringthem with high fidelity. In AirNet, we propose the direct mapping of the DNNparameters to transmitted channel symbols, while the network is trained to meetthe channel constraints, and exhibit robustness against channel noise. AirNetachieves higher accuracy compared to separation-based alternatives. We furtherimprove the performance of AirNet by pruning the network below the availablebandwidth, and expanding it for improved robustness. We also benefit fromunequal error protection by selectively expanding important layers of thenetwork. Finally, we develop an approach, which simultaneously trains aspectrum of DNNs, each targeting a different channel condition, resolving theimpractical memory requirements of training distinct networks for differentchannel conditions.", "output": "AirNet: Neural Network Transmission over the Air."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating adversarial scenarios, which have the potential to fail autonomousdriving systems, provides an effective way to improve robustness. Extendingpurely data-driven generative models, recent specialized models satisfyadditional controllable requirements such as embedding a traffic sign in adriving scene by manipulating patterns implicitly in the neuron level. In thispaper, we introduce a method to incorporate domain knowledge explicitly in thegeneration process to achieve the Semantically Adversarial Generation (SAG). Tobe consistent with the composition of driving scenes, we first categorize theknowledge into two types, the property of objects and the relationship amongobjects. We then propose a tree-structured variational auto-encoder (T-VAE) tolearn hierarchical scene representation. By imposing semantic rules on theproperties of nodes and edges in the tree structure, explicit knowledgeintegration enables controllable generation. We construct a synthetic exampleto illustrate the controllability and explainability of our method in asuccinct setting. We further extend to realistic environments for autonomousvehicles: our method efficiently identifies adversarial driving scenes againstdifferent state-of-the-art 3D point cloud segmentation models and satisfies thetraffic rules specified as the explicit knowledge.", "output": "Semantically Adversarial Scenario Generation with Explicit Knowledge Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent self-supervised pretraining methods for object detection largely focuson pretraining the backbone of the object detector, neglecting key parts ofdetection architecture. Instead, we introduce DETReg, a new self-supervisedmethod that pretrains the entire object detection network, including the objectlocalization and embedding components. During pretraining, DETReg predictsobject localizations to match the localizations from an unsupervised regionproposal generator and simultaneously aligns the corresponding featureembeddings with embeddings from a self-supervised image encoder. We implementDETReg using the DETR family of detectors and show that it improves overcompetitive baselines when finetuned on COCO, PASCAL VOC, and Airbus Shipbenchmarks. In low-data regimes DETReg achieves improved performance, e.g.,when training with only 1% of the labels and in the few-shot learning settings.", "output": "DETReg: Unsupervised Pretraining with Region Priors for Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We aim at capturing high-order statistics of feature vectors formed by aneural network, and propose end-to-end second- and higher-order pooling to forma tensor descriptor. Tensor descriptors require a robust similarity measure dueto low numbers of aggregated vectors and the burstiness phenomenon, when agiven feature appears more/less frequently than statistically expected. TheHeat Diffusion Process (HDP) on a graph Laplacian is closely related to theEigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix,whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPNplay the same role, i.e., to boost or dampen the magnitude of the eigenspectrumthus preventing the burstiness. We equip higher-order tensors with EPN whichacts as a spectral detector of higher-order occurrences to prevent burstiness.We also prove that for a tensor of order r built from d dimensional featuredescriptors, such a detector gives the likelihood if at least one higher-orderoccurrence is 'projected' into one of binom(d,r) subspaces represented by thetensor; thus forming a tensor power normalization metric endowed withbinom(d,r) such 'detectors'. For experimental contributions, we apply severalsecond- and higher-order pooling variants to action recognition, providepreviously not presented comparisons of such pooling variants, and showstate-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.", "output": "High-order Tensor Pooling with Attention for Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Encoding a driving scene into vector representations has been an essentialtask for autonomous driving that can benefit downstream tasks e.g. trajectoryprediction. The driving scene often involves heterogeneous elements such as thedifferent types of objects (agents, lanes, traffic signs) and the semanticrelations between objects are rich and diverse. Meanwhile, there also existrelativity across elements, which means that the spatial relation is a relativeconcept and need be encoded in a ego-centric manner instead of in a globalcoordinate system. Based on these observations, we propose HeterogeneousDriving Graph Transformer (HDGT), a backbone modelling the driving scene as aheterogeneous graph with different types of nodes and edges. For heterogeneousgraph construction, we connect different types of nodes according to diversesemantic relations. For spatial relation encoding, the coordinates of the nodeas well as its in-edges are in the local node-centric coordinate system. Forthe aggregation module in the graph neural network (GNN), we adopt thetransformer structure in a hierarchical way to fit the heterogeneous nature ofinputs. Experimental results show that HDGT achieves state-of-the-artperformance for the task of trajectory prediction, on INTERACTION PredictionChallenge and Waymo Open Motion Challenge.", "output": "HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision transformers have shown great success on numerous computer visiontasks. However, its central component, softmax attention, prohibits visiontransformers from scaling up to high-resolution images, due to both thecomputational complexity and memory footprint being quadratic. Although linearattention was introduced in natural language processing (NLP) tasks to mitigatea similar issue, directly applying existing linear attention to visiontransformers may not lead to satisfactory results. We investigate this problemand find that computer vision tasks focus more on local information comparedwith NLP tasks. Based on this observation, we present a Vicinity Attention thatintroduces a locality bias to vision transformers with linear complexity.Specifically, for each image patch, we adjust its attention weight based on its2D Manhattan distance measured by its neighbouring patches. In this case, theneighbouring patches will receive stronger attention than far-away patches.Moreover, since our Vicinity Attention requires the token length to be muchlarger than the feature dimension to show its efficiency advantages, we furtherpropose a new Vicinity Vision Transformer (VVT) structure to reduce the featuredimension without degenerating the accuracy. We perform extensive experimentson the CIFAR100, ImageNet1K, and ADE20K datasets to validate the effectivenessof our method. Our method has a slower growth rate of GFlops than previoustransformer-based and convolution-based networks when the input resolutionincreases. In particular, our approach achieves state-of-the-art imageclassification accuracy with 50% fewer parameters than previous methods.", "output": "Vicinity Vision Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a successful approach to self-supervised learning, contrastive learningaims to learn invariant information shared among distortions of the inputsample. While contrastive learning has yielded continuous advancements insampling strategy and architecture design, it still remains two persistentdefects: the interference of task-irrelevant information and sampleinefficiency, which are related to the recurring existence of trivial constantsolutions. From the perspective of dimensional analysis, we find out that thedimensional redundancy and dimensional confounder are the intrinsic issuesbehind the phenomena, and provide experimental evidence to support ourviewpoint. We further propose a simple yet effective approach MetaMask, shortfor the dimensional Mask learned by Meta-learning, to learn representationsagainst dimensional redundancy and confounder. MetaMask adopts theredundancy-reduction technique to tackle the dimensional redundancy issue andinnovatively introduces a dimensional mask to reduce the gradient effects ofspecific dimensions containing the confounder, which is trained by employing ameta-learning paradigm with the objective of improving the performance ofmasked representations on a typical self-supervised task. We provide solidtheoretical analyses to prove MetaMask can obtain tighter risk bounds fordownstream classification compared to typical contrastive methods. Empirically,our method achieves state-of-the-art performance on various benchmarks.", "output": "MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate medical classification requires a large number of multi-modal data,and in many cases, different feature types. Previous studies have shownpromising results when using multi-modal data, outperforming single-modalitymodels when classifying diseases such as Alzheimer's Disease (AD). However,those models are usually not flexible enough to handle missing modalities.Currently, the most common workaround is discarding samples with missingmodalities which leads to considerable data under-utilization. Adding to thefact that labeled medical images are already scarce, the performance ofdata-driven methods like deep learning can be severely hampered. Therefore, amulti-modal method that can handle missing data in various clinical settings ishighly desirable. In this paper, we present Multi-Modal Mixing Transformer(3MAT), a disease classification transformer that not only leveragesmulti-modal data but also handles missing data scenarios. In this work, we test3MT for AD and Cognitively normal (CN) classification and mild cognitiveimpairment (MCI) conversion prediction to progressive MCI (pMCI) or stable MCI(sMCI) using clinical and neuroimaging data. The model uses a novel CascadedModality Transformer architecture with cross-attention to incorporatemulti-modal information for more informed predictions. We propose a novelmodality dropout mechanism to ensure an unprecedented level of modalityindependence and robustness to handle missing data scenarios. The result is aversatile network that enables the mixing of arbitrary numbers of modalitieswith different feature types and also ensures full data utilization missingdata scenarios. The model is trained and evaluated on the ADNI dataset with theSOTRA performance and further evaluated with the AIBL dataset with missingdata.", "output": "Cascaded Multi-Modal Mixing Transformers for Alzheimer's Disease Classification with Incomplete Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal semantic understanding often has to deal with uncertainty, whichmeans the obtained messages tend to refer to multiple targets. Such uncertaintyis problematic for our interpretation, including inter- and intra-modaluncertainty. Little effort has studied the modeling of this uncertainty,particularly in pre-training on unlabeled datasets and fine-tuning intask-specific downstream datasets. In this paper, we project therepresentations of all modalities as probabilistic distributions via aProbability Distribution Encoder (PDE) by utilizing sequence-levelinteractions. Compared to the existing deterministic methods, such uncertaintymodeling can convey richer multimodal semantic information and more complexrelationships. Furthermore, we integrate uncertainty modeling with popularpre-training frameworks and propose suitable pre-training tasks:Distribution-based Vision-Language Contrastive learning (D-VLC),Distribution-based Masked Language Modeling (D-MLM), and Distribution-basedImage-Text Matching (D-ITM). The fine-tuned models are applied to challengingdownstream tasks, including image-text retrieval, visual question answering,visual reasoning, and visual entailment, and achieve state-of-the-art results.", "output": "MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a unified perspective on tackling various human-centric videotasks by learning human motion representations from large-scale andheterogeneous data resources. Specifically, we propose a pretraining stage inwhich a motion encoder is trained to recover the underlying 3D motion fromnoisy partial 2D observations. The motion representations acquired in this wayincorporate geometric, kinematic, and physical knowledge about human motion,which can be easily transferred to multiple downstream tasks. We implement themotion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer)neural network. It could capture long-range spatio-temporal relationships amongthe skeletal joints comprehensively and adaptively, exemplified by the lowest3D pose estimation error so far when trained from scratch. Furthermore, ourproposed framework achieves state-of-the-art performance on all threedownstream tasks by simply finetuning the pretrained motion encoder with asimple regression head (1-2 layers), which demonstrates the versatility of thelearned motion representations. Code and models are available at", "output": "MotionBERT: A Unified Perspective on Learning Human Motion Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Coronavirus Disease 2019 (COVID-19) pandemic has increased the publichealth burden and brought profound disaster to humans. For the particularity ofthe COVID-19 medical images with blurred boundaries, low contrast and differentinfection sites, some researchers have improved the accuracy by adding morecomplexity. Also, they overlook the complexity of lesions, which hinder theirability to capture the relationship between segmentation sites and thebackground, as well as the edge contours and global context. However,increasing the computational complexity, parameters and inference speed isunfavorable for model transfer from laboratory to clinic. A perfectsegmentation network needs to balance the above three factors completely. Tosolve the above issues, this paper propose a symmetric automatic segmentationframework named MS-DCANet. We introduce Tokenized MLP block, a novel attentionscheme that use a shift-window mechanism to conditionally fuse local and globalfeatures to get more continuous boundaries and spatial positioningcapabilities. It has greater understanding of irregular lesions contours.MS-DCANet also uses several Dual Channel blocks and a Res-ASPP block to improvethe ability to recognize small targets. On multi-modality COVID-19 tasks,MS-DCANet achieved state-of-the-art performance compared with other baselines.It can well trade off the accuracy and complexity. To prove the stronggeneralization ability of our proposed model, we apply it to other tasks (ISIC2018 and BAA) and achieve satisfactory results.", "output": "MS-DCANet: A Novel Segmentation Network For Multi-Modality COVID-19 Medical Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Positive unlabeled learning is a binary classification problem with positiveand unlabeled data. It is common in domains where negative labels are costly orimpossible to obtain, e.g., medicine and personalized advertising. Mostapproaches to positive unlabeled learning apply to specific data types (e.g.,images, categorical data) and can not generate new positive and negativesamples. This work introduces a feature-space distance-based tensor networkapproach to the positive unlabeled learning problem. The presented method isnot domain specific and significantly improves the state-of-the-art results onthe MNIST image and 15 categorical/mixed datasets. The trained tensor networkmodel is also a generative model and enables the generation of new positive andnegative instances.", "output": "Positive unlabeled learning with tensor networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The long-standing theory that a colour-naming system evolves under dualpressure of efficient communication and perceptual mechanism is supported bymore and more linguistic studies, including analysing four decades ofdiachronic data from the Nafaanra language. This inspires us to explore whethermachine learning could evolve and discover a similar colour-naming system viaoptimising the communication efficiency represented by high-level recognitionperformance. Here, we propose a novel colour quantisation transformer,CQFormer, that quantises colour space while maintaining the accuracy of machinerecognition on the quantised images. Given an RGB image, Annotation Branch mapsit into an index map before generating the quantised image with a colourpalette; meanwhile the Palette Branch utilises a key-point detection way tofind proper colours in the palette among the whole colour space. By interactingwith colour annotation, CQFormer is able to balance both the machine visionaccuracy and colour perceptual structure such as distinct and stable colourdistribution for discovered colour system. Very interestingly, we even observethe consistent evolution pattern between our artificial colour system and basiccolour terms across human languages. Besides, our colour quantisation methodalso offers an efficient quantisation method that effectively compresses theimage storage while maintaining high performance in high-level recognitiontasks such as classification and detection. Extensive experiments demonstratethe superior performance of our method with extremely low bit-rate colours,showing potential to integrate into quantisation network to quantities fromimage to network activation. The source code is available at", "output": "Name Your Colour For the Task: Artificially Discover Colour Naming via Colour Quantisation Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we show the surprisingly good properties of plain visiontransformers for body pose estimation from various aspects, namely simplicityin model structure, scalability in model size, flexibility in trainingparadigm, and transferability of knowledge between models, through a simplebaseline model dubbed ViTPose. Specifically, ViTPose employs the plain andnon-hierarchical vision transformer as an encoder to encode features and alightweight decoder to decode body keypoints in either a top-down or abottom-up manner. It can be scaled up from about 20M to 1B parameters by takingadvantage of the scalable model capacity and high parallelism of the visiontransformer, setting a new Pareto front for throughput and performance.Besides, ViTPose is very flexible regarding the attention type, inputresolution, and pre-training and fine-tuning strategy. Based on theflexibility, a novel ViTPose+ model is proposed to deal with heterogeneous bodykeypoint categories in different types of body pose estimation tasks viaknowledge factorization, i.e., adopting task-agnostic and task-specificfeed-forward networks in the transformer. We also empirically demonstrate thatthe knowledge of large ViTPose models can be easily transferred to small onesvia a simple knowledge token. Experimental results show that our ViTPose modeloutperforms representative methods on the challenging MS COCO Human KeypointDetection benchmark at both top-down and bottom-up settings. Furthermore, ourViTPose+ model achieves state-of-the-art performance simultaneously on a seriesof body pose estimation tasks, including MS COCO, AI Challenger, OCHuman, MPIIfor human keypoint detection, COCO-Wholebody for whole-body keypoint detection,as well as AP-10K and APT-36K for animal keypoint detection, withoutsacrificing inference speed.", "output": "ViTPose++: Vision Transformer Foundation Model for Generic Body Pose Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Periocular refers to the externally visible region of the face that surroundsthe eye socket. This feature-rich area can provide accurate identification inunconstrained or uncooperative scenarios, where the iris or face modalities maynot offer sufficient biometric cues due to factors such as partial occlusion orhigh subject-to-camera distance. The COVID-19 pandemic has further highlightedits importance, as the ocular region remained the only visible facial area evenin controlled settings due to the widespread use of masks. This paper discussesthe state of the art in periocular biometrics, presenting an overall frameworkencompassing its most significant research aspects, which include: (a) oculardefinition, acquisition, and detection; (b) identity recognition, includingcombination with other modalities and use of various spectra; and (c) ocularsoft-biometric analysis. Finally, we conclude by addressing current challengesand proposing future directions.", "output": "Periocular Biometrics: A Modality for Unconstrained Scenarios."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a pre-trained neural network for handling event cameradata. Our model is a self-supervised learning framework, and uses paired eventcamera data and natural RGB images for training.Our method contains three modules connected in a sequence: i) a family ofevent data augmentations, generating meaningful event images forself-supervised training; ii) a conditional masking strategy to sampleinformative event patches from event images, encouraging our model to capturethe spatial layout of a scene and accelerating training; iii) a contrastivelearning approach, enforcing the similarity of embeddings between matchingevent images, and between paired event and RGB images. An embedding projectionloss is proposed to avoid the model collapse when enforcing the event imageembedding similarities. A probability distribution alignment loss is proposedto encourage the event image to be consistent with its paired RGB image in thefeature space.Transfer learning performance on downstream tasks shows the superiority ofour method over state-of-the-art methods. For example, we achieve top-1accuracy at 64.83% on the N-ImageNet dataset.", "output": "Event Camera Data Pre-training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Collaborative perception is essential to address occlusion and sensor failureissues in autonomous driving. In recent years, theoretical and experimentalinvestigations of novel works for collaborative perception have increasedtremendously. So far, however, few reviews have focused on systematicalcollaboration modules and large-scale collaborative perception datasets. Thiswork reviews recent achievements in this field to bridge this gap and motivatefuture research. We start with a brief overview of collaboration schemes. Afterthat, we systematically summarize the collaborative perception methods forideal scenarios and real-world issues. The former focus on collaborationmodules and efficiency, and the latter is devoted to addressing the problems inactual application. Furthermore, we present large-scale public datasets andsummarize quantitative results on these benchmarks. Finally, we highlight gapsand overlooked challenges between current academic research and real-worldapplications.", "output": "Collaborative Perception in Autonomous Driving: Methods, Datasets and Challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous driving (AD) perception today relies heavily on deep learningbased architectures requiring large scale annotated datasets with theirassociated costs for curation and annotation. The 3D semantic data are usefulfor core perception tasks such as obstacle detection and ego-vehiclelocalization. We propose a new dataset, Navya 3D Segmentation (Navya3DSeg),with a diverse label space corresponding to a large scale production gradeoperational domain, including rural, urban, industrial sites and universitiesfrom 13 countries. It contains 23 labeled sequences and 25 supplementarysequences without labels, designed to explore self-supervised andsemi-supervised semantic segmentation benchmarks on point clouds. We alsopropose a novel method for sequential dataset split generation based oniterative multi-label stratification, and demonstrated to achieve a +1.2% mIoUimprovement over the original split proposed by SemanticKITTI dataset. Acomplete benchmark for semantic segmentation task was performed, with state ofthe art methods. Finally, we demonstrate an Active Learning (AL) based datasetdistillation framework. We introduce a novel heuristic-free sampling methodcalled ego-pose distance based sampling in the context of AL. A detailedpresentation on the dataset is available here", "output": "Navya3DSeg -- Navya 3D Semantic Segmentation Dataset & split generation for autonomous vehicles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The CLIP model has been recently proven to be very effective for a variety ofcross-modal tasks, including the evaluation of captions generated fromvision-and-language architectures. In this paper, we propose a new recipe for acontrastive-based evaluation metric for image captioning, namelyPositive-Augmented Contrastive learning Score (PAC-S), that in a novel wayunifies the learning of a contrastive visual-semantic space with the additionof generated images and text on curated data. Experiments spanning severaldatasets demonstrate that our new metric achieves the highest correlation withhuman judgments on both images and videos, outperforming existingreference-based metrics like CIDEr and SPICE and reference-free metrics likeCLIP-Score. Finally, we test the system-level correlation of the proposedmetric when considering popular image captioning approaches, and assess theimpact of employing different cross-modal features. Our source code and trainedmodels are publicly available at: ", "output": "Positive-Augmented Contrastive Learning for Image and Video Captioning Evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although point cloud registration has achieved remarkable advances inobject-level and indoor scenes, large-scale registration methods are rarelyexplored. Challenges mainly arise from the huge point number, complexdistribution, and outliers of outdoor LiDAR scans. In addition, most existingregistration works generally adopt a two-stage paradigm: They first findcorrespondences by extracting discriminative local features, and then leverageestimators (eg. RANSAC) to filter outliers, which are highly dependent onwell-designed descriptors and post-processing choices. To address theseproblems, we propose an end-to-end transformer network (RegFormer) forlarge-scale point cloud alignment without any further post-processing.Specifically, a projection-aware hierarchical transformer is proposed tocapture long-range dependencies and filter outliers by extracting pointfeatures globally. Our transformer has linear complexity, which guarantees highefficiency even for large-scale scenes. Furthermore, to effectively reducemismatches, a bijective association transformer is designed for regressing theinitial transformation. Extensive experiments on KITTI and NuScenes datasetsdemonstrate that our RegFormer achieves competitive performance in terms ofboth accuracy and efficiency.", "output": "RegFormer: An Efficient Projection-Aware Transformer Network for Large-Scale Point Cloud Registration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a new, provably-convergent algorithm for computing theflag-mean and flag-median of a set of points on a flag manifold under thechordal metric. The flag manifold is a mathematical space consisting of flags,which are sequences of nested subspaces of a vector space that increase indimension. The flag manifold is a superset of a wide range of known matrixspaces, including Stiefel and Grassmanians, making it a general object that isuseful in a wide variety computer vision problems.To tackle the challenge of computing first order flag statistics, we firsttransform the problem into one that involves auxiliary variables constrained tothe Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, andleveraging the numerical stability and efficiency of Stiefel-manifoldoptimization enables us to compute the flag-mean effectively. Through a seriesof experiments, we show the competence of our method in Grassmann and rotationaveraging, as well as principal component analysis. We release our source codeunder ", "output": "Chordal Averaging on Flag Manifolds and Its Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Successful deployment of artificial intelligence (AI) in various settings hasled to numerous positive outcomes for individuals and society. However, AIsystems have also been shown to harm parts of the population due to biasedpredictions. AI fairness focuses on mitigating such biases to ensure AIdecision making is not discriminatory towards certain groups. We take a closerlook at AI fairness and analyze how lack of AI fairness can lead to deepeningof biases over time and act as a social stressor. More specifically, we discusshow biased models can lead to more negative real-world outcomes for certaingroups, which may then become more prevalent by deploying new AI models trainedon increasingly biased data, resulting in a feedback loop. If the issuespersist, they could be reinforced by interactions with other risks and havesevere implications on society in the form of social unrest. We examine currentstrategies for improving AI fairness, assess their limitations in terms ofreal-world deployment, and explore potential paths forward to ensure we reapAI's benefits without causing society's collapse.", "output": "Fairness in AI and Its Long-Term Implications on Society."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The application of visual tracking to the performance analysis of sportsplayers in dynamic competitions is vital for effective coaching. In doublesmatches, coordinated positioning is crucial for maintaining control of thecourt and minimizing opponents' scoring opportunities. The analysis of suchteamwork plays a vital role in understanding the dynamics of the game. However,previous studies have primarily focused on analyzing and assessing singlesplayers without considering occlusion in broadcast videos. These studies haverelied on discrete representations, which involve the analysis andrepresentation of specific actions (e.g., strokes) or events that occur duringthe game while overlooking the meaningful spatial distribution. In this work,we present the first annotated drone dataset from top and back views inbadminton doubles and propose a framework to estimate the control areaprobability map, which can be used to evaluate teamwork performance. We presentan efficient framework of deep neural networks that enables the calculation offull probability surfaces. This framework utilizes the embedding of a Gaussianmixture map of players' positions and employs graph convolution on their poses.In the experiment, we verify our approach by comparing various baselines anddiscovering the correlations between the score and control area. Additionally,we propose a practical application for assessing optimal positioning to provideinstructions during a game. Our approach offers both visual and quantitativeevaluations of players' movements, thereby providing valuable insights intodoubles teamwork. The dataset and related project code is available at", "output": "Estimation of control area in badminton doubles with pose information from top and back view drone videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Statistical Shape Modeling (SSM) is a valuable tool for investigating andquantifying anatomical variations within populations of anatomies. However,traditional correspondence-based SSM generation methods have a prohibitiveinference process and require complete geometric proxies (e.g., high-resolutionbinary volumes or surface meshes) as input shapes to construct the SSM.Unordered 3D point cloud representations of shapes are more easily acquiredfrom various medical imaging practices (e.g., thresholded images and surfacescanning). Point cloud deep networks have recently achieved remarkable successin learning permutation-invariant features for different point cloud tasks(e.g., completion, semantic segmentation, classification). However, theirapplication to learning SSM from point clouds is to-date unexplored. In thiswork, we demonstrate that existing point cloud encoder-decoder-based completionnetworks can provide an untapped potential for SSM, capturing population-levelstatistical representations of shapes while reducing the inference burden andrelaxing the input requirement. We discuss the limitations of these techniquesto the SSM application and suggest future improvements. Our work paves the wayfor further exploration of point cloud deep learning for SSM, a promisingavenue for advancing shape analysis literature and broadening SSM to diverseuse cases.", "output": "Can point cloud networks learn statistical shape models of anatomies?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Statistical shape modeling (SSM) enables population-based quantitativeanalysis of anatomical shapes, informing clinical diagnosis. Deep learningapproaches predict correspondence-based SSM directly from unsegmented 3D imagesbut require calibrated uncertainty quantification, motivating Bayesianformulations. Variational information bottleneck DeepSSM (VIB-DeepSSM) is aneffective, principled framework for predicting probabilistic shapes of anatomyfrom images with aleatoric uncertainty quantification. However, VIB is onlyhalf-Bayesian and lacks epistemic uncertainty inference. We derive a fullyBayesian VIB formulation and demonstrate the efficacy of two scalableimplementation approaches: concrete dropout and batch ensemble. Additionally,we introduce a novel combination of the two that further enhances uncertaintycalibration via multimodal marginalization. Experiments on synthetic shapes andleft atrium data demonstrate that the fully Bayesian VIB network predicts SSMfrom images with improved uncertainty reasoning without sacrificing accuracy.", "output": "Fully Bayesian VIB-DeepSSM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The 3rd Anti-UAV Workshop &amp; Challenge aims to encourage research indeveloping novel and accurate methods for multi-scale object tracking. TheAnti-UAV dataset used for the Anti-UAV Challenge has been publicly released.There are two main differences between this year's competition and the previoustwo. First, we have expanded the existing dataset, and for the first time,released a training set so that participants can focus on improving theirmodels. Second, we set up two tracks for the first time, i.e., Anti-UAVTracking and Anti-UAV Detection &amp; Tracking. Around 76 participating teams fromthe globe competed in the 3rd Anti-UAV Challenge. In this paper, we provide abrief summary of the 3rd Anti-UAV Workshop &amp; Challenge including briefintroductions to the top three methods in each track. The submissionleaderboard will be reopened for researchers that are interested in theAnti-UAV challenge. The benchmark dataset and other information can be foundat: ", "output": "The 3rd Anti-UAV Workshop & Challenge: Methods and Results."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional Neural Networks (CNNs) have made significant strides in medicalimage analysis in recent years. However, the local nature of the convolutionoperator may pose a limitation for capturing global and long-range interactionsin CNNs. Recently, Transformers have gained popularity in the computer visioncommunity and also medical image segmentation due to their ability to processglobal features effectively. The scalability issues of self-attention mechanismand lack of the CNN-like inductive bias may have limited their adoption.Therefore, hybrid Vision transformers (CNN-Transformer), exploiting advantagesof both Convolution and Self-attention Mechanisms, have gained importance. Inthis work, we present MaxViT-UNet, an Encoder-Decoder based hybrid visiontransformer (CNN-Transformer) for medical image segmentation. The proposedHybrid Decoder, based on MaxViT-block, is designed to harness the power of boththe convolution and self-attention mechanisms at each decoding stage withnominal computational burden. The inclusion of multi-axis self-attention,within each decoder stage, significantly enhances the discriminating capacitybetween the object and background regions, and thereby helps in improving thesegmentation efficiency. In the Hybrid Decoder block, the fusion processcommences by integrating the upsampled lower level decoder features, obtainedthrough transpose convolution, with the skip-connection features derived fromthe hybrid encoder. Subsequently, the fused features undergo refinement throughthe utilization of a multi-axis attention mechanism. The proposed decoder blockis repeated multiple times to progressively segment the nuclei regions.Experimental results on MoNuSeg18 and MoNuSAC20 dataset demonstrates theeffectiveness of the proposed technique.", "output": "MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The accurate modelling of the Point Spread Function (PSF) is of paramountimportance in astronomical observations, as it allows for the correction ofdistortions and blurring caused by the telescope and atmosphere. PSF modellingis crucial for accurately measuring celestial objects' properties. The lastdecades brought us a steady increase in the power and complexity ofastronomical telescopes and instruments. Upcoming galaxy surveys like Euclidand LSST will observe an unprecedented amount and quality of data. Modellingthe PSF for these new facilities and surveys requires novel modellingtechniques that can cope with the ever-tightening error requirements. Thepurpose of this review is three-fold. First, we introduce the opticalbackground required for a more physically-motivated PSF modelling and proposean observational model that can be reused for future developments. Second, weprovide an overview of the different physical contributors of the PSF,including the optic- and detector-level contributors and the atmosphere. Weexpect that the overview will help better understand the modelled effects.Third, we discuss the different methods for PSF modelling from the parametricand non-parametric families for ground- and space-based telescopes, with theiradvantages and limitations. Validation methods for PSF models are thenaddressed, with several metrics related to weak lensing studies discussed indetail. Finally, we explore current challenges and future directions in PSFmodelling for astronomical telescopes.", "output": "Point spread function modelling for astronomical telescopes: a review focused on weak gravitational lensing studies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Open-vocabulary object detection has benefited greatly from pretrainedvision-language models, but is still limited by the amount of availabledetection training data. While detection training data can be expanded by usingWeb image-text pairs as weak supervision, this has not been done at scalescomparable to image-level pretraining. Here, we scale up detection data withself-training, which uses an existing detector to generate pseudo-boxannotations on image-text pairs. Major challenges in scaling self-training arethe choice of label space, pseudo-annotation filtering, and trainingefficiency. We present the OWLv2 model and OWL-ST self-training recipe, whichaddress these challenges. OWLv2 surpasses the performance of previousstate-of-the-art open-vocabulary detectors already at comparable trainingscales (~10M examples). However, with OWL-ST, we can scale to over 1B examples,yielding further large improvement: With an L/14 architecture, OWL-ST improvesAP on LVIS rare classes, for which the model has seen no human box annotations,from 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scaletraining for open-world localization, similar to what has been seen for imageclassification and language modelling.", "output": "Scaling Open-Vocabulary Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Regular group convolutional neural networks (G-CNNs) have been shown toincrease model performance and improve equivariance to different geometricalsymmetries. This work addresses the problem of SE(3), i.e., roto-translationequivariance, on volumetric data. Volumetric image data is prevalent in manymedical settings. Motivated by the recent work on separable group convolutions,we devise a SE(3) group convolution kernel separated into a continuous SO(3)(rotation) kernel and a spatial kernel. We approximate equivariance to thecontinuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernelis parameterized via RBF interpolation on similarly uniform grids. Wedemonstrate the advantages of our approach in volumetric medical imageanalysis. Our SE(3) equivariant models consistently outperform CNNs and regulardiscrete G-CNNs on challenging medical classification tasks and showsignificantly improved generalization capabilities. Our approach achieves up toa 16.5% gain in accuracy over regular CNNs.", "output": "Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning-based deformable registration methods have been widelyinvestigated in diverse medical applications. Learning-based deformableregistration relies on weighted objective functions trading off registrationaccuracy and smoothness of the deformation field. Therefore, they inevitablyrequire tuning the hyperparameter for optimal registration performance. Tuningthe hyperparameters is highly computationally expensive and introducesundesired dependencies on domain knowledge. In this study, we construct aregistration model based on the gradient surgery mechanism, named GSMorph, toachieve a hyperparameter-free balance on multiple losses. In GSMorph, wereformulate the optimization procedure by projecting the gradient of similarityloss orthogonally to the plane associated with the smoothness constraint,rather than additionally introducing a hyperparameter to balance these twocompeting terms. Furthermore, our method is model-agnostic and can be mergedinto any deep registration network without introducing extra parameters orslowing down inference. In this study, We compared our method withstate-of-the-art (SOTA) deformable registration approaches over two publiclyavailable cardiac MRI datasets. GSMorph proves superior to five SOTAlearning-based registration models and two conventional registrationtechniques, SyN and Demons, on both registration accuracy and smoothness.", "output": "GSMorph: Gradient Surgery for cine-MRI Cardiac Deformable Registration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Though the advancement of pre-trained large language models unfolds, theexploration of building a unified model for language and other multi-modaldata, such as motion, remains challenging and untouched so far. Fortunately,human motion displays a semantic coupling akin to human language, oftenperceived as a form of body language. By fusing language data with large-scalemotion models, motion-language pre-training that can enhance the performance ofmotion-related tasks becomes feasible. Driven by this insight, we proposeMotionGPT, a unified, versatile, and user-friendly motion-language model tohandle multiple motion-relevant tasks. Specifically, we employ the discretevector quantization for human motion and transfer 3D motion into motion tokens,similar to the generation process of word tokens. Building upon this \"motionvocabulary\", we perform language modeling on both motion and text in a unifiedmanner, treating human motion as a specific language. Moreover, inspired byprompt learning, we pre-train MotionGPT with a mixture of motion-language dataand fine-tune it on prompt-based question-and-answer tasks. Extensiveexperiments demonstrate that MotionGPT achieves state-of-the-art performanceson multiple motion tasks including text-driven motion generation, motioncaptioning, motion prediction, and motion in-between.", "output": "MotionGPT: Human Motion as a Foreign Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "State-of-the-art deep learning-based registration methods employ threedifferent learning strategies: supervised learning, which requires costlymanual annotations, unsupervised learning, which heavily relies on hand-craftedsimilarity metrics designed by domain experts, or learning from synthetic data,which introduces a domain shift. To overcome the limitations of thesestrategies, we propose a novel self-supervised learning paradigm forunsupervised registration, relying on self-training. Our idea is based on twokey insights. Feature-based differentiable optimizers 1) perform reasonableregistration even from random features and 2) stabilize the training of thepreceding feature extraction network on noisy labels. Consequently, we proposecyclical self-training, where pseudo labels are initialized as the displacementfields inferred from random features and cyclically updated based on more andmore expressive features from the learning feature extractor, yielding aself-reinforcement effect. We evaluate the method for abdomen and lungregistration, consistently surpassing metric-based supervision andoutperforming diverse state-of-the-art competitors. Source code is available at", "output": "Unsupervised 3D registration through optimization-guided cyclical self-training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Underwater images are fundamental for studying and understanding the statusof marine life. We focus on reducing the memory space required for imagestorage while the memory space consumption in the collecting phase limits thetime lasting of this phase leading to the need for more image collectioncampaigns. We present a novel machine-learning model that reconstructs thecolours of underwater images from their luminescence channel, thus saving 2/3of the available storage space. Our model specialises in underwater colourreconstruction and consists of an encoder-decoder architecture. The encoder iscomposed of a convolutional encoder and a parallel specialised classifiertrained with webly-supervised data. The encoder and the decoder use layers ofcapsules to capture the features of the entities in the image. The colourreconstruction process recalls the progressive and the generative adversarialtraining procedures. The progressive training gives the ground for a generativeadversarial routine focused on the refining of colours giving the image brightand saturated colours which bring the image back to life. We validate the modelboth qualitatively and quantitatively on four benchmark datasets. This is thefirst attempt at colour reconstruction in greyscale underwater images.Extensive results on four benchmark datasets demonstrate that our solutionoutperforms state-of-the-art (SOTA) solutions. We also demonstrate that thegenerated colourisation enhances the quality of images compared to enhancementmodels at the SOTA.", "output": "UW-ProCCaps: UnderWater Progressive Colourisation with Capsules."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Trustworthy deployment of deep learning medical imaging models intoreal-world clinical practice requires that they be calibrated. However, modelsthat are well calibrated overall can still be poorly calibrated for asub-population, potentially resulting in a clinician unwittingly making poordecisions for this group based on the recommendations of the model. Althoughmethods have been shown to successfully mitigate biases across subgroups interms of model accuracy, this work focuses on the open problem of mitigatingcalibration biases in the context of medical image analysis. Our method doesnot require subgroup attributes during training, permitting the flexibility tomitigate biases for different choices of sensitive attributes withoutre-training. To this end, we propose a novel two-stage method: Cluster-Focal tofirst identify poorly calibrated samples, cluster them into groups, and thenintroduce group-wise focal loss to improve calibration bias. We evaluate ourmethod on skin lesion classification with the public HAM10000 dataset, and onpredicting future lesional activity for multiple sclerosis (MS) patients. Inaddition to considering traditional sensitive attributes (e.g. age, sex) withdemographic subgroups, we also consider biases among groups with differentimage-derived attributes, such as lesion load, which are required in medicalimage analysis. Our results demonstrate that our method effectively controlscalibration error in the worst-performing subgroups while preserving predictionperformance, and outperforming recent baselines.", "output": "Mitigating Calibration Bias Without Fixed Attribute Grouping for Improved Fairness in Medical Imaging Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models recently have been successfully applied for the visualsynthesis of strikingly realistic appearing images. This raises strong concernsabout their potential for malicious purposes. In this paper, we propose usingthe lightweight multi Local Intrinsic Dimensionality (multiLID), which has beenoriginally developed in context of the detection of adversarial examples, forthe automatic detection of synthetic images and the identification of theaccording generator networks. In contrast to many existing detectionapproaches, which often only work for GAN-generated images, the proposed methodprovides close to perfect detection results in many realistic use cases.Extensive experiments on known and newly created datasets demonstrate that theproposed multiLID approach exhibits superiority in diffusion detection andmodel identification. Since the empirical evaluations of recent publications onthe detection of generated images are often mainly focused on the\"LSUN-Bedroom\" dataset, we further establish a comprehensive benchmark for thedetection of diffusion-generated images, including samples from severaldiffusion models with different image sizes.", "output": "Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated radiology report generation aims to generate radiology reports thatcontain rich, fine-grained descriptions of radiology imaging. Compared withimage captioning in the natural image domain, medical images are very similarto each other, with only minor differences in the occurrence of diseases. Giventhe importance of these minor differences in the radiology report, it iscrucial to encourage the model to focus more on the subtle regions of diseaseoccurrence. Secondly, the problem of visual and textual data biases is serious.Not only do normal cases make up the majority of the dataset, but sentencesdescribing areas with pathological changes also constitute only a small part ofthe paragraph. Lastly, generating medical image reports involves the challengeof long text generation, which requires more expertise and empirical trainingin medical knowledge. As a result, the difficulty of generating such reports isincreased. To address these challenges, we propose a disease-oriented retrievalframework that utilizes similar reports as prior knowledge references. Wedesign a factual consistency captioning generator to generate more accurate andfactually consistent disease descriptions. Our framework can find most similarreports for a given disease from the CXR database by retrieving adisease-oriented mask consisting of the position and morphologicalcharacteristics. By referencing the disease-oriented similar report and thevisual features, the factual consistency model can generate a more accurateradiology report.", "output": "Reading Radiology Imaging Like The Radiologist."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is imperative to ensure the robustness of deep learning models in criticalapplications such as, healthcare. While recent advances in deep learning haveimproved the performance of volumetric medical image segmentation models, thesemodels cannot be deployed for real-world applications immediately due to theirvulnerability to adversarial attacks. We present a 3D frequency domainadversarial attack for volumetric medical image segmentation models anddemonstrate its advantages over conventional input or voxel domain attacks.Using our proposed attack, we introduce a novel frequency domain adversarialtraining approach for optimizing a robust model against voxel and frequencydomain attacks. Moreover, we propose frequency consistency loss to regulate ourfrequency domain adversarial training that achieves a better tradeoff betweenmodel's performance on clean and adversarial samples. Code is publiclyavailable at ", "output": "Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image retrieval-based cross-view localization methods often lead to verycoarse camera pose estimation, due to the limited sampling density of thedatabase satellite images. In this paper, we propose a method to increase theaccuracy of a ground camera's location and orientation by estimating therelative rotation and translation between the ground-level image and itsmatched/retrieved satellite image. Our approach designs a geometry-guidedcross-view transformer that combines the benefits of conventional geometry andlearnable cross-view transformers to map the ground-view observations to anoverhead view. Given the synthesized overhead view and observed satellitefeature maps, we construct a neural pose optimizer with strong globalinformation embedding ability to estimate the relative rotation between them.After aligning their rotations, we develop an uncertainty-guided spatialcorrelation to generate a probability map of the vehicle locations, from whichthe relative translation can be determined. Experimental results demonstratethat our method significantly outperforms the state-of-the-art. Notably, thelikelihood of restricting the vehicle lateral pose to be within 1m of itsGround Truth (GT) value on the cross-view KITTI dataset has been improved from$35.54%$ to $76.44%$, and the likelihood of restricting the vehicleorientation to be within $1^{circ}$ of its GT value has been improved from$19.64%$ to $99.10%$.", "output": "Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via Geometry-Guided Cross-View Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We contribute to the sparsely populated area of unsupervised deep graphmatching with application to keypoint matching in images. Contrary to thestandard emph{supervised} approach, our method does not require ground truthcorrespondences between keypoint pairs. Instead, it is self-supervised byenforcing consistency of matchings between images of the same object category.As the matching and the consistency loss are discrete, their derivatives cannotbe straightforwardly used for learning. We address this issue in a principledway by building our method upon the recent results on black-box differentiationof combinatorial solvers. This makes our method exceptionally flexible, as itis compatible with arbitrary network architectures and combinatorial solvers.Our experimental evaluation suggests that our technique sets a newstate-of-the-art for unsupervised graph matching.", "output": "Unsupervised Deep Graph Matching Based on Cycle Consistency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Facial expression recognition (FER) remains a challenging task due to theambiguity of expressions. The derived noisy labels significantly harm theperformance in real-world scenarios. To address this issue, we present a newFER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarksto mitigate the impact of label noise from two perspectives. Firstly, LA-Netuses landmark information to suppress the uncertainty in expression space andconstructs the label distribution of each sample by neighborhood aggregation,which in turn improves the quality of training supervision. Secondly, the modelincorporates landmark information into expression representations using thedevised expression-landmark contrastive loss. The enhanced expression featureextractor can be less susceptible to label noise. Our method can be integratedwith any deep neural network for better training supervision withoutintroducing extra inference costs. We conduct extensive experiments on bothin-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Netachieves state-of-the-art performance.", "output": "LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Typically, object detection methods for autonomous driving that rely onsupervised learning make the assumption of a consistent feature distributionbetween the training and testing data, however such assumption may fail indifferent weather conditions. Due to the domain gap, a detection model trainedunder clear weather may not perform well in foggy and rainy conditions.Overcoming detection bottlenecks in foggy and rainy weather is a real challengefor autonomous vehicles deployed in the wild. To bridge the domain gap andimprove the performance of object detectionin foggy and rainy weather, thispaper presents a novel framework for domain-adaptive object detection. Theadaptations at both the image-level and object-level are intended to minimizethe differences in image style and object appearance between domains.Furthermore, in order to improve the model's performance on challengingexamples, we introduce a novel adversarial gradient reversal layer thatconducts adversarial mining on difficult instances in addition to domainadaptation. Additionally, we suggest generating an auxiliary domain throughdata augmentation to enforce a new domain-level metric regularization.Experimental findings on public V2V benchmark exhibit a substantial enhancementin object detection specifically for foggy and rainy driving scenarios.", "output": "Domain Adaptation based Enhanced Detection for Autonomous Driving in Foggy and Rainy Weather."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To deliver the artistic expression of the target style, recent studiesexploit the attention mechanism owing to its ability to map the local patchesof the style image to the corresponding patches of the content image. However,because of the low semantic correspondence between arbitrary content andartworks, the attention module repeatedly abuses specific local patches fromthe style image, resulting in disharmonious and evident repetitive artifacts.To overcome this limitation and accomplish impeccable artistic style transfer,we focus on enhancing the attention mechanism and capturing the rhythm ofpatterns that organize the style. In this paper, we introduce a novel metric,namely pattern repeatability, that quantifies the repetition of patterns in thestyle image. Based on the pattern repeatability, we propose AestheticPattern-Aware style transfer Networks (AesPA-Net) that discover the sweet spotof local and global style expressions. In addition, we propose a novelself-supervisory task to encourage the attention mechanism to learn precise andmeaningful semantic correspondence. Lastly, we introduce the patch-wise styleloss to transfer the elaborate rhythm of local patterns. Through qualitativeand quantitative evaluations, we verify the reliability of the proposed patternrepeatability that aligns with human perception, and demonstrate thesuperiority of the proposed framework.", "output": "AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Talking head video generation aims to animate a human face in a still imagewith dynamic poses and expressions using motion information derived from atarget-driving video, while maintaining the person's identity in the sourceimage. However, dramatic and complex motions in the driving video causeambiguous generation, because the still source image cannot provide sufficientappearance information for occluded regions or delicate expression variations,which produces severe artifacts and significantly degrades the generationquality. To tackle this problem, we propose to learn a global facialrepresentation space, and design a novel implicit identity representationconditioned memory compensation network, coined as MCNet, for high-fidelitytalking head generation.~Specifically, we devise a network module to learn aunified spatial facial meta-memory bank from all training samples, which canprovide rich facial structure and appearance priors to compensate warped sourcefacial features for the generation. Furthermore, we propose an effective querymechanism based on implicit identity representations learned from the discretekeypoints of the source image. It can greatly facilitate the retrieval of morecorrelated information from the memory bank for the compensation. Extensiveexperiments demonstrate that MCNet can learn representative and complementaryfacial memory, and can clearly outperform previous state-of-the-art talkinghead generation methods on VoxCeleb1 and CelebV datasets. Please check ourhref{", "output": "Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated medical image classification is the key component in intelligentdiagnosis systems. However, most medical image datasets contain plenty ofsamples of common diseases and just a handful of rare ones, leading to majorclass imbalances. Currently, it is an open problem in intelligent diagnosis toeffectively learn from imbalanced training data. In this paper, we propose asimple yet effective framework, named textbf{C}lass textbf{A}ttention totextbf{RE}gions of the lesion (CARE), to handle data imbalance issues byembedding attention into the training process of textbf{C}onvolutionaltextbf{N}eural textbf{N}etworks (CNNs). The proposed attention module helpsCNNs attend to lesion regions of rare diseases, therefore helping CNNs to learntheir characteristics more effectively. In addition, this attention moduleworks only during the training phase and does not change the architecture ofthe original network, so it can be directly combined with any existing CNNarchitecture. The CARE framework needs bounding boxes to represent the lesionregions of rare diseases. To alleviate the need for manual annotation, wefurther developed variants of CARE by leveraging the traditional saliencymethods or a pretrained segmentation model for bounding box generation. Resultsshow that the CARE variants with automated bounding box generation arecomparable to the original CARE framework with textit{manual} bounding boxannotations. A series of experiments on an imbalanced skin image dataset and apneumonia dataset indicates that our method can effectively help the networkfocus on the lesion regions of rare diseases and remarkably improves theclassification performance of rare diseases.", "output": "Class Attention to Regions of Lesion for Imbalanced Medical Image Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Millions of vehicles are transported every year, tightly parked in vessels orboats. To reduce the risks of associated safety issues like fires, knowing thelocation of vehicles is essential, since different vehicles may need differentmitigation measures, e.g. electric cars. This work is aimed at creating asolution based on a nano-drone that navigates across rows of parked vehiclesand detects their license plates. We do so via a wall-following algorithm, anda CNN trained to detect license plates. All computations are done in real-timeon the drone, which just sends position and detected images that allow thecreation of a 2D map with the position of the plates. Our solution is capableof reading all plates across eight test cases (with several rows of plates,different drone speeds, or low light) by aggregation of measurements acrossseveral drone journeys.", "output": "Drone navigation and license place detection for vehicle location in indoor spaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Advances in neuroscience have enabled researchers to measure the activitiesof large numbers of neurons simultaneously in behaving animals. We have accessto the fluorescence of each of the neurons which provides a first-orderapproximation of the neural activity over time. Determining the exact spike ofa neuron from this fluorescence trace constitutes an active area of researchwithin the field of computational neuroscience. We propose a novel Bayesianapproach based on a mixture of half-non-local prior densities and point massesfor this task. Instead of a computationally expensive MCMC algorithm, we adopta stochastic search-based approach that is capable of taking advantage ofmodern computing environments often equipped with multiple processors, toexplore all possible arrangements of spikes and lack thereof in an observedspike train. It then reports the highest posterior probability arrangement ofspikes and posterior probability for a spike at each location of the spiketrain. Our proposals lead to substantial improvements over existing proposalsbased on L1 regularization, and enjoy comparable estimation accuracy to thestate-of-the-art L0 proposal, in simulations, and on recent calcium imagingdata sets. Notably, contrary to optimization-based frequentist approaches, ourmethodology yields automatic uncertainty quantification associated with thespike-train inference.", "output": "Bayesian Spike Train Inference via Non-Local Priors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autism spectrum disorder(ASD) is a lifelong neurodevelopmental condition thataffects social communication and behavior. Investigating functional magneticresonance imaging (fMRI)-based brain functional connectome can aid in theunderstanding and diagnosis of ASD, leading to more effective treatments. Thebrain is modeled as a network of brain Regions of Interest (ROIs), and ROIsform communities and knowledge of these communities is crucial for ASDdiagnosis. On the one hand, Transformer-based models have proven to be highlyeffective across several tasks, including fMRI connectome analysis to learnuseful representations of ROIs. On the other hand, existing transformer-basedmodels treat all ROIs equally and overlook the impact of community-specificassociations when learning node embeddings. To fill this gap, we propose anovel method, Com-BrainTF, a hierarchical local-global transformer architecturethat learns intra and inter-community aware node embeddings for ASD predictiontask. Furthermore, we avoid over-parameterization by sharing the localtransformer parameters for different communities but optimize unique learnableprompt tokens for each community. Our model outperforms state-of-the-art (SOTA)architecture on ABIDE dataset and has high interpretability, evident from theattention module. Our code is available at", "output": "Community-Aware Transformer for Autism Prediction in fMRI Connectome."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Backdoor attacks pose serious security threats to deep neural networks(DNNs). Backdoored models make arbitrarily (targeted) incorrect predictions oninputs embedded with well-designed triggers while behaving normally on cleaninputs. Many works have explored the invisibility of backdoor triggers toimprove attack stealthiness. However, most of them only consider theinvisibility in the spatial domain without explicitly accounting for thegeneration of invisible triggers in the frequency domain, making the generatedpoisoned images be easily detected by recent defense methods. To address thisissue, in this paper, we propose a DUal stealthy BAckdoor attack method namedDUBA, which simultaneously considers the invisibility of triggers in both thespatial and frequency domains, to achieve desirable attack performance, whileensuring strong stealthiness. Specifically, we first use Discrete WaveletTransform to embed the high-frequency information of the trigger image into theclean image to ensure attack effectiveness. Then, to attain strongstealthiness, we incorporate Fourier Transform and Discrete Cosine Transform tomix the poisoned image and clean image in the frequency domain. Moreover, theproposed DUBA adopts a novel attack strategy, in which the model is trainedwith weak triggers and attacked with strong triggers to further enhance theattack performance and stealthiness. We extensively evaluate DUBA againstpopular image classifiers on four datasets. The results demonstrate that itsignificantly outperforms the state-of-the-art backdoor attacks in terms of theattack success rate and stealthiness", "output": "A Dual Stealthy Backdoor: From Both Spatial and Frequency Perspectives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hyperspectral images have significant applications in various domains, sincethey register numerous semantic and spatial information in the spectral bandwith spatial variability of spectral signatures. Two critical challenges inidentifying pixels of the hyperspectral image are respectively representing thecorrelated information among the local and global, as well as the abundantparameters of the model. To tackle this challenge, we propose a Multi-ScaleU-shape Multi-Layer Perceptron (MUMLP) a model consisting of the designed MSC(Multi-Scale Channel) block and the UMLP (U-shape Multi-Layer Perceptron)structure. MSC transforms the channel dimension and mixes spectral band featureto embed the deep-level representation adequately. UMLP is designed by theencoder-decoder structure with multi-layer perceptron layers, which is capableof compressing the large-scale parameters. Extensive experiments are conductedto demonstrate our model can outperform state-of-the-art methodsacross-the-board on three wide-adopted public datasets, namely PaviaUniversity, Houston 2013 and Houston 2018", "output": "Multi-Scale U-Shape MLP for Hyperspectral Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We examine the privacy-enhancing properties of subsampling a data set viaimportance sampling as a pre-processing step for differentially privatemechanisms. This extends the established privacy amplification by subsamplingresult to importance sampling where each data point is weighted by thereciprocal of its selection probability. The implications for privacy ofweighting each point are not obvious. On the one hand, a lower selectionprobability leads to a stronger privacy amplification. On the other hand, thehigher the weight, the stronger the influence of the point on the output of themechanism in the event that the point does get selected. We provide a generalresult that quantifies the trade-off between these two effects. We show thatheterogeneous sampling probabilities can lead to both stronger privacy andbetter utility than uniform subsampling while retaining the subsample size. Inparticular, we formulate and solve the problem of privacy-optimal sampling,that is, finding the importance weights that minimize the expected subset sizesubject to a given privacy budget. Empirically, we evaluate the privacy,efficiency, and accuracy of importance sampling-based privacy amplification onthe example of k-means clustering.", "output": "Privacy Amplification via Importance Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models(LLMs)have become effective tools for natural languageprocessing and have been used in many different fields. This essay offers asuccinct summary of various LLM subcategories. The survey emphasizes recentdevelopments and efforts made for various LLM kinds, including task-basedfinancial LLMs, multilingual language LLMs, biomedical and clinical LLMs,vision language LLMs, and code language models. The survey gives a generalsummary of the methods, attributes, datasets, transformer models, andcomparison metrics applied in each category of LLMs. Furthermore, it highlightsunresolved problems in the field of developing chatbots and virtual assistants,such as boosting natural language processing, enhancing chatbot intelligence,and resolving moral and legal dilemmas. The purpose of this study is to providereaders, developers, academics, and users interested in LLM-based chatbots andvirtual intelligent assistant technologies with useful information and futuredirections.", "output": "Several categories of Large Language Models (LLMs): A Short Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One barrier to the clinical deployment of deep learning-based models is thepresence of images at runtime that lie far outside the training distribution ofa given model. We aim to detect these out-of-distribution (OOD) images with agenerative adversarial network (GAN). Our training dataset was comprised of3,234 liver-containing computed tomography (CT) scans from 456 patients. OurOOD test data consisted of CT images of the brain, head and neck, lung, cervix,and abnormal livers. A StyleGAN2-ADA architecture was employed to model thetraining distribution. Images were reconstructed using backpropagation.Reconstructions were evaluated using the Wasserstein distance, mean squarederror, and the structural similarity index measure. OOD detection was evaluatedwith the area under the receiver operating characteristic curve (AUROC). Ourparadigm distinguished between liver and non-liver CT with greater than 90%AUROC. It was also completely unable to reconstruct liver artifacts, such asneedles and ascites.", "output": "StyleGAN2-based Out-of-Distribution Detection for Medical Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Divorce is the legal dissolution of a marriage by a court. Since this isusually an unpleasant outcome of a marital union, each party may have reasonsto call the decision to quit which is generally documented in detail in thecourt proceedings. Via a substantial corpus of 17,306 court proceedings, thispaper investigates gender inequality through the lens of divorce courtproceedings. While emerging data sources (e.g., public court records) onsensitive societal issues hold promise in aiding social science research,biases present in cutting-edge natural language processing (NLP) methods mayinterfere with or affect such studies. We thus require a thorough analysis ofpotential gaps and limitations present in extant NLP resources. In this paper,on the methodological side, we demonstrate that existing NLP resources requiredseveral non-trivial modifications to quantify societal inequalities. On thesubstantive side, we find that while a large number of court cases perhapssuggest changing norms in India where women are increasingly challengingpatriarchy, AI-powered analyses of these court proceedings indicate strikinggender inequality with women often subjected to domestic violence.", "output": "Disentangling Societal Inequality from Model Biases: Gender Inequality in Divorce Court Proceedings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In modern recommendation systems, unbiased learning-to-rank (LTR) is crucialfor prioritizing items from biased implicit user feedback, such as click data.Several techniques, such as Inverse Propensity Weighting (IPW), have beenproposed for single-sided markets. However, less attention has been paid totwo-sided markets, such as job platforms or dating services, where successfulconversions require matching preferences from both users. This paper addressesthe complex interaction of biases between users in two-sided markets andproposes a tailored LTR approach. We first present a formulation of feedbackmechanisms in two-sided matching platforms and point out that their implicitfeedback may include position bias from both user groups. On the basis of thisobservation, we extend the IPW estimator and propose a new estimator, namedtwo-sided IPW, to address the position bases in two-sided markets. We provethat the proposed estimator satisfies the unbiasedness for the ground-truthranking metric. We conducted numerical experiments on real-world two-sidedplatforms and demonstrated the effectiveness of our proposed method in terms ofboth precision and robustness. Our experiments showed that our methodoutperformed baselines especially when handling rare items, which are lessfrequently observed in the training data.", "output": "An IPW-based Unbiased Ranking Metric in Two-sided Markets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study adversarial training on datasets that obey thelong-tailed distribution, which is practical but rarely explored in previousworks. Compared with conventional adversarial training on balanced datasets,this process falls into the dilemma of generating uneven adversarial examples(AEs) and an unbalanced feature embedding space, causing the resulting model toexhibit low robustness and accuracy on tail data. To combat that, we propose anew adversarial training framework -- Re-balancing Adversarial Training (REAT).This framework consists of two components: (1) a new training strategy inspiredby the term effective number to guide the model to generate more balanced andinformative AEs; (2) a carefully constructed penalty function to force asatisfactory feature space. Evaluation results on different datasets and modelstructures prove that REAT can effectively enhance the model's robustness andpreserve the model's clean accuracy. The code can be found in", "output": "Adversarial Training Over Long-Tailed Distribution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Non-intrusive Load Monitoring (NILM) algorithms, commonly referred to as loaddisaggregation algorithms, are fundamental tools for effective energymanagement. Despite the success of deep models in load disaggregation, theyface various challenges, particularly those pertaining to privacy and security.This paper investigates the sensitivity of prominent deep NILM baselines toadversarial attacks, which have proven to be a significant threat in domainssuch as computer vision and speech recognition. Adversarial attacks entail theintroduction of imperceptible noise into the input data with the aim ofmisleading the neural network into generating erroneous outputs. We investigatethe Fast Gradient Sign Method (FGSM), a well-known adversarial attack, toperturb the input sequences fed into two commonly employed CNN-based NILMbaselines: the Sequence-to-Sequence (S2S) and Sequence-to-Point (S2P) models.Our findings provide compelling evidence for the vulnerability of these models,particularly the S2P model which exhibits an average decline of 20% in theF1-score even with small amounts of noise. Such weakness has the potential togenerate profound implications for energy management systems in residential andindustrial sectors reliant on NILM models.", "output": "On the Sensitivity of Deep Load Disaggregation to Adversarial Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cyber Threat Intelligence (CTI) plays a crucial role in assessing risks andenhancing security for organizations. However, the process of extractingrelevant information from unstructured text sources can be expensive andtime-consuming. Our empirical experience shows that existing tools forautomated structured CTI extraction have performance limitations. Furthermore,the community lacks a common benchmark to quantitatively assess theirperformance. We fill these gaps providing a new large open benchmark datasetand aCTIon, a structured CTI information extraction tool. The dataset includes204 real-world publicly available reports and their corresponding structuredCTI information in STIX format. Our team curated the dataset involving threeindependent groups of CTI analysts working over the course of several months.To the best of our knowledge, this dataset is two orders of magnitude largerthan previously released open source datasets. We then design aCTIon,leveraging recently introduced large language models (GPT3.5) in the context oftwo custom information extraction pipelines. We compare our method with 10solutions presented in previous work, for which we develop our ownimplementations when open-source implementations were lacking. Our results showthat aCTIon outperforms previous work for structured CTI extraction with animprovement of the F1-score from 10%points to 50%points across all tasks.", "output": "Time for aCTIon: Automated Analysis of Cyber Threat Intelligence in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Stemming from traditional knowledge graphs (KGs), hyper-relational KGs (HKGs)provide additional key-value pairs (i.e., qualifiers) for each KG fact thathelp to better restrict the fact validity. In recent years, there has been anincreasing interest in studying graph reasoning over HKGs. In the meantime, dueto the ever-evolving nature of world knowledge, extensive parallel works havebeen focusing on reasoning over temporal KGs (TKGs), where each TKG fact can beviewed as a KG fact coupled with a timestamp (or time period) specifying itstime validity. The existing HKG reasoning approaches do not consider temporalinformation because it is not explicitly specified in previous benchmarkdatasets. Besides, all the previous TKG reasoning methods only lay emphasis ontemporal reasoning and have no way to learn from qualifiers. To this end, weaim to fill the gap between TKG reasoning and HKG reasoning. We develop two newbenchmark hyper-relational TKG (HTKG) datasets, i.e., Wiki-hy and YAGO-hy, andpropose a HTKG reasoning model that efficiently models both temporal facts andqualifiers. We further exploit additional time-invariant relational knowledgefrom the Wikidata knowledge base and study its effectiveness in HTKG reasoning.Time-invariant relational knowledge serves as the knowledge that remainsunchanged in time (e.g., Sasha Obama is the child of Barack Obama), and it hasnever been fully explored in previous TKG reasoning benchmarks and approaches.Experimental results show that our model substantially outperforms previousrelated methods on HTKG link prediction and can be enhanced by jointlyleveraging both temporal and time-invariant relational knowledge.", "output": "Exploring Link Prediction over Hyper-Relational Temporal Knowledge Graphs Enhanced with Time-Invariant Relational Knowledge."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Clinical Practice Guidelines (CPGs) for cancer diseases evolve rapidly due tonew evidence generated by active research. Currently, CPGs are primarilypublished in a document format that is ill-suited for managing this developingknowledge. A knowledge model of the guidelines document suitable forprogrammatic interaction is required. This work proposes an automated methodfor extraction of knowledge from National Comprehensive Cancer Network (NCCN)CPGs in Oncology and generating a structured model containing the retrievedknowledge. The proposed method was tested using two versions of NCCN Non-SmallCell Lung Cancer (NSCLC) CPG to demonstrate the effectiveness in faithfulextraction and modeling of knowledge. Three enrichment strategies using Cancerstaging information, Unified Medical Language System (UMLS) Metathesaurus &amp;National Cancer Institute thesaurus (NCIt) concepts, and Node classificationare also presented to enhance the model towards enabling programmatic traversaland querying of cancer care guidelines. The Node classification was performedusing a Support Vector Machine (SVM) model, achieving a classification accuracyof 0.81 with 10-fold cross-validation.", "output": "Automated Knowledge Modeling for Cancer Clinical Practice Guidelines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study presents a thorough examination of various Generative PretrainedTransformer (GPT) methodologies in sentiment analysis, specifically in thecontext of Task 4 on the SemEval 2017 dataset. Three primary strategies areemployed: 1) prompt engineering using the advanced GPT-3.5 Turbo, 2)fine-tuning GPT models, and 3) an inventive approach to embeddingclassification. The research yields detailed comparative insights among thesestrategies and individual GPT models, revealing their unique strengths andpotential limitations. Additionally, the study compares these GPT-basedmethodologies with other contemporary, high-performing models previously usedwith the same dataset. The results illustrate the significant superiority ofthe GPT approaches in terms of predictive performance, more than 22% inF1-score compared to the state-of-the-art. Further, the paper addresses commonchallenges in sentiment analysis tasks, such as understanding context anddetecting sarcasm. It underscores the enhanced capabilities of the GPT modelsto effectively navigate these complexities. Collectively, these findingshighlight the promising potential of GPT models in sentiment analysis, settingthe stage for future research in this field. The code can be found at", "output": "SentimentGPT: Exploiting GPT for Advanced Sentiment Analysis and its Departure from Current Machine Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face recognition from image sets acquired under unregulated and uncontrolledsettings, such as at large distances, low resolutions, varying viewpoints,illumination, pose, and atmospheric conditions, is challenging. Face featureaggregation, which involves aggregating a set of N feature representationspresent in a template into a single global representation, plays a pivotal rolein such recognition systems. Existing works in traditional face featureaggregation either utilize metadata or high-dimensional intermediate featurerepresentations to estimate feature quality for aggregation. However,generating high-quality metadata or style information is not feasible forextremely low-resolution faces captured in long-range and high altitudesettings. To overcome these limitations, we propose a feature distributionconditioning approach called CoNAN for template aggregation. Specifically, ourmethod aims to learn a context vector conditioned over the distributioninformation of the incoming feature set, which is utilized to weigh thefeatures based on their estimated informativeness. The proposed method producesstate-of-the-art results on long-range unconstrained face recognition datasetssuch as BTS, and DroneSURF, validating the advantages of such an aggregationstrategy.", "output": "CoNAN: Conditional Neural Aggregation Network For Unconstrained Face Feature Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep recommendation systems (DRS) heavily depend on specialized HPC hardwareand accelerators to optimize energy, efficiency, and recommendation quality.Despite the growing number of hardware errors observed in large-scale fleetsystems where DRS are deployed, the robustness of DRS has been largelyoverlooked. This paper presents the first systematic study of DRS robustnessagainst hardware errors. We develop Terrorch, a user-friendly, efficient andflexible error injection framework on top of the widely-used PyTorch. Weevaluate a wide range of models and datasets and observe that the DRSrobustness against hardware errors is influenced by various factors from modelparameters to input characteristics. We also explore 3 error mitigation methodsincluding algorithm based fault tolerance (ABFT), activation clipping andselective bit protection (SBP). We find that applying activation clipping canrecover up to 30% of the degraded AUC-ROC score, making it a promisingmitigation method.", "output": "Evaluating and Enhancing Robustness of Deep Recommendation Systems Against Hardware Errors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How does the brain represent different modes of information? Can we design asystem that automatically understands what the user is thinking? Such questionscan be answered by studying brain recordings like functional magnetic resonanceimaging (fMRI). As a first step, the neuroscience community has contributedseveral large cognitive neuroscience datasets related to passivereading/listening/viewing of concept words, narratives, pictures and movies.Encoding and decoding models using these datasets have also been proposed inthe past two decades. These models serve as additional tools for basic researchin cognitive science and neuroscience. Encoding models aim at generating fMRIbrain representations given a stimulus automatically. They have severalpractical applications in evaluating and diagnosing neurological conditions andthus also help design therapies for brain damage. Decoding models solve theinverse problem of reconstructing the stimuli given the fMRI. They are usefulfor designing brain-machine or brain-computer interfaces. Inspired by theeffectiveness of deep learning models for natural language processing, computervision, and speech, recently several neural encoding and decoding models havebeen proposed. In this survey, we will first discuss popular representations oflanguage, vision and speech stimuli, and present a summary of neurosciencedatasets. Further, we will review popular deep learning based encoding anddecoding architectures and note their benefits and limitations. Finally, wewill conclude with a brief summary and discussion about future trends. Giventhe large amount of recently published work in the `computational cognitiveneuroscience' community, we believe that this survey nicely organizes theplethora of work and presents it as a coherent story.", "output": "Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Action models, which take the form of precondition/effect axioms, facilitatecausal and motivational connections between actions for AI agents. Action modelacquisition has been identified as a bottleneck in the application of planningtechnology, especially within narrative planning. Acquiring action models fromnarrative texts in an automated way is essential, but challenging because ofthe inherent complexities of such texts. We present NaRuto, a system thatextracts structured events from narrative text and subsequently generatesplanning-language-style action models based on predictions of commonsense eventrelations, as well as textual contradictions and similarities, in anunsupervised manner. Experimental results in classical narrative planningdomains show that NaRuto can generate action models of significantly betterquality than existing fully automated methods, and even on par with those ofsemi-automated methods.", "output": "Automated Action Model Acquisition from Narrative Texts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cyber threat attribution is the process of identifying the actor of an attackincident in cyberspace. An accurate and timely threat attribution plays animportant role in deterring future attacks by applying appropriate and timelydefense mechanisms. Manual analysis of attack patterns gathered by honeypotdeployments, intrusion detection systems, firewalls, and via trace-backprocedures is still the preferred method of security analysts for cyber threatattribution. Such attack patterns are low-level Indicators of Compromise (IOC).They represent Tactics, Techniques, Procedures (TTP), and software tools usedby the adversaries in their campaigns. The adversaries rarely re-use them. Theycan also be manipulated, resulting in false and unfair attribution. Toempirically evaluate and compare the effectiveness of both kinds of IOC, thereare two problems that need to be addressed. The first problem is that in recentresearch works, the ineffectiveness of low-level IOC for cyber threatattribution has been discussed intuitively. An empirical evaluation for themeasure of the effectiveness of low-level IOC based on a real-world dataset ismissing. The second problem is that the available dataset for high-level IOChas a single instance for each predictive class label that cannot be useddirectly for training machine learning models. To address these problems inthis research work, we empirically evaluate the effectiveness of low-level IOCbased on a real-world dataset that is specifically built for comparativeanalysis with high-level IOC. The experimental results show that the high-levelIOC trained models effectively attribute cyberattacks with an accuracy of 95%as compared to the low-level IOC trained models where accuracy is 40%.", "output": "A Machine Learning based Empirical Evaluation of Cyber Threat Actors High Level Attack Patterns over Low level Attack Patterns in Attributing Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Non-core drilling has gradually become the primary exploration method ingeological engineering, and well logging curves have increasingly gainedimportance as the main carriers of geological information. However, factorssuch as geological environment, logging equipment, borehole quality, andunexpected events can all impact the quality of well logging curves. Previousmethods of re-logging or manual corrections have been associated with highcosts and low efficiency. This paper proposes a machine learning method thatutilizes existing data to predict missing well logging curves, and itseffectiveness and feasibility have been validated through experiments. Theproposed method builds upon the traditional Long Short-Term Memory (LSTM)neural network by incorporating a self-attention mechanism to analyze thespatial dependencies of the data. It selectively includes the dominantcomputational results in the LSTM, reducing the computational complexity fromO(n^2) to O(nlogn) and improving model efficiency. Experimental resultsdemonstrate that the proposed method achieves higher accuracy compared totraditional curve synthesis methods based on Fully Connected Neural Networks(FCNN) and LSTM. This accurate, efficient, and cost-effective prediction methodholds practical value in engineering applications.", "output": "Efficient selective attention LSTM for well log curve synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Effective and efficient malware detection is at the forefront of researchinto building secure digital systems. As with many other fields, malwaredetection research has seen a dramatic increase in the application of machinelearning algorithms. One machine learning technique that has been used widelyin the field of pattern matching in general-and malware detection inparticular-is hidden Markov models (HMMs). HMM training is based on a hillclimb, and hence we can often improve a model by training multiple times withdifferent initial values. In this research, we compare boosted HMMs (usingAdaBoost) to HMMs trained with multiple random restarts, in the context ofmalware detection. These techniques are applied to a variety of challengingmalware datasets. We find that random restarts perform surprisingly well incomparison to boosting. Only in the most difficult \"cold start\" cases (wheretraining data is severely limited) does boosting appear to offer sufficientimprovement to justify its higher computational cost in the scoring phase.", "output": "Hidden Markov Models with Random Restarts vs Boosting for Malware Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hands-on cybersecurity training allows students and professionals to practicevarious tools and improve their technical skills. The training occurs in aninteractive learning environment that enables completing sophisticated tasks infull-fledged operating systems, networks, and applications. During thetraining, the learning environment allows collecting data about trainees'interactions with the environment, such as their usage of command-line tools.These data contain patterns indicative of trainees' learning processes, andrevealing them allows to assess the trainees and provide feedback to help themlearn. However, automated analysis of these data is challenging. The trainingtasks feature complex problem-solving, and many different solution approachesare possible. Moreover, the trainees generate vast amounts of interaction data.This paper explores a dataset from 18 cybersecurity training sessions usingdata mining and machine learning techniques. We employed pattern mining andclustering to analyze 8834 commands collected from 113 trainees, revealingtheir typical behavior, mistakes, solution strategies, and difficult trainingstages. Pattern mining proved suitable in capturing timing information and toolusage frequency. Clustering underlined that many trainees often face the sameissues, which can be addressed by targeted scaffolding. Our results show thatdata mining methods are suitable for analyzing cybersecurity training data.Educational researchers and practitioners can apply these methods in theircontexts to assess trainees, support them, and improve the training design.Artifacts associated with this research are publicly available.", "output": "Student Assessment in Cybersecurity Training Automated by Pattern Mining and Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This document provides a comprehensive guide to hyperparameter tuning usingspotPython for scikit-learn, PyTorch, and river. The first part introducesspotPython's surrogate model-based optimization process, while the second partfocuses on hyperparameter tuning. Several case studies are presented, includinghyperparameter tuning for sklearn models such as Support Vector Classification,Random Forests, Gradient Boosting (XGB), and K-nearest neighbors (KNN), as wellas a Hoeffding Adaptive Tree Regressor from river. The integration ofspotPython into the PyTorch and PyTorch Lightning training workflow is alsodiscussed. With a hands-on approach and step-by-step explanations, thiscookbook serves as a practical starting point for anyone interested inhyperparameter tuning with Python. Highlights include the interplay betweenTensorboard, PyTorch Lightning, spotPython, and river. This publication isunder development, with updates available on the corresponding webpage.", "output": "Hyperparameter Tuning Cookbook: A guide for scikit-learn, PyTorch, river, and spotPython."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Neural Networks (DNNs) have emerged as an effective approach to tacklingreal-world problems. However, like human-written software,automatically-generated DNNs can have bugs and be attacked. This thus attractsmany recent interests in developing effective and scalable DNN verificationtechniques and tools. In this work, we introduce a NeuralSAT, a new constraintsolving approach to DNN verification. The design of NeuralSAT follows theDPLL(T) algorithm used modern SMT solving, which includes (conflict) clauselearning, abstraction, and theory solving, and thus NeuralSAT can be consideredas an SMT framework for DNNs. Preliminary results show that the NeuralSATprototype is competitive to the state-of-the-art. We hope, with properoptimization and engineering, NeuralSAT will carry the power and success ofmodern SAT/SMT solvers to DNN verification. NeuralSAT is avaliable from:", "output": "A DPLL(T) Framework for Verifying Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose a method to create domain-sensitive speechrecognition models that utilize textual domain information by conditioning itsgeneration on a given text prompt. This is accomplished by fine-tuning apre-trained, end-to-end model (Whisper) to learn from demonstrations withprompt examples. We show that this ability can be generalized to differentdomains and even various prompt contexts, with our model gaining a Word ErrorRate (WER) reduction of up to 33% on unseen datasets from various domains, suchas medical conversation, air traffic control communication, and financialmeetings. Considering the limited availability of audio-transcript pair data,we further extend our method to text-only fine-tuning to achieve domainsensitivity as well as domain adaptation. We demonstrate that our text-onlyfine-tuned model can also attend to various prompt contexts, with the modelreaching the most WER reduction of 29% on the medical conversation dataset.", "output": "Zero-shot Domain-sensitive Speech Recognition with Prompt-conditioning Fine-tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present ECSIC, a novel learned method for stereo imagecompression. Our proposed method compresses the left and right images in ajoint manner by exploiting the mutual information between the images of thestereo image pair using a novel stereo cross attention (SCA) module and twostereo context modules. The SCA module performs cross-attention restricted tothe corresponding epipolar lines of the two images and processes them inparallel. The stereo context modules improve the entropy estimation of thesecond encoded image by using the first image as a context. We conduct anextensive ablation study demonstrating the effectiveness of the proposedmodules and a comprehensive quantitative and qualitative comparison withexisting methods. ECSIC achieves state-of-the-art performance among stereoimage compression models on the two popular stereo image datasets Cityscapesand InStereo2k while allowing for fast encoding and decoding, making it highlypractical for real-time applications.", "output": "ECSIC: Epipolar Cross Attention for Stereo Image Compression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mammography images are widely used to detect non-palpable breast lesions ornodules, preventing cancer and providing the opportunity to plan interventionswhen necessary. The identification of some structures of interest is essentialto make a diagnosis and evaluate image adequacy. Thus, computer-aided detectionsystems can be helpful in assisting medical interpretation by automaticallysegmenting these landmark structures. In this paper, we propose a deeplearning-based framework for the segmentation of the nipple, the pectoralmuscle, the fibroglandular tissue, and the fatty tissue on standard-viewmammography images. We introduce a large private segmentation dataset andextensive experiments considering different deep-learning model architectures.Our experiments demonstrate accurate segmentation performance on variate andchallenging cases, showing that this framework can be integrated into clinicalpractice.", "output": "Towards Automated Semantic Segmentation in Mammography Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Since distribution shifts are common in real-world applications, there is apressing need for developing prediction models that are robust against suchshifts. Existing frameworks, such as empirical risk minimization ordistributionally robust optimization, either lack generalizability for unseendistributions or rely on postulated distance measures. Alternatively, causalityoffers a data-driven and structural perspective to robust predictions. However,the assumptions necessary for causal inference can be overly stringent, and therobustness offered by such causal models often lacks flexibility. In thispaper, we focus on causality-oriented robustness and propose DistributionalRobustness via Invariant Gradients (DRIG), a method that exploits generaladditive interventions in training data for robust predictions against unseeninterventions, and naturally interpolates between in-distribution predictionand causality. In a linear setting, we prove that DRIG yields predictions thatare robust among a data-dependent class of distribution shifts. Furthermore, weshow that our framework includes anchor regression (Rothenh\"ausler et al.2021) as a special case, and that it yields prediction models that protectagainst more diverse perturbations. We extend our approach to thesemi-supervised domain adaptation setting to further improve predictionperformance. Finally, we empirically validate our methods on syntheticsimulations and on single-cell data.", "output": "Causality-oriented robustness: exploiting general additive interventions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we carefully investigate how we can use multiple differentNatural Language Processing techniques and methods in order to automaticallyrecognize the main actions in sports events. We aim to extract insights byanalyzing live sport commentaries from different sources and by classifyingthese major actions into different categories. We also study if sentimentanalysis could help detect these main actions.", "output": "Analyzing sports commentary in order to automatically recognize events and extract insights."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Polyffusion, a diffusion model that generates polyphonic musicscores by regarding music as image-like piano roll representations. The modelis capable of controllable music generation with two paradigms: internalcontrol and external control. Internal control refers to the process in whichusers pre-define a part of the music and then let the model infill the rest,similar to the task of masked music generation (or music inpainting). Externalcontrol conditions the model with external yet related information, such aschord, texture, or other features, via the cross-attention mechanism. We showthat by using internal and external controls, Polyffusion unifies a wide rangeof music creation tasks, including melody generation given accompaniment,accompaniment generation given melody, arbitrary music segment inpainting, andmusic arrangement given chords or textures. Experimental results show that ourmodel significantly outperforms existing Transformer and sampling-basedbaselines, and using pre-trained disentangled representations as externalconditions yields more effective controls.", "output": "Polyffusion: A Diffusion Model for Polyphonic Score Generation with Internal and External Controls."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human beings always engage in a vast range of activities and tasks thatdemonstrate their ability to adapt to different scenarios. Any human activitycan be represented as a temporal sequence of actions performed to achieve acertain goal. Unlike the time series datasets extracted from electronics ormachines, these action sequences are highly disparate in their nature -- thetime to finish a sequence of actions can vary between different persons.Therefore, understanding the dynamics of these sequences is essential for manydownstream tasks such as activity length prediction, goal prediction, nextaction recommendation, etc. Existing neural network-based approaches that learna continuous-time activity sequence (or CTAS) are limited to the presence ofonly visual data or are designed specifically for a particular task, i.e.,limited to next action or goal prediction. In this paper, we present ProActive,a neural marked temporal point process (MTPP) framework for modeling thecontinuous-time distribution of actions in an activity sequence whilesimultaneously addressing three high-impact problems -- next action prediction,sequence-goal prediction, and end-to-end sequence generation. Specifically, weutilize a self-attention module with temporal normalizing flows to model theinfluence and the inter-arrival times between actions in a sequence. Inaddition, we propose a novel addition over the ProActive model that can handlevariations in the order of actions, i.e., different methods of achieving agiven goal. We demonstrate that this variant can learn the order in which theperson or actor prefers to do their actions. Extensive experiments on sequencesderived from three activity recognition datasets show the significant accuracyboost of ProActive over the state-of-the-art in terms of action and goalprediction, and the first-ever application of end-to-end action sequencegeneration.", "output": "Tapestry of Time and Actions: Modeling Human Activity Sequences using Temporal Point Process Flows."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Music can evoke various emotions, and with the advancement of technology, ithas become more accessible to people. Bangla music, which portrays differenthuman emotions, lacks sufficient research. The authors of this article aim toanalyze Bangla songs and classify their moods based on the lyrics. To achievethis, this research has compiled a dataset of 4000 Bangla song lyrics, genres,and used Natural Language Processing and the Bert Algorithm to analyze thedata. Among the 4000 songs, 1513 songs are represented for the sad mood, 1362for the romantic mood, 886 for happiness, and the rest 239 are classified asrelaxation. By embedding the lyrics of the songs, the authors have classifiedthe songs into four moods: Happy, Sad, Romantic, and Relaxed. This research iscrucial as it enables a multi-class classification of songs' moods, making themusic more relatable to people's emotions. The article presents the automatedresult of the four moods accurately derived from the song lyrics.", "output": "Mood Classification of Bangla Songs Based on Lyrics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) offers a collaborative training framework, allowingmultiple clients to contribute to a shared model without compromising dataprivacy. Due to the heterogeneous nature of local datasets, updated clientmodels may overfit and diverge from one another, commonly known as the problemof client drift. In this paper, we propose FedBug (Federated Learning withBottom-Up Gradual Unfreezing), a novel FL framework designed to effectivelymitigate client drift. FedBug adaptively leverages the client model parameters,distributed by the server at each global round, as the reference points forcross-client alignment. Specifically, on the client side, FedBug begins byfreezing the entire model, then gradually unfreezes the layers, from the inputlayer to the output layer. This bottom-up approach allows models to train thenewly thawed layers to project data into a latent space, wherein the separatinghyperplanes remain consistent across all clients. We theoretically analyzeFedBug in a novel over-parameterization FL setup, revealing its superiorconvergence rate compared to FedAvg. Through comprehensive experiments,spanning various datasets, training conditions, and network architectures, wevalidate the efficacy of FedBug. Our contributions encompass a novel FLframework, theoretical analysis, and empirical validation, demonstrating thewide potential and applicability of FedBug.", "output": "FedBug: A Bottom-Up Gradual Unfreezing Framework for Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vertical federated learning (VFL) enables multiple parties with disjointfeatures of a common user set to train a machine learning model without sharingtheir private data. Tree-based models have become prevalent in VFL due to theirinterpretability and efficiency. However, the vulnerability of tree-based VFLhas not been sufficiently investigated. In this study, we first introduce anovel label inference attack, ID2Graph, which utilizes the sets of record-IDsassigned to each node (i.e., instance space) to deduce private training labels.The ID2Graph attack generates a graph structure from training samples, extractscommunities from the graph, and clusters the local dataset using communityinformation. To counteract label leakage from the instance space, we propose aneffective defense mechanism, ID-LMID, which prevents label leakage by focusingon mutual information regularization. Comprehensive experiments conducted onvarious datasets reveal that the ID2Graph attack presents significant risks totree-based models such as Random Forest and XGBoost. Further evaluations onthese benchmarks demonstrate that ID-LMID effectively mitigates label leakagein such instances.", "output": "Eliminating Label Leakage in Tree-Based Vertical Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Research is facing a reproducibility crisis, in which the results andfindings of many studies are difficult or even impossible to reproduce. This isalso the case in machine learning (ML) and artificial intelligence (AI)research. Often, this is the case due to unpublished data and/or source-code,and due to sensitivity to ML training conditions. Although different solutionsto address this issue are discussed in the research community such as using MLplatforms, the level of reproducibility in ML-driven research is not increasingsubstantially. Therefore, in this mini survey, we review the literature onreproducibility in ML-driven research with three main aims: (i) reflect on thecurrent situation of ML reproducibility in various research fields, (ii)identify reproducibility issues and barriers that exist in these researchfields applying ML, and (iii) identify potential drivers such as tools,practices, and interventions that support ML reproducibility. With this, wehope to contribute to decisions on the viability of different solutions forsupporting ML reproducibility.", "output": "Reproducibility in Machine Learning-Driven Research."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Differentiable Search Index is a recently proposed paradigm for documentretrieval, that encodes information about a corpus of documents within theparameters of a neural network and directly maps queries to correspondingdocuments. These models have achieved state-of-the-art performances fordocument retrieval across many benchmarks. These kinds of models have asignificant limitation: it is not easy to add new documents after a model istrained. We propose IncDSI, a method to add documents in real time (about20-50ms per document), without retraining the model on the entire dataset (oreven parts thereof). Instead we formulate the addition of documents as aconstrained optimization problem that makes minimal changes to the networkparameters. Although orders of magnitude faster, our approach is competitivewith re-training the model on the whole dataset and enables the development ofdocument retrieval systems that can be updated with new information inreal-time. Our code for IncDSI is available at", "output": "IncDSI: Incrementally Updatable Document Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Prokaryotic gene prediction plays an important role in understanding thebiology of organisms and their function with applications in medicine andbiotechnology. Although the current gene finders are highly sensitive infinding long genes, their sensitivity decreases noticeably in finding shortergenes (&lt;180 nts). The culprit is insufficient annotated gene data to identifydistinguishing features in short open reading frames (ORFs). We develop a deeplearning-based method called ProtiGeno, specifically targeting shortprokaryotic genes using a protein language model trained on millions of evolvedproteins. In systematic large-scale experiments on 4,288 prokaryotic genomes,we demonstrate that ProtiGeno predicts short coding and noncoding genes withhigher accuracy and recall than the current state-of-the-art gene finders. Wediscuss the predictive features of ProtiGeno and possible limitations byvisualizing the three-dimensional structure of the predicted short genes. Data,codes, and models are available at ", "output": "ProtiGeno: a prokaryotic short gene finder using protein language models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have been massively applied to many tasks, oftensurpassing state-of-the-art approaches. While their effectiveness in codegeneration has been extensively studied (e.g., AlphaCode), their potential forcode detection remains unexplored.This work presents the first analysis of code detection using LLMs. Our studyexamines essential kernels, including matrix multiplication, convolution, andfast-fourier transform, implemented in C/C++. We propose both a preliminary,naive prompt and a novel prompting strategy for code detection.Results reveal that conventional prompting achieves great precision but pooraccuracy (68.8%, 22.3%, and 79.2% for GEMM, convolution, and FFT, respectively)due to a high number of false positives. Our novel prompting strategysubstantially reduces false positives, resulting in excellent overall accuracy(91.1%, 97.9%, and 99.7%, respectively). These results pose a considerablechallenge to existing state-of-the-art code detection methods.", "output": "Code Detection for Hardware Acceleration Using Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Massive web datasets play a key role in the success of large vision-languagemodels like CLIP and Flamingo. However, the raw web data is noisy, and existingfiltering methods to reduce noise often come at the expense of data diversity.Our work focuses on caption quality as one major source of noise, and studieshow generated captions can increase the utility of web-scraped datapoints withnondescript text. Through exploring different mixing strategies for raw andgenerated captions, we outperform the best filtering method proposed by theDataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given acandidate pool of 128M image-text pairs. Our best approach is also 2x better atFlickr and MS-COCO retrieval. We then analyze what makes synthetic captions aneffective source of text supervision. In experimenting with different imagecaptioning models, we also demonstrate that the performance of a model onstandard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliableindicator of the utility of the captions it generates for multimodal training.Finally, our experiments with using generated captions at DataComp's largescale (1.28B image-text pairs) offer insights into the limitations of synthetictext, as well as the importance of image curation with increasing training dataquantity.", "output": "Improving Multimodal Datasets with Image Captioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Sliced Wasserstein (SW) distance has become a popular alternative to theWasserstein distance for comparing probability measures. Widespreadapplications include image processing, domain adaptation and generativemodelling, where it is common to optimise some parameters in order to minimiseSW, which serves as a loss function between discrete probability measures(since measures admitting densities are numerically unattainable). All theseoptimisation problems bear the same sub-problem, which is minimising the SlicedWasserstein energy. In this paper we study the properties of $mathcal{E}: Ylongmapsto mathrm{SW}_2^2(gamma_Y, gamma_Z)$, i.e. the SW distance betweentwo uniform discrete measures with the same amount of points as a function ofthe support $Y in mathbb{R}^{n times d}$ of one of the measures. Weinvestigate the regularity and optimisation properties of this energy, as wellas its Monte-Carlo approximation $mathcal{E}_p$ (estimating the expectation inSW using only $p$ samples) and show convergence results on the critical pointsof $mathcal{E}_p$ to those of $mathcal{E}$, as well as an almost-sure uniformconvergence. Finally, we show that in a certain sense, Stochastic GradientDescent methods minimising $mathcal{E}$ and $mathcal{E}_p$ converge towards(Clarke) critical points of these energies.", "output": "Properties of Discrete Sliced Wasserstein Losses."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolution Neural Networks trained for the task of lens finding with similararchitecture and training data as is commonly found in the literature arebiased classifiers. An understanding of the selection function of lens findingneural networks will be key to fully realising the potential of the largesamples of strong gravitational lens systems that will be found in upcomingwide-field surveys. We use three training datasets, representative of thoseused to train galaxy-galaxy and galaxy-quasar lens finding neural networks. Thenetworks preferentially select systems with larger Einstein radii and largersources with more concentrated source-light distributions. Increasing thedetection significance threshold to 12$sigma$ from 8$sigma$ results in 50 percent of the selected strong lens systems having Einstein radii$theta_mathrm{E}$ $ge$ 1.04 arcsec from $theta_mathrm{E}$ $ge$ 0.879arcsec, source radii $R_S$ $ge$ 0.194 arcsec from $R_S$ $ge$ 0.178 arcsec andsource S'ersic indices $n_{mathrm{Sc}}^{mathrm{S}}$ $ge$ 2.62 from$n_{mathrm{Sc}}^{mathrm{S}}$ $ge$ 2.55. The model trained to find lensedquasars shows a stronger preference for higher lens ellipticities than thosetrained to find lensed galaxies. The selection function is independent of theslope of the power-law of the mass profiles, hence measurements of thisquantity will be unaffected. The lens finder selection function reinforces thatof the lensing cross-section, and thus we expect our findings to be a generalresult for all galaxy-galaxy and galaxy-quasar lens finding neural networks.", "output": "Selection functions of strong lens finding neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Part-prototype models are explainable-by-design image classifiers, and apromising alternative to black box AI. This paper explores the applicabilityand potential of interpretable machine learning, in particular PIP-Net, forautomated diagnosis support on real-world medical imaging data. PIP-Net learnshuman-understandable prototypical image parts and we evaluate its accuracy andinterpretability for fracture detection and skin cancer diagnosis. We find thatPIP-Net's decision making process is in line with medical classificationstandards, while only provided with image-level class labels. Because ofPIP-Net's unsupervised pretraining of prototypes, data quality problems such asundesired text in an X-ray or labelling errors can be easily identified.Additionally, we are the first to show that humans can manually correct thereasoning of PIP-Net by directly disabling undesired prototypes. We concludethat part-prototype models are promising for medical applications due to theirinterpretability and potential for advanced model debugging.", "output": "Interpreting and Correcting Medical Image Classification with PIP-Net."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Earth system forecasting has traditionally relied on complex physical modelsthat are computationally expensive and require significant domain expertise. Inthe past decade, the unprecedented increase in spatiotemporal Earth observationdata has enabled data-driven forecasting models using deep learning techniques.These models have shown promise for diverse Earth system forecasting tasks buteither struggle with handling uncertainty or neglect domain-specific priorknowledge, resulting in averaging possible futures to blurred forecasts orgenerating physically implausible predictions. To address these limitations, wepropose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1)We develop PreDiff, a conditional latent diffusion model capable ofprobabilistic forecasts. 2) We incorporate an explicit knowledge controlmechanism to align forecasts with domain-specific physical constraints. This isachieved by estimating the deviation from imposed constraints at each denoisingstep and adjusting the transition distribution accordingly. We conductempirical studies on two datasets: N-body MNIST, a synthetic dataset withchaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset.Specifically, we impose the law of conservation of energy in N-body MNIST andanticipated precipitation intensity in SEVIR. Experiments demonstrate theeffectiveness of PreDiff in handling uncertainty, incorporating domain-specificprior knowledge, and generating forecasts that exhibit high operationalutility.", "output": "PreDiff: Precipitation Nowcasting with Latent Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The generation of synthetic tabular data that preserves differential privacyis a problem of growing importance. While traditional marginal-based methodshave achieved impressive results, recent work has shown that deeplearning-based approaches tend to lag behind. In this work, we presentDifferentially-Private TaBular AutoRegressive Transformer (DP-TBART), atransformer-based autoregressive model that maintains differential privacy andachieves performance competitive with marginal-based methods on a wide varietyof datasets, capable of even outperforming state-of-the-art methods in certainsettings. We also provide a theoretical framework for understanding thelimitations of marginal-based approaches and where deep learning-basedapproaches stand to contribute most. These results suggest that deeplearning-based techniques should be considered as a viable alternative tomarginal-based methods in the generation of differentially private synthetictabular data.", "output": "DP-TBART: A Transformer-based Autoregressive Model for Differentially Private Tabular Data Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Active learning is a well-studied approach to learning formal specifications,such as automata. In this work, we extend active specification learning byproposing a novel framework that strategically requests a combination ofmembership labels and pair-wise preferences, a popular alternative tomembership labels. The combination of pair-wise preferences and membershiplabels allows for a more flexible approach to active specification learning,which previously relied on membership labels only. We instantiate our frameworkin two different domains, demonstrating the generality of our approach. Ourresults suggest that learning from both modalities allows us to robustly andconveniently identify specifications via membership and preferences.", "output": "Learning Formal Specifications from Membership and Preference Queries."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Learners (DLs) are the state-of-art predictive mechanism withapplications in many fields requiring complex high dimensional data processing.Although conventional DLs get trained via gradient descent withback-propagation, Kalman Filter (KF)-based techniques that do not need gradientcomputation have been developed to approximate DLs. We propose a multi-armextension of a KF-based DL approximator that can mimic DL when the sample sizeis too small to train a multi-arm DL. The proposed Matrix Ensemble KalmanFilter-based multi-arm ANN (MEnKF-ANN) also performs explicit model stackingthat becomes relevant when the training sample has an unequal-size feature set.Our proposed technique can approximate Long Short-term Memory (LSTM) Networksand attach uncertainty to the predictions obtained from these LSTMs withdesirable coverage. We demonstrate how MEnKF-ANN can \"adequately\" approximatean LSTM network trained to classify what carbohydrate substrates are digestedand utilized by a microbiome sample whose genomic sequences consist ofpolysaccharide utilization loci (PULs) and their encoded genes.", "output": "A Matrix Ensemble Kalman Filter-based Multi-arm Neural Network to Adequately Approximate Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Traffic simulation software is used by transportation researchers andengineers to design and evaluate changes to roadways. These simulators aredriven by models of microscopic driver behavior from which macroscopic measureslike flow and congestion can be derived. Many models are designed for a subsetof possible traffic scenarios and roadway configurations, while others have noexplicit constraints on their application. Work zones (WZs) are one scenariofor which no model to date has reproduced realistic driving behavior. Thismakes it difficult to optimize for safety and other metrics when designing aWZ. The Federal Highway Administration commissioned the USDOT Volpe Center todevelop a car-following (CF) model for use in microscopic simulators that cancapture and reproduce driver behavior accurately within and outside of WZs.Volpe also performed a naturalistic driving study to collect telematics datafrom vehicles driven on roads with WZs for use in model calibration. Duringmodel development, Volpe researchers observed difficulties in calibrating theirmodel, leaving them to question whether there existed flaws in their model, inthe data, or in the procedure used to calibrate the model using the data. Inthis thesis, I use Bayesian methods for data analysis and parameter estimationto explore and, where possible, address these questions. First, I use Bayesianinference to measure the sufficiency of the size of the data set. Second, Icompare the procedure and results of the genetic algorithm based calibrationperformed by the Volpe researchers with those of Bayesian calibration. Third, Iexplore the benefits of modeling CF hierarchically. Finally, I apply what waslearned in the first three phases using an established CF model, Wiedemann 99,to the probabilistic modeling of the Volpe model. Validation is performed usinginformation criteria as an estimate of predictive accuracy.", "output": "A Bayesian Programming Approach to Car-following Model Calibration and Validation using Limited Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks (GNNs) have emerged as a prominent class of data-drivenmethods for molecular property prediction. However, a key limitation of typicalGNN models is their inability to quantify uncertainties in the predictions.This capability is crucial for ensuring the trustworthy use and deployment ofmodels in downstream tasks. To that end, we introduce AutoGNNUQ, an automateduncertainty quantification (UQ) approach for molecular property prediction.AutoGNNUQ leverages architecture search to generate an ensemble ofhigh-performing GNNs, enabling the estimation of predictive uncertainties. Ourapproach employs variance decomposition to separate data (aleatoric) and model(epistemic) uncertainties, providing valuable insights for reducing them. Inour computational experiments, we demonstrate that AutoGNNUQ outperformsexisting UQ methods in terms of both prediction accuracy and UQ performance onmultiple benchmark datasets. Additionally, we utilize t-SNE visualization toexplore correlations between molecular features and uncertainty, offeringinsight for dataset improvement. AutoGNNUQ has broad applicability in domainssuch as drug discovery and materials science, where accurate uncertaintyquantification is crucial for decision-making.", "output": "Uncertainty Quantification for Molecular Property Predictions with Graph Neural Architecture Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Overconfidence is a common issue for deep neural networks, limiting theirdeployment in real-world applications. To better estimate confidence, existingmethods mostly focus on fully-supervised scenarios and rely on training labels.In this paper, we propose the first confidence estimation method for asemi-supervised setting, when most training labels are unavailable. Westipulate that even with limited training labels, we can still reasonablyapproximate the confidence of model on unlabeled samples by inspecting theprediction consistency through the training process. We use trainingconsistency as a surrogate function and propose a consistency ranking loss forconfidence estimation. On both image classification and segmentation tasks, ourmethod achieves state-of-the-art performances in confidence estimation.Furthermore, we show the benefit of the proposed method through a downstreamactive learning task. The code is available at", "output": "Confidence Estimation Using Unlabeled Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the significant progress made by transformer models in machinereading comprehension tasks, they still face limitations in handling complexreasoning tasks due to the absence of explicit knowledge in the input sequence.This paper proposes a novel attention pattern to overcome this limitation,which integrates reasoning knowledge derived from a heterogeneous graph intothe transformer architecture using a graph-enhanced self-attention mechanism.The proposed attention pattern comprises three key elements: global-localattention for word tokens, graph attention for entity tokens that exhibitstrong attention towards tokens connected in the graph as opposed to thoseunconnected, and the consideration of the type of relationship between eachentity token and word token. This results in optimized attention between thetwo if a relationship exists. The pattern is coupled with special relativeposition labels, allowing it to integrate with LUKE's entity-awareself-attention mechanism. The experimental findings corroborate that our modeloutperforms both the cutting-edge LUKE-Graph and the baseline LUKE model on theReCoRD dataset that focuses on commonsense reasoning.", "output": "Integrating a Heterogeneous Graph with Entity-aware Self-attention using Relative Position Labels for Reading Comprehension Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In an effort to catalog insect biodiversity, we propose a new large datasetof hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record istaxonomically classified by an expert, and also has associated geneticinformation including raw nucleotide barcode sequences and assigned barcodeindex numbers, which are genetically-based proxies for species classification.This paper presents a curated million-image dataset, primarily to traincomputer-vision models capable of providing image-based taxonomic assessment,however, the dataset also presents compelling characteristics, the study ofwhich would be of interest to the broader machine learning community. Driven bythe biological nature inherent to the dataset, a characteristic long-tailedclass-imbalance distribution is exhibited. Furthermore, taxonomic labelling isa hierarchical classification scheme, presenting a highly fine-grainedclassification problem at lower levels. Beyond spurring interest inbiodiversity research within the machine learning community, progress oncreating an image-based taxonomic classifier will also further the ultimategoal of all BIOSCAN research: to lay the foundation for a comprehensive surveyof global biodiversity. This paper introduces the dataset and explores theclassification task through the implementation and analysis of a baselineclassifier.", "output": "A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A new computationally simple method of imposing hard convex constraints onthe neural network output values is proposed. The key idea behind the method isto map a vector of hidden parameters of the network to a point that isguaranteed to be inside the feasible set defined by a set of constraints. Themapping is implemented by the additional neural network layer with constraintsfor output. The proposed method is simply extended to the case when constraintsare imposed not only on the output vectors, but also on joint constraintsdepending on inputs. The projection approach to imposing constraints on outputscan simply be implemented in the framework of the proposed method. It is shownhow to incorporate different types of constraints into the proposed method,including linear and quadratic constraints, equality constraints, and dynamicconstraints, constraints in the form of boundaries. An important feature of themethod is its computational simplicity. Complexities of the forward pass of theproposed neural network layer by linear and quadratic constraints are O(n*m)and O(n^2*m), respectively, where n is the number of variables, m is the numberof constraints. Numerical experiments illustrate the method by solvingoptimization and classification problems. The code implementing the method ispublicly available.", "output": "A New Computationally Simple Approach for Implementing Neural Networks with Output Hard Constraints."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data science is not a science. It is a research paradigm with an unfathomedscope, scale, complexity, and power for knowledge discovery that is nototherwise possible and can be beyond human reasoning. It is changing our worldpractically and profoundly already widely deployed in tens of thousands ofapplications in every discipline in an AI Arms Race that, due to itsinscrutability, can lead to unfathomed risks. This paper presents an axiologyof data science, its purpose, nature, importance, risks, and value for problemsolving, by exploring and evaluating its remarkable, definitive features. Asdata science is in its infancy, this initial, speculative axiology is intendedto aid in understanding and defining data science to recognize its potentialbenefits, risks, and open research challenges. AI based data science isinherently about uncertainty that may be more realistic than our preference forthe certainty of science. Data science will have impacts far beyond knowledgediscovery and will take us into new ways of understanding the world.", "output": "A data science axiology: the nature, value, and risks of data science."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to the swift growth of patent applications each year, information andmultimedia retrieval approaches that facilitate patent exploration andretrieval are of utmost importance. Different types of visualizations (e.g.,graphs, technical drawings) and perspectives (e.g., side view, perspective) areused to visualize details of innovations in patents. The classification ofthese images enables a more efficient search and allows for further analysis.So far, datasets for image type classification miss some importantvisualization types for patents. Furthermore, related work does not make use ofrecent deep learning approaches including transformers. In this paper, we adoptstate-of-the-art deep learning methods for the classification of visualizationtypes and perspectives in patent images. We extend the CLEF-IP dataset forimage type classification in patents to ten classes and provide manual groundtruth annotations. In addition, we derive a set of hierarchical classes from adataset that provides weakly-labeled data for image perspectives. Experimentalresults have demonstrated the feasibility of the proposed approaches. Sourcecode, models, and dataset will be made publicly available.", "output": "Classification of Visualization Types and Perspectives in Patents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the breadth and depth of language model applications continue to expandrapidly, it is increasingly important to build efficient frameworks formeasuring and mitigating the learned or inherited social biases of thesemodels. In this paper, we present our work on evaluating instruction fine-tunedlanguage models' ability to identify bias through zero-shot prompting,including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instructionfine-tuned versions, Alpaca 7B performs best on the bias identification taskwith an accuracy of 56.7%. We also demonstrate that scaling up LLM size anddata diversity could lead to further performance gain. This is awork-in-progress presenting the first component of our bias mitigationframework. We will keep updating this work as we get more results.", "output": "Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have demonstrated remarkable proficiency inunderstanding and generating human-like texts, which may potentiallyrevolutionize the finance industry. However, existing LLMs often fall short inthe financial field, which is mainly attributed to the disparities betweengeneral text data and financial text data. Unfortunately, there is only alimited number of financial text datasets available (quite small size), andBloombergGPT, the first financial LLM (FinLLM), is close-sourced (only thetraining logs were released). In light of this, we aim to democratizeInternet-scale financial data for LLMs, which is an open challenge due todiverse data sources, low signal-to-noise ratio, and high time-validity. Toaddress the challenges, we introduce an open-sourced and data-centricframework, textit{Financial Generative Pre-trained Transformer (FinGPT)}, thatautomates the collection and curation of real-time financial data from &gt;34diverse sources on the Internet, providing researchers and practitioners withaccessible and transparent resources to develop their FinLLMs. Additionally, wepropose a simple yet effective strategy for fine-tuning FinLLM using theinherent feedback from the market, dubbed Reinforcement Learning with StockPrices (RLSP). We also adopt the Low-rank Adaptation (LoRA, QLoRA) method thatenables users to customize their own FinLLMs from open-source general-purposeLLMs at a low cost. Finally, we showcase several FinGPT applications, includingrobo-advisor, sentiment analysis for algorithmic trading, and low-codedevelopment. FinGPT aims to democratize FinLLMs, stimulate innovation, andunlock new opportunities in open finance. The codes are available at and", "output": "FinGPT: Democratizing Internet-scale Data for Financial Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Traditionally, sparse retrieval systems relied on lexical representations toretrieve documents, such as BM25, dominated information retrieval tasks. Withthe onset of pre-trained transformer models such as BERT, neural sparseretrieval has led to a new paradigm within retrieval. Despite the success,there has been limited software supporting different sparse retrievers runningin a unified, common environment. This hinders practitioners from fairlycomparing different sparse models and obtaining realistic evaluation results.Another missing piece is, that a majority of prior work evaluates sparseretrieval models on in-domain retrieval, i.e. on a single dataset: MS MARCO.However, a key requirement in practical retrieval systems requires models thatcan generalize well to unseen out-of-domain, i.e. zero-shot retrieval tasks. Inthis work, we provide SPRINT, a unified Python toolkit based on Pyserini andLucene, supporting a common interface for evaluating neural sparse retrieval.The toolkit currently includes five built-in models: uniCOIL, DeepImpact,SPARTA, TILDEv2 and SPLADEv2. Users can also easily add customized models bydefining their term weighting method. Using our toolkit, we establish strongand reproducible zero-shot sparse retrieval baselines across thewell-acknowledged benchmark, BEIR. Our results demonstrate that SPLADEv2achieves the best average score of 0.470 nDCG@10 on BEIR amongst all neuralsparse retrievers. In this work, we further uncover the reasons behind itsperformance gain. We show that SPLADEv2 produces sparse representations with amajority of tokens outside of the original query and document which is oftencrucial for its performance gains, i.e. a limitation among its other sparsecounterparts. We provide our SPRINT toolkit, models, and data used in ourexperiments publicly here at ", "output": "SPRINT: A Unified Toolkit for Evaluating and Demystifying Zero-shot Neural Sparse Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We demonstrate how images and sounds can be used for indirect prompt andinstruction injection in multi-modal LLMs. An attacker generates an adversarialperturbation corresponding to the prompt and blends it into an image or audiorecording. When the user asks the (unmodified, benign) model about theperturbed image or audio, the perturbation steers the model to output theattacker-chosen text and/or make the subsequent dialog follow the attacker'sinstruction. We illustrate this attack with several proof-of-concept examplestargeting LLaVa and PandaGPT.", "output": "(Ab)using Images and Sounds for Indirect Instruction Injection in Multi-Modal LLMs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the increasing importance of data sharing for collaboration andinnovation, it is becoming more important to ensure that data is managed andshared in a secure and trustworthy manner. Data governance is a common approachto managing data, but it faces many challenges such as data silos, dataconsistency, privacy, security, and access control. To address thesechallenges, this paper proposes a comprehensive framework that integrates datatrust in federated learning with InterPlanetary File System, blockchain, andsmart contracts to facilitate secure and mutually beneficial data sharing whileproviding incentives, access control mechanisms, and penalizing any dishonestbehavior. The experimental results demonstrate that the proposed model iseffective in improving the accuracy of federated learning models while ensuringthe security and fairness of the data-sharing process. The research paper alsopresents a decentralized federated learning platform that successfully traineda CNN model on the MNIST dataset using blockchain technology. The platformenables multiple workers to train the model simultaneously while maintainingdata privacy and security. The decentralized architecture and use of blockchaintechnology allow for efficient communication and coordination between workers.This platform has the potential to facilitate decentralized machine learningand support privacy-preserving collaboration in various domains.", "output": "Blockchain-Based Federated Learning: Incentivizing Data Sharing and Penalizing Dishonest Behavior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Active learning improves the performance of machine learning methods byjudiciously selecting a limited number of unlabeled data points to query forlabels, with the aim of maximally improving the underlying classifier'sperformance. Recent gains have been made using sequential active learning forsynthetic aperture radar (SAR) data <a href=\"/abs/2204.00005\">arXiv:2204.00005</a>. In each iteration,sequential active learning selects a query set of size one while batch activelearning selects a query set of multiple datapoints. While batch activelearning methods exhibit greater efficiency, the challenge lies in maintainingmodel accuracy relative to sequential active learning methods. We developed anovel, two-part approach for batch active learning: Dijkstra's Annulus Core-Set(DAC) for core-set generation and LocalMax for batch sampling. The batch activelearning process that combines DAC and LocalMax achieves nearly identicalaccuracy as sequential active learning but is more efficient, proportional tothe batch size. As an application, a pipeline is built based on transferlearning feature embedding, graph learning, DAC, and LocalMax to classify theFUSAR-Ship and OpenSARShip datasets. Our pipeline outperforms thestate-of-the-art CNN-based methods.", "output": "Novel Batch Active Learning Approach and Its Application to Synthetic Aperture Radar Datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Complex systems in science and engineering sometimes exhibit behavior thatchanges across different regimes. Traditional global models struggle to capturethe full range of this complex behavior, limiting their ability to accuratelyrepresent the system. In response to this challenge, we propose a novelcompetitive learning approach for obtaining data-driven models of physicalsystems. The primary idea behind the proposed approach is to employ dynamicloss functions for a set of models that are trained concurrently on the data.Each model competes for each observation during training, allowing for theidentification of distinct functional regimes within the dataset. Todemonstrate the effectiveness of the learning approach, we coupled it withvarious regression methods that employ gradient-based optimizers for training.The proposed approach was tested on various problems involving model discoveryand function approximation, demonstrating its ability to successfully identifyfunctional regimes, discover true governing equations, and reduce test errors.", "output": "A Competitive Learning Approach for Specialized Models: A Solution for Complex Physical Systems with Distinct Functional Regimes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose Automatic Feature Explanation using Contrasting Concepts (FALCON),an interpretability framework to explain features of image representations. Fora target feature, FALCON captions its highly activating cropped images using alarge captioning dataset (like LAION-400m) and a pre-trained vision-languagemodel like CLIP. Each word among the captions is scored and ranked leading to asmall number of shared, human-understandable concepts that closely describe thetarget feature. FALCON also applies contrastive interpretation using lowlyactivating (counterfactual) images, to eliminate spurious concepts. Althoughmany existing approaches interpret features independently, we observe instate-of-the-art self-supervised and supervised models, that less than 20% ofthe representation space can be explained by individual features. We show thatfeatures in larger spaces become more interpretable when studied in groups andcan be explained with high-order scoring concepts through FALCON. We discusshow extracted concepts can be used to explain and debug failures in downstreamtasks. Finally, we present a technique to transfer concepts from one(explainable) representation space to another unseen representation space bylearning a simple linear transformation.", "output": "Identifying Interpretable Subspaces in Image Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cross-silo federated learning (FL) enables the development of machinelearning models on datasets distributed across data centers such as hospitalsand clinical research laboratories. However, recent research has found thatcurrent FL algorithms face a trade-off between local and global performancewhen confronted with distribution shifts. Specifically, personalized FL methodshave a tendency to overfit to local data, leading to a sharp valley in thelocal model and inhibiting its ability to generalize to out-of-distributiondata. In this paper, we propose a novel federated model soup method (i.e.,selective interpolation of model parameters) to optimize the trade-off betweenlocal and global performance. Specifically, during the federated trainingphase, each client maintains its own global model pool by monitoring theperformance of the interpolated model between the local and global models. Thisallows us to alleviate overfitting and seek flat minima, which cansignificantly improve the model's generalization performance. We evaluate ourmethod on retinal and pathological image classification tasks, and our proposedmethod achieves significant improvements for out-of-distributiongeneralization. Our code is available at ", "output": "FedSoup: Improving Generalization and Personalization in Federated Learning via Selective Model Interpolation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the tradeoff between consistency and robustness in the context of asingle-trajectory time-varying Markov Decision Process (MDP) with untrustedmachine-learned advice. Our work departs from the typical approach of treatingadvice as coming from black-box sources by instead considering a setting whereadditional information about how the advice is generated is available. We provea first-of-its-kind consistency and robustness tradeoff given Q-value adviceunder a general MDP model that includes both continuous and discretestate/action spaces. Our results highlight that utilizing Q-value adviceenables dynamic pursuit of the better of machine-learned advice and a robustbaseline, thus result in near-optimal performance guarantees, which provablyimproves what can be obtained solely with black-box advice.", "output": "Beyond Black-Box Advice: Learning-Augmented Algorithms for MDPs with Q-Value Predictions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Outlier detection (OD) finds many applications with a rich literature ofnumerous techniques. Deep neural network based OD (DOD) has seen a recent surgeof attention thanks to the many advances in deep learning. In this paper, weconsider a critical-yet-understudied challenge with unsupervised DOD, that is,effective hyperparameter (HP) tuning/model selection. While several prior workreport the sensitivity of OD models to HPs, it becomes ever so critical for themodern DOD models that exhibit a long list of HPs. We introduce HYPER fortuning DOD models, tackling two fundamental challenges: (1) validation withoutsupervision (due to lack of labeled anomalies), and (2) efficient search of theHP/model space (due to exponential growth in the number of HPs). A key idea isto design and train a novel hypernetwork (HN) that maps HPs onto optimalweights of the main DOD model. In turn, HYPER capitalizes on a single HN thatcan dynamically generate weights for many DOD models (corresponding to varyingHPs), which offers significant speed-up. In addition, it employs meta-learningon historical OD tasks with labels to train a proxy validation function,likewise trained with our proposed HN efficiently. Extensive experiments on 35OD tasks show that HYPER achieves high performance against 8 baselines withsignificant efficiency gains.", "output": "Fast Unsupervised Deep Outlier Model Selection with Hypernetworks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning-based optimal control algorithms control unknown systems using pasttrajectory data and a learned model of the system dynamics. These controllersuse either a linear approximation of the learned dynamics, trading performancefor faster computation, or nonlinear optimization methods, which typicallyperform better but can limit real-time applicability. In this work, we presenta novel nonlinear controller that exploits differential flatness to achievesimilar performance to state-of-the-art learning-based controllers but withsignificantly less computational effort. Differential flatness is a property ofdynamical systems whereby nonlinear systems can be exactly linearized through anonlinear input mapping. Here, the nonlinear transformation is learned as aGaussian process and is used in a safety filter that guarantees, with highprobability, stability as well as input and flat state constraint satisfaction.This safety filter is then used to refine inputs from a flat model predictivecontroller to perform constrained nonlinear learning-based optimal controlthrough two successive convex optimizations. We compare our method tostate-of-the-art learning-based control strategies and achieve similarperformance, but with significantly better computational efficiency, while alsorespecting flat state and input constraints, and guaranteeing stability.", "output": "Differentially Flat Learning-based Model Predictive Control Using a Stability, State, and Input Constraining Safety Filter."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Expressive speech synthesis models are trained by adding corpora with diversespeakers, various emotions, and different speaking styles to the dataset, inorder to control various characteristics of speech and generate the desiredvoice. In this paper, we propose a style control (SC) VALL-E model based on theneural codec language model (called VALL-E), which follows the structure of thegenerative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes inputfrom text sentences and prompt audio and is designed to generate controllablespeech by not simply mimicking the characteristics of the prompt audio but bycontrolling the attributes to produce diverse voices. We identify tokens in thestyle embedding matrix of the newly designed style network that representattributes such as emotion, speaking rate, pitch, and voice intensity, anddesign a model that can control these attributes. To evaluate the performanceof SC VALL-E, we conduct comparative experiments with three representativeexpressive speech synthesis models: global style token (GST) Tacotron2,variational autoencoder (VAE) Tacotron2, and original VALL-E. We measure worderror rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) asevaluation metrics to assess the accuracy of generated sentences. For comparingthe quality of synthesized speech, we measure comparative mean option score(CMOS) and similarity mean option score (SMOS). To evaluate the style controlability of the generated speech, we observe the changes in F0 andmel-spectrogram by modifying the trained tokens. When using prompt audio thatis not present in the training data, SC VALL-E generates a variety ofexpressive sounds and demonstrates competitive performance compared to theexisting models. Our implementation, pretrained models, and audio samples arelocated on GitHub.", "output": "SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Air traffic control (ATC) is a safety-critical service system that demandsconstant attention from ground air traffic controllers (ATCos) to maintaindaily aviation operations. The workload of the ATCos can have negative effectson operational safety and airspace usage. To avoid overloading and ensure anacceptable workload level for the ATCos, it is important to predict the ATCos'workload accurately for mitigation actions. In this paper, we first perform areview of research on ATCo workload, mostly from the air traffic perspective.Then, we briefly introduce the setup of the human-in-the-loop (HITL)simulations with retired ATCos, where the air traffic data and workload labelsare obtained. The simulations are conducted under three Phoenix approachscenarios while the human ATCos are requested to self-evaluate their workloadratings (i.e., low-1 to high-7). Preliminary data analysis is conducted. Next,we propose a graph-based deep-learning framework with conformal prediction toidentify the ATCo workload levels. The number of aircraft under thecontroller's control varies both spatially and temporally, resulting indynamically evolving graphs. The experiment results suggest that (a) besidesthe traffic density feature, the traffic conflict feature contributes to theworkload prediction capabilities (i.e., minimum horizontal/vertical separationdistance); (b) directly learning from the spatiotemporal graph layout ofairspace with graph neural network can achieve higher prediction accuracy,compare to hand-crafted traffic complexity features; (c) conformal predictionis a valuable tool to further boost model prediction accuracy, resulting arange of predicted workload labels. The code used is available athref{", "output": "Air Traffic Controller Workload Level Prediction using Conformalized Dynamical Graph Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum computing has the potential to provide substantial computationaladvantages over current state-of-the-art classical supercomputers. However,current hardware is not advanced enough to execute fault-tolerant quantumalgorithms. An alternative of using hybrid quantum-classical computing withvariational algorithms can exhibit barren plateau issues, causing slowconvergence of gradient-based optimization techniques. In this paper, wediscuss \"post-variational strategies\", which shift tunable parameters from thequantum computer to the classical computer, opting for ensemble strategies whenoptimizing quantum models. We discuss various strategies and design principlesfor constructing individual quantum circuits, where the resulting ensembles canbe optimized with convex programming. Further, we discuss architectural designsof post-variational quantum neural networks and analyze the propagation ofestimation errors throughout such neural networks. Lastly, we show that ouralgorithm can be applied to real-world applications such as imageclassification on handwritten digits, producing a 96% classification accuracy.", "output": "Post-variational quantum neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Backdoor attacks are serious security threats to machine learning modelswhere an adversary can inject poisoned samples into the training set, causing abackdoored model which predicts poisoned samples with particular triggers toparticular target classes, while behaving normally on benign samples. In thispaper, we explore the task of purifying a backdoored model using a small cleandataset. By establishing the connection between backdoor risk and adversarialrisk, we derive a novel upper bound for backdoor risk, which mainly capturesthe risk on the shared adversarial examples (SAEs) between the backdoored modeland the purified model. This upper bound further suggests a novel bi-leveloptimization problem for mitigating backdoor using adversarial trainingtechniques. To solve it, we propose Shared Adversarial Unlearning (SAU).Specifically, SAU first generates SAEs, and then, unlearns the generated SAEssuch that they are either correctly classified by the purified model and/ordifferently classified by the two models, such that the backdoor effect in thebackdoored model will be mitigated in the purified model. Experiments onvarious benchmark datasets and network architectures show that our proposedmethod achieves state-of-the-art performance for backdoor defense.", "output": "Shared Adversarial Unlearning: Backdoor Mitigation by Unlearning Shared Adversarial Examples."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present FACADE, a novel probabilistic and geometric framework designed forunsupervised mechanistic anomaly detection in deep neural networks. Its primarygoal is advancing the understanding and mitigation of adversarial attacks.FACADE aims to generate probabilistic distributions over circuits, whichprovide critical insights to their contribution to changes in the manifoldproperties of pseudo-classes, or high-dimensional modes in activation space,yielding a powerful tool for uncovering and combating adversarial attacks. Ourapproach seeks to improve model robustness, enhance scalable model oversight,and demonstrates promising applications in real-world deployment settings.", "output": "FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the capabilities of large machine learning models continue to grow, and asthe autonomy afforded to such models continues to expand, the spectre of a newadversary looms: the models themselves. The threat that a model might behave ina seemingly reasonable manner, while secretly and subtly modifying its behaviorfor ulterior reasons is often referred to as deceptive alignment in the AISafety &amp; Alignment communities. Consequently, we call this new directionDeceptive Alignment Monitoring. In this work, we identify emerging directionsin diverse machine learning subfields that we believe will become increasinglyimportant and intertwined in the near future for deceptive alignmentmonitoring, and we argue that advances in these fields present both long-termchallenges and new research opportunities. We conclude by advocating forgreater involvement by the adversarial machine learning community in theseemerging directions.", "output": "Deceptive Alignment Monitoring."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a distributed machine learning technique, federated learning (FL) requiresclients to collaboratively train a shared model with an edge server withoutleaking their local data. However, the heterogeneous data distribution amongclients often leads to a decrease in model performance. To tackle this issue,this paper introduces a prototype-based regularization strategy to address theheterogeneity in the data distribution. Specifically, the regularizationprocess involves the server aggregating local prototypes from distributedclients to generate a global prototype, which is then sent back to theindividual clients to guide their local training. The experimental results onMNIST and Fashion-MNIST show that our proposal achieves improvements of 3.3%and 8.9% in average test accuracy, respectively, compared to the most popularbaseline FedAvg. Furthermore, our approach has a fast convergence rate inheterogeneous settings.", "output": "Boosting Federated Learning Convergence with Prototype Regularization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "SecureBoost is a tree-boosting algorithm leveraging homomorphic encryption toprotect data privacy in vertical federated learning setting. It is widely usedin fields such as finance and healthcare due to its interpretability,effectiveness, and privacy-preserving capability. However, SecureBoost suffersfrom high computational complexity and risk of label leakage. To harness thefull potential of SecureBoost, hyperparameters of SecureBoost should becarefully chosen to strike an optimal balance between utility, efficiency, andprivacy. Existing methods either set hyperparameters empirically orheuristically, which are far from optimal. To fill this gap, we propose aConstrained Multi-Objective SecureBoost (CMOSB) algorithm to find Paretooptimal solutions that each solution is a set of hyperparameters achievingoptimal tradeoff between utility loss, training cost, and privacy leakage. Wedesign measurements of the three objectives. In particular, the privacy leakageis measured using our proposed instance clustering attack. Experimental resultsdemonstrate that the CMOSB yields not only hyperparameters superior to thebaseline but also optimal sets of hyperparameters that can support the flexiblerequirements of FL participants.", "output": "SecureBoost Hyperparameter Tuning via Multi-Objective Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurate and timely prediction of sea fog is very important for effectivelymanaging maritime and coastal economic activities. Given the intricate natureand inherent variability of sea fog, traditional numerical and statisticalforecasting methods are often proven inadequate. This study aims to develop anadvanced sea fog forecasting method embedded in a numerical weather predictionmodel using the Yangtze River Estuary (YRE) coastal area as a case study. Priorto training our machine learning model, we employ a time-lagged correlationanalysis technique to identify key predictors and decipher the underlyingmechanisms driving sea fog occurrence. In addition, we implement ensemblelearning and a focal loss function to address the issue of imbalanced data,thereby enhancing the predictive ability of our model. To verify the accuracyof our method, we evaluate its performance using a comprehensive datasetspanning one year, which encompasses both weather station observations andhistorical forecasts. Remarkably, our machine learning-based approach surpassesthe predictive performance of two conventional methods, the weather researchand forecasting nonhydrostatic mesoscale model (WRF-NMM) and the algorithmdeveloped by the National Oceanic and Atmospheric Administration (NOAA)Forecast Systems Laboratory (FSL). Specifically, in regard to predicting seafog with a visibility of less than or equal to 1 km with a lead time of 60hours, our methodology achieves superior results by increasing the probabilityof detection (POD) while simultaneously reducing the false alarm ratio (FAR).", "output": "Intelligent model for offshore China sea fog forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As machine learning (ML) systems increasingly permeate high-stakes settingssuch as healthcare, transportation, military, and national security, concernsregarding their reliability have emerged. Despite notable progress, theperformance of these systems can significantly diminish due to adversarialattacks or environmental changes, leading to overconfident predictions,failures to detect input faults, and an inability to generalize in unexpectedscenarios. This paper proposes a holistic assessment methodology for thereliability of ML systems. Our framework evaluates five key properties:in-distribution accuracy, distribution-shift robustness, adversarialrobustness, calibration, and out-of-distribution detection. A reliability scoreis also introduced and used to assess the overall system reliability. Toprovide insights into the performance of different algorithmic approaches, weidentify and categorize state-of-the-art techniques, then evaluate a selectionon real-world tasks using our proposed reliability metrics and reliabilityscore. Our analysis of over 500 models reveals that designing for one metricdoes not necessarily constrain others but certain algorithmic techniques canimprove reliability across multiple metrics simultaneously. This studycontributes to a more comprehensive understanding of ML reliability andprovides a roadmap for future research and development.", "output": "A Holistic Assessment of the Reliability of Machine Learning Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy systems, climate change, and public health are among the primaryreasons for moving toward electrification in transportation. Transportationelectrification is being promoted worldwide to reduce emissions. As a result,many automakers will soon start making only battery electric vehicles (BEVs).BEV adoption rates are rising in California, mainly due to climate change andair pollution concerns. While great for climate and pollution goals, improperlymanaged BEV charging can lead to insufficient charging infrastructure and poweroutages. This study develops a novel Micro Clustering Deep Neural Network(MCDNN), an artificial neural network algorithm that is highly effective atlearning BEVs trip and charging data to forecast BEV charging events,information that is essential for electricity load aggregators and utilitymanagers to provide charging stations and electricity capacity effectively. TheMCDNN is configured using a robust dataset of trips and charges that occurredin California between 2015 and 2020 from 132 BEVs, spanning 5 BEV models for atotal of 1570167 vehicle miles traveled. The numerical findings revealed thatthe proposed MCDNN is more effective than benchmark approaches in this field,such as support vector machine, k nearest neighbors, decision tree, and otherneural network-based models in predicting the charging events.", "output": "Forecasting Battery Electric Vehicle Charging Behavior: A Deep Learning Approach Equipped with Micro-Clustering and SMOTE Techniques."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Internet of Things (IoT) integrates more than billions of intelligentdevices over the globe with the capability of communicating with otherconnected devices with little to no human intervention. IoT enables dataaggregation and analysis on a large scale to improve life quality in manydomains. In particular, data collected by IoT contain a tremendous amount ofinformation for anomaly detection. The heterogeneous nature of IoT is both achallenge and an opportunity for cybersecurity. Traditional approaches incybersecurity monitoring often require different kinds of data pre-processingand handling for various data types, which might be problematic for datasetsthat contain heterogeneous features. However, heterogeneous types of networkdevices can often capture a more diverse set of signals than a single type ofdevice readings, which is particularly useful for anomaly detection. In thispaper, we present a comprehensive study on using ensemble machine learningmethods for enhancing IoT cybersecurity via anomaly detection. Rather thanusing one single machine learning model, ensemble learning combines thepredictive power from multiple models, enhancing their predictive accuracy inheterogeneous datasets rather than using one single machine learning model. Wepropose a unified framework with ensemble learning that utilises Bayesianhyperparameter optimisation to adapt to a network environment that containsmultiple IoT sensor readings. Experimentally, we illustrate their highpredictive power when compared to traditional methods.", "output": "Ensemble Learning based Anomaly Detection for IoT Cybersecurity via Bayesian Hyperparameters Sensitivity Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) has drawn increasing attention owing to its potentialuse in large-scale industrial applications. Existing federated learning worksmainly focus on model homogeneous settings. However, practical federatedlearning typically faces the heterogeneity of data distributions, modelarchitectures, network environments, and hardware devices among participantclients. Heterogeneous Federated Learning (HFL) is much more challenging, andcorresponding solutions are diverse and complex. Therefore, a systematic surveyon this topic about the research challenges and state-of-the-art is essential.In this survey, we firstly summarize the various research challenges in HFLfrom five aspects: statistical heterogeneity, model heterogeneity,communication heterogeneity, device heterogeneity, and additional challenges.In addition, recent advances in HFL are reviewed and a new taxonomy of existingHFL methods is proposed with an in-depth analysis of their pros and cons. Weclassify existing methods from three different levels according to the HFLprocedure: data-level, model-level, and server-level. Finally, several criticaland promising future research directions in HFL are discussed, which mayfacilitate further developments in this field. A periodically updatedcollection on HFL is available at ", "output": "Heterogeneous Federated Learning: State-of-the-art and Research Challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, online reviews play a vital role for promoting any kind ofproduct or services. Businesses may embed fake reviews in order to attractcustomers to purchase their products. They may even highlight the benefits oftheir own product or criticize the competition's product. Marketers,advertisers, and other online business users have incentive to create fakepositive reviews for products which they want to promote or give fake negativereviews for products which they really don't like. So now-a-days writing adeceptive review is inevitable thing for promoting their own business ordegrading competitor's reputation. Thus, identifying deceptive reviews is anintense and on-going research area. This research paper proposes machinelearning model approach to identify deceptive reviews. The paper investigatesthe performance of the several experiments done on a Deceptive Opinion SpamCorpus dataset of restaurants reviews. We developed a n-gram model and maxfeatures to identify deceptive contents with a particular focus on fakereviews. Further, we conduct a benchmark study to investigate the performanceof two different features extraction techniques and apply five machine learningclassification techniques. The experimental results show that passiveaggressive classifier outperforms other algorithms, and it reaches the highestaccuracy not only in text classification but also to fake reviews. We alsostudy the data augmentation and implement different deep learning techniques.", "output": "Detecting deceptive reviews using text classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models have many methods for solving the same problem. Thisintroduces novel strengths (different methods may work well for differentproblems) and weaknesses (it may be difficult for users to know which method touse). In this paper, we introduce Multi-Method Self-Training (MMST), where onemethod is trained on the filtered outputs of another, allowing us to augmentthe strengths and ameliorate the weaknesses of each method. Using a 176Bparameter model trained on both language and code, we show that MMST can 1)improve the less performant method (up to 30%) making the model easier to use,2) improve the more performant method (up to 32.2%) making the model moreperformant, and 3) improve the performance of related but distinct tasks (up to10.3%) by improving the ability of the model to generate rationales. We thenconduct ablation analyses to explore why MMST works. We show that MMSTgenerates more data than traditional self-training, but the improvement inperformance is driven by the use of multiple methods. We also analyzeprompt-engineering and anti-correlated performance between methods as means ofmaking MMST more effective. We hope the evidence from our paper motivatesmachine learning researchers to explore ways in which advances in languagemodels allow for new forms of training.", "output": "Multi-Method Self-Training: Improving Code Generation With Text, And Vice Versa."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language models, primarily transformer-based ones, obtained colossal successin NLP. To be more precise, studies like BERT in NLU and works such as GPT-3for NLG are very crucial. DNA sequences are very close to natural language interms of structure, so if the DNA-related bioinformatics domain is concerned,discriminative models, like DNABert, exist. Yet, the generative side of thecoin is mainly unexplored to the best of our knowledge. Consequently, wefocused on developing an autoregressive generative language model like GPT-3for DNA sequences. Because working with whole DNA sequences is challengingwithout substantial computational resources, we decided to carry out our studyon a smaller scale, focusing on nucleotide sequences of human genes, uniqueparts in DNA with specific functionalities, instead of the whole DNA. Thisdecision did not change the problem structure a lot due to the fact that bothDNA and genes can be seen as 1D sequences consisting of four differentnucleotides without losing much information and making too much simplification.First of all, we systematically examined an almost entirely unexplored problemand observed that RNNs performed the best while simple techniques like N-gramswere also promising. Another beneficial point was learning how to work withgenerative models on languages we do not understand, unlike natural language.How essential using real-life tasks beyond the classical metrics such asperplexity is observed. Furthermore, checking whether the data-hungry nature ofthese models can be changed through selecting a language with minimalvocabulary size, four owing to four different types of nucleotides, isexamined. The reason for reviewing this was that choosing such a language mightmake the problem easier. However, what we observed in this study was it did notprovide that much of a change in the amount of data needed.", "output": "Generative Language Models on Nucleotide Sequences of Human Genes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advances in large language models (LLMs) have demonstrated notableprogress on many mathematical benchmarks. However, most of these benchmarksonly feature problems grounded in junior and senior high school subjects,contain only multiple-choice questions, and are confined to a limited scope ofelementary arithmetic operations. To address these issues, this paperintroduces an expansive benchmark suite SciBench that aims to systematicallyexamine the reasoning capabilities required for complex scientific problemsolving. SciBench contains two carefully curated datasets: an open setfeaturing a range of collegiate-level scientific problems drawn frommathematics, chemistry, and physics textbooks, and a closed set comprisingproblems from undergraduate-level exams in computer science and mathematics.Based on the two datasets, we conduct an in-depth benchmark study of tworepresentative LLMs with various prompting strategies. The results reveal thatcurrent LLMs fall short of delivering satisfactory performance, with an overallscore of merely 35.80%. Furthermore, through a detailed user study, wecategorize the errors made by LLMs into ten problem-solving abilities. Ouranalysis indicates that no single prompting strategy significantly outperformsothers and some strategies that demonstrate improvements in certainproblem-solving skills result in declines in other skills. We envision thatSciBench will catalyze further developments in the reasoning abilities of LLMs,thereby ultimately contributing to scientific research and discovery.", "output": "SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data sets of multivariate normal distributions abound in many scientificareas like diffusion tensor imaging, structure tensor computer vision, radarsignal processing, machine learning, just to name a few. In order to processthose normal data sets for downstream tasks like filtering, classification orclustering, one needs to define proper notions of dissimilarities betweennormals and paths joining them. The Fisher-Rao distance defined as theRiemannian geodesic distance induced by the Fisher information metric is such aprincipled metric distance which however is not known in closed-form exceptsfor a few particular cases. In this work, we first report a fast and robustmethod to approximate arbitrarily finely the Fisher-Rao distance betweenmultivariate normal distributions. Second, we introduce a class of distancesbased on diffeomorphic embeddings of the normal manifold into a submanifold ofthe higher-dimensional symmetric positive-definite cone corresponding to themanifold of centered normal distributions. We show that the projective Hilbertdistance on the cone yields a metric on the embedded normal submanifold and wepullback that cone distance with its associated straight line Hilbert conegeodesics to obtain a distance and smooth paths between normal distributions.Compared to the Fisher-Rao distance approximation, the pullback Hilbert conedistance is computationally light since it requires to compute only the extrememinimal and maximal eigenvalues of matrices. Finally, we show how to use thosedistances in clustering tasks.", "output": "Fisher-Rao distance and pullback SPD cone distances between multivariate normal distributions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the emergence of new application areas, such as cyber-physical systemsand human-in-the-loop applications, there is a need to guarantee a certainlevel of end-to-end network latency with extremely high reliability, e.g.,99.999%. While mechanisms specified under IEEE 802.1as time-sensitivenetworking (TSN) can be used to achieve these requirements for switchedEthernet networks, implementing TSN mechanisms in wireless networks ischallenging due to their stochastic nature. To conform the wireless link to areliability level of 99.999%, the behavior of extremely rare outliers in thelatency probability distribution, or the tail of the distribution, must beanalyzed and controlled. This work proposes predicting the tail of the latencydistribution using state-of-the-art data-driven approaches, such as mixturedensity networks (MDN) and extreme value mixture models, to estimate thelikelihood of rare latencies conditioned on the network parameters, which canbe used to make more informed decisions in wireless transmission. Actuallatency measurements of IEEE 802.11g (WiFi), commercial private and asoftware-defined 5G network are used to benchmark the proposed approaches andevaluate their sensitivities concerning the tail probabilities.", "output": "Data-Driven Latency Probability Prediction for Wireless Networks: Focusing on Tail Probabilities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series anomaly detection is crucial for industrial monitoring servicesthat handle a large volume of data, aiming to ensure reliability and optimizesystem performance. Existing methods often require extensive labeled resourcesand manual parameter selection, highlighting the need for automation. Thispaper proposes a comprehensive framework for automatic parameter optimizationin time series anomaly detection models. The framework introduces threeoptimization targets: prediction score, shape score, and sensitivity score,which can be easily adapted to different model backbones without priorknowledge or manual labeling efforts. The proposed framework has beensuccessfully applied online for over six months, serving more than 50,000 timeseries every minute. It simplifies the user's experience by requiring only anexpected sensitive value, offering a user-friendly interface, and achievingdesired detection results. Extensive evaluations conducted on public datasetsand comparison with other methods further confirm the effectiveness of theproposed framework.", "output": "Refining the Optimization Target for Automatic Univariate Time Series Anomaly Detection in Monitoring Services."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A very popular model-agnostic technique for explaining predictive models isthe SHapley Additive exPlanation (SHAP). The two most popular versions of SHAPare a conditional expectation version and an unconditional expectation version(the latter is also known as interventional SHAP). Except for tree-basedmethods, usually the unconditional version is used (for computational reasons).We provide a (surrogate) neural network approach which allows us to efficientlycalculate the conditional version for both neural networks and other regressionmodels, and which properly considers the dependence structure in the featurecomponents. This proposal is also useful to provide drop1 and anova analyses incomplex regression models which are similar to their generalized linear model(GLM) counterparts, and we provide a partial dependence plot (PDP) counterpartthat considers the right dependence structure in the feature components.", "output": "Conditional expectation network for SHAP."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) has emerged as a highly effective paradigm forprivacy-preserving collaborative training among different parties. Unliketraditional centralized learning, which requires collecting data from eachparty, FL allows clients to share privacy-preserving information withoutexposing private datasets. This approach not only guarantees enhanced privacyprotection but also facilitates more efficient and secure collaboration amongmultiple participants. Therefore, FL has gained considerable attention fromresearchers, promoting numerous surveys to summarize the related works.However, the majority of these surveys concentrate on methods sharing modelparameters during the training process, while overlooking the potential ofsharing other forms of local information. In this paper, we present asystematic survey from a new perspective, i.e., what to share in FL, with anemphasis on the model utility, privacy leakage, and communication efficiency.This survey differs from previous ones due to four distinct contributions.First, we present a new taxonomy of FL methods in terms of the sharing methods,which includes three categories of shared information: model sharing, syntheticdata sharing, and knowledge sharing. Second, we analyze the vulnerability ofdifferent sharing methods to privacy attacks and review the defense mechanismsthat provide certain privacy guarantees. Third, we conduct extensiveexperiments to compare the performance and communication overhead of varioussharing methods in FL. Besides, we assess the potential privacy leakage throughmodel inversion and membership inference attacks, while comparing theeffectiveness of various defense approaches. Finally, we discuss potentialdeficiencies in current methods and outline future directions for improvement.", "output": "A Survey of What to Share in Federated Learning: Perspectives on Model Utility, Privacy Leakage, and Communication Efficiency."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We wish to define the limits of a classical classification model based ondeep learning when applied to abstract images, which do not represent visuallyidentifiable objects.QR codes (Quick Response codes) fall into this category ofabstract images: one bit corresponding to one encoded character, QR codes werenot designed to be decoded manually. To understand the limitations of a deeplearning-based model for abstract image classification, we train an imageclassification model on QR codes generated from information obtained whenreading a health pass. We compare a classification model with a classical(deterministic) decoding method in the presence of noise. This study allows usto conclude that a model based on deep learning can be relevant for theunderstanding of abstract images.", "output": "Deep learning for classification of noisy QR codes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Coordinate denoising is a promising 3D molecular pre-training method, whichhas achieved remarkable performance in various downstream drug discovery tasks.Theoretically, the objective is equivalent to learning the force field, whichis revealed helpful for downstream tasks. Nevertheless, there are twochallenges for coordinate denoising to learn an effective force field, i.e. lowcoverage samples and isotropic force field. The underlying reason is thatmolecular distributions assumed by existing denoising methods fail to capturethe anisotropic characteristic of molecules. To tackle these challenges, wepropose a novel hybrid noise strategy, including noises on both dihedral angeland coordinate. However, denoising such hybrid noise in a traditional way is nomore equivalent to learning the force field. Through theoretical deductions, wefind that the problem is caused by the dependency of the input conformation forcovariance. To this end, we propose to decouple the two types of noise anddesign a novel fractional denoising method (Frad), which only denoises thelatter coordinate part. In this way, Frad enjoys both the merits of samplingmore low-energy structures and the force field equivalence. Extensiveexperiments show the effectiveness of Frad in molecular representation, with anew state-of-the-art on 9 out of 12 tasks of QM9 and on 7 out of 8 targets ofMD17.", "output": "Fractional Denoising for 3D Molecular Pre-training."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, denoising methods based on supervised learning have exhibitedpromising performance. However, their reliance on external datasets containingnoisy-clean image pairs restricts their applicability. To address thislimitation, researchers have focused on training denoising networks usingsolely a set of noisy inputs. To improve the feasibility of denoisingprocedures, in this study, we proposed a single-image self-supervised learningmethod in which only the noisy input image is used for network training. Gatedconvolution was used for feature extraction and no-reference image qualityassessment was used for guiding the training process. Moreover, the proposedmethod sampled instances from the input image dataset using Bernoulli samplingwith a certain dropout rate for training. The corresponding result was producedby averaging the generated predictions from various instances of the trainednetwork with dropouts. The experimental results indicated that the proposedmethod achieved state-of-the-art denoising performance on both synthetic andreal-world datasets. This highlights the effectiveness and practicality of ourmethod as a potential solution for various noise removal tasks.", "output": "Self2Self+: Single-Image Denoising with Self-Supervised Learning and Image Quality Assessment Loss."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Granger causality (GC) is often considered not an actual form of causality.Still, it is arguably the most widely used method to assess the predictabilityof a time series from another one. Granger causality has been widely used inmany applied disciplines, from neuroscience and econometrics to Earth sciences.We revisit GC under a graphical perspective of state-space models. For that, weuse GraphEM, a recently presented expectation-maximisation algorithm forestimating the linear matrix operator in the state equation of alinear-Gaussian state-space model. Lasso regularisation is included in theM-step, which is solved using a proximal splitting Douglas-Rachford algorithm.Experiments in toy examples and challenging climate problems illustrate thebenefits of the proposed model and inference technique over standard Grangercausality methods.", "output": "Graphs in State-Space Models for Granger Causality in Climate Science."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The drastic growth of electric vehicles and photovoltaics can introduce newchallenges, such as electrical current congestion and voltage limit violationsdue to peak load demands. These issues can be mitigated by controlling theoperation of electric vehicles i.e., smart charging. Centralized smart chargingsolutions have already been proposed in the literature. But such solutions maylack scalability and suffer from inherent drawbacks of centralization, such asa single point of failure, and data privacy concerns. Decentralization can helptackle these challenges. In this paper, a fully decentralized smart chargingsystem is proposed using the philosophy of adaptive multi-agent systems. Theproposed system utilizes multi-armed bandit learning to handle uncertainties inthe system. The presented system is decentralized, scalable, real-time,model-free, and takes fairness among different players into account. A detailedcase study is also presented for performance evaluation.", "output": "Decentralized Smart Charging of Large-Scale EVs using Adaptive Multi-Agent Multi-Armed Bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation is a common task in autonomous driving to understandthe surrounding environment. Driveable Area Segmentation and Lane Detection areparticularly important for safe and efficient navigation on the road. However,original semantic segmentation models are computationally expensive and requirehigh-end hardware, which is not feasible for embedded systems in autonomousvehicles. This paper proposes a lightweight model for the driveable area andlane line segmentation. TwinLiteNet is designed cheaply but achieves accurateand efficient segmentation results. We evaluate TwinLiteNet on the BDD100Kdataset and compare it with modern models. Experimental results show that ourTwinLiteNet performs similarly to existing approaches, requiring significantlyfewer computational resources. Specifically, TwinLiteNet achieves a mIoU scoreof 91.3% for the Drivable Area task and 31.08% IoU for the Lane Detection taskwith only 0.4 million parameters and achieves 415 FPS on GPU RTX A5000.Furthermore, TwinLiteNet can run in real-time on embedded devices with limitedcomputing power, especially since it achieves 60FPS on Jetson Xavier NX, makingit an ideal solution for self-driving vehicles. Code is available:url{", "output": "TwinLiteNet: An Efficient and Lightweight Model for Driveable Area and Lane Segmentation in Self-Driving Cars."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the challenge of parametrizing policies for reinforcementlearning (RL) in high-dimensional continuous action spaces. Our objective is todevelop a multimodal policy that overcomes limitations inherent in thecommonly-used Gaussian parameterization. To achieve this, we propose aprincipled framework that models the continuous RL policy as a generative modelof optimal trajectories. By conditioning the policy on a latent variable, wederive a novel variational bound as the optimization objective, which promotesexploration of the environment. We then present a practical model-based RLmethod, called Reparameterized Policy Gradient (RPG), which leverages themultimodal policy parameterization and learned world model to achieve strongexploration capabilities and high data efficiency. Empirical resultsdemonstrate that our method can help agents evade local optima in tasks withdense rewards and solve challenging sparse-reward environments by incorporatingan object-centric intrinsic reward. Our method consistently outperformsprevious approaches across a range of tasks. Code and supplementary materialsare available on the project page ", "output": "Reparameterized Policy Learning for Multimodal Trajectory Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extracting noisy or incorrectly labeled samples from a labeled dataset withhard/difficult samples is an important yet under-explored topic. Two generaland often independent lines of work exist, one focuses on addressing noisylabels, and another deals with hard samples. However, when both types of dataare present, most existing methods treat them equally, which results in adecline in the overall performance of the model. In this paper, we first designvarious synthetic datasets with custom hardness and noisiness levels fordifferent samples. Our proposed systematic empirical study enables us to betterunderstand the similarities and more importantly the differences betweenhard-to-learn samples and incorrectly-labeled samples. These controlledexperiments pave the way for the development of methods that distinguishbetween hard and noisy samples. Through our study, we introduce a simple yeteffective metric that filters out noisy-labeled samples while keeping the hardsamples. We study various data partitioning methods in the presence of labelnoise and observe that filtering out noisy samples from hard samples with thisproposed metric results in the best datasets as evidenced by the high testaccuracy achieved after models are trained on the filtered datasets. Wedemonstrate this for both our created synthetic datasets and for datasets withreal-world label noise. Furthermore, our proposed data partitioning methodsignificantly outperforms other methods when employed within a semi-supervisedlearning framework.", "output": "Differences Between Hard and Noisy-labeled Samples: An Empirical Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We suggest a simple Gaussian mixture model for data generation that complieswith Feldman's long tail theory (2020). We demonstrate that a linear classifiercannot decrease the generalization error below a certain level in the proposedmodel, whereas a nonlinear classifier with a memorization capacity can. Thisconfirms that for long-tailed distributions, rare training examples must beconsidered for optimal generalization to new data. Finally, we show that theperformance gap between linear and nonlinear models can be lessened as the tailbecomes shorter in the subpopulation frequency distribution, as confirmed byexperiments on synthetic and real data.", "output": "Long-Tail Theory under Gaussian Mixtures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) has enabled multiple data owners (a.k.a. FL clients)to train machine learning models collaboratively without revealing privatedata. Since the FL server can only engage a limited number of clients in eachtraining round, FL client selection has become an important research problem.Existing approaches generally focus on either enhancing FL model performance orenhancing the fair treatment of FL clients. The problem of balancingperformance and fairness considerations when selecting FL clients remains open.To address this problem, we propose the Fairness-aware Federated ClientSelection (FairFedCS) approach. Based on Lyapunov optimization, it dynamicallyadjusts FL clients' selection probabilities by jointly considering theirreputations, times of participation in FL tasks and contributions to theresulting model performance. By not using threshold-based reputation filtering,it provides FL clients with opportunities to redeem their reputations after aperceived poor performance, thereby further enhancing fair client treatment.Extensive experiments based on real-world multimedia datasets show thatFairFedCS achieves 19.6% higher fairness and 0.73% higher test accuracy onaverage than the best-performing state-of-the-art approach.", "output": "Fairness-Aware Client Selection for Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The aggregation of multiple opinions plays a crucial role in decision-making,such as in hiring and loan review, and in labeling data for supervisedlearning. Although majority voting and existing opinion aggregation models areeffective for simple tasks, they are inappropriate for tasks withoutobjectively true labels in which disagreements may occur. In particular, whenvoter attributes such as gender or race introduce bias into opinions, theaggregation results may vary depending on the composition of voter attributes.A balanced group of voters is desirable for fair aggregation results but may bedifficult to prepare. In this study, we consider methods to achieve fairopinion aggregation based on voter attributes and evaluate the fairness of theaggregated results. To this end, we consider an approach that combines opinionaggregation models such as majority voting and the Dawid and Skene model (D&amp;Smodel) with fairness options such as sample weighting. To evaluate the fairnessof opinion aggregation, probabilistic soft labels are preferred over discreteclass labels. First, we address the problem of soft label estimation withoutconsidering voter attributes and identify some issues with the D&amp;S model. Toaddress these limitations, we propose a new Soft D&amp;S model with improvedaccuracy in estimating soft labels. Moreover, we evaluated the fairness of anopinion aggregation model, including Soft D&amp;S, in combination with differentfairness options using synthetic and semi-synthetic data. The experimentalresults suggest that the combination of Soft D&amp;S and data splitting as afairness option is effective for dense data, whereas weighted majority votingis effective for sparse data. These findings should prove particularly valuablein supporting decision-making by human and machine-learning models withbalanced opinion aggregation.", "output": "Mitigating Voter Attribute Bias for Fair Opinion Aggregation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing action recognition methods are typically actor-specific due to theintrinsic topological and apparent differences among the actors. This requiresactor-specific pose estimation (e.g., humans vs. animals), leading tocumbersome model design complexity and high maintenance costs. Moreover, theyoften focus on learning the visual modality alone and single-labelclassification whilst neglecting other available information sources (e.g.,class name text) and the concurrent occurrence of multiple actions. To overcomethese limitations, we propose a new approach called 'actor-agnostic multi-modalmulti-label action recognition,' which offers a unified solution for varioustypes of actors, including humans and animals. We further formulate a novelMulti-modal Semantic Query Network (MSQNet) model in a transformer-based objectdetection framework (e.g., DETR), characterized by leveraging visual andtextual modalities to represent the action classes better. The elimination ofactor-specific model designs is a key advantage, as it removes the need foractor pose estimation altogether. Extensive experiments on five publiclyavailable benchmarks show that our MSQNet consistently outperforms the priorarts of actor-specific alternatives on human and animal single- and multi-labelaction recognition tasks by up to 50%. Code will be released at", "output": "MSQNet: Actor-agnostic Action Recognition with Multi-modal Query."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Working memory (WM), a fundamental cognitive process facilitating thetemporary storage, integration, manipulation, and retrieval of information,plays a vital role in reasoning and decision-making tasks. Robust benchmarkdatasets that capture the multifaceted nature of WM are crucial for theeffective development and evaluation of AI WM models. Here, we introduce acomprehensive Working Memory (WorM) benchmark dataset for this purpose. WorMcomprises 10 tasks and a total of 1 million trials, assessing 4functionalities, 3 domains, and 11 behavioral and neural characteristics of WM.We jointly trained and tested state-of-the-art recurrent neural networks andtransformers on all these tasks. We also include human behavioral benchmarks asan upper bound for comparison. Our results suggest that AI models replicatesome characteristics of WM in the brain, most notably primacy and recencyeffects, and neural clusters and correlates specialized for different domainsand functionalities of WM. In the experiments, we also reveal some limitationsin existing models to approximate human behavior. This dataset serves as avaluable resource for communities in cognitive psychology, neuroscience, andAI, offering a standardized framework to compare and enhance WM models,investigate WM's neural underpinnings, and develop WM models with human-likecapabilities. Our source code and data are available at", "output": "Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Music recommendation systems have emerged as a vital component to enhanceuser experience and satisfaction for the music streaming services, whichdominates music consumption. The key challenge in improving these recommendersystems lies in comprehending the complexity of music data, specifically forthe underpinning music genre classification. The limitations of manual genreclassification have highlighted the need for a more advanced system, namely theAutomatic Music Genre Classification (AMGC) system. While traditional machinelearning techniques have shown potential in genre classification, they heavilyrely on manually engineered features and feature selection, failing to capturethe full complexity of music data. On the other hand, deep learningclassification architectures like the traditional Convolutional Neural Networks(CNN) are effective in capturing the spatial hierarchies but struggle tocapture the temporal dynamics inherent in music data. To address thesechallenges, this study proposes a novel approach using visual spectrograms asinput, and propose a hybrid model that combines the strength of the Residualneural Network (ResNet) and the Gated Recurrent Unit (GRU). This model isdesigned to provide a more comprehensive analysis of music data, offering thepotential to improve the music recommender systems through achieving a morecomprehensive analysis of music data and hence potentially more accurate genreclassification.", "output": "Music Genre Classification with ResNet and Bi-GRU Using Visual Spectrograms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Background. Due to the widespread adoption of Artificial Intelligence (AI)and Machine Learning (ML) for building software applications, companies arestruggling to recruit employees with a deep understanding of such technologies.In this scenario, AutoML is soaring as a promising solution to fill the AI/MLskills gap since it promises to automate the building of end-to-end AI/MLpipelines that would normally be engineered by specialized team members. Aims.Despite the growing interest and high expectations, there is a dearth ofinformation about the extent to which AutoML is currently adopted by teamsdeveloping AI/ML-enabled systems and how it is perceived by practitioners andresearchers. Method. To fill these gaps, in this paper, we present amixed-method study comprising a benchmark of 12 end-to-end AutoML tools on twoSE datasets and a user survey with follow-up interviews to further ourunderstanding of AutoML adoption and perception. Results. We found that AutoMLsolutions can generate models that outperform those trained and optimized byresearchers to perform classification tasks in the SE domain. Also, ourfindings show that the currently available AutoML solutions do not live up totheir names as they do not equally support automation across the stages of theML development workflow and for all the team members. Conclusions. We deriveinsights to inform the SE research community on how AutoML can facilitate theiractivities and tool builders on how to design the next generation of AutoMLtechnologies.", "output": "Assessing the Use of AutoML for Data-Driven Software Engineering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as asimple extension of Gumbel Tree RvNN and it was shown to achievestate-of-the-art length generalization performance in ListOps while maintainingcomparable performance on other tasks. However, although not the worst in itskind, BT-RvNN can be still exorbitantly expensive in memory usage. In thispaper, we identify the main bottleneck in BT-RvNN's memory usage to be theentanglement of the scorer function and the recursive cell function. We proposestrategies to remove this bottleneck and further simplify its memory usage.Overall, our strategies not only reduce the memory usage of BT-RvNN by$10$-$16$ times but also create a new state-of-the-art in ListOps whilemaintaining similar performance in other tasks. In addition, we also propose astrategy to utilize the induced latent-tree node representations produced byBT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:mathbb{R}^{ntimes d} rightarrow mathbb{R}^{d}$ into a sequence contextualizer of theform $f:mathbb{R}^{n times d} rightarrow mathbb{R}^{n times d}$. Thus, ourproposals not only open up a path for further scalability of RvNNs but alsostandardize a way to use BT-RvNNs as another building block in the deeplearning toolkit that can be easily stacked or interfaced with other popularmodels such as Transformers and Structured State Space models.", "output": "Efficient Beam Tree Recursion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Source-free domain adaptation has become popular because of its practicalusefulness and no need to access source data. However, the adaptation processstill takes a considerable amount of time and is predominantly based onoptimization that relies on back-propagation. In this work we present a simplefeed-forward approach that challenges the need for back-propagation basedadaptation. Our approach is based on computing prototypes of classes under thedomain shift using a pre-trained model. It achieves strong improvements inaccuracy compared to the pre-trained model and requires only a small fractionof time of existing domain adaptation methods.", "output": "Feed-Forward Source-Free Domain Adaptation via Class Prototypes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mixtures of classifiers (a.k.a. randomized ensembles) have been proposed as away to improve robustness against adversarial attacks. However, it has beenshown that existing attacks are not well suited for this kind of classifiers.In this paper, we discuss the problem of attacking a mixture in a principledway and introduce two desirable properties of attacks based on a geometricalanalysis of the problem (effectiveness and maximality). We then show thatexisting attacks do not meet both of these properties. Finally, we introduce anew attack called lattice climber attack with theoretical guarantees on thebinary linear setting, and we demonstrate its performance by conductingexperiments on synthetic and real datasets.", "output": "Adversarial attacks for mixtures of classifiers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot anomaly detection (AD) is an emerging sub-field of general AD, andtries to distinguish between normal and anomalous data using only few selectedsamples. While newly proposed few-shot AD methods do compare againstpre-existing algorithms developed for the full-shot domain as baselines, theydo not dedicatedly optimize them for the few-shot setting. It thus remainsunclear if the performance of such pre-existing algorithms can be furtherimproved. We address said question in this work. Specifically, we present astudy on the AD/anomaly segmentation (AS) performance of PatchCore, the currentstate-of-the-art full-shot AD/AS algorithm, in both the few-shot and themany-shot settings. We hypothesize that further performance improvements can berealized by (I) optimizing its various hyperparameters, and by (II)transferring techniques known to improve few-shot supervised learning to the ADdomain. Exhaustive experiments on the public VisA and MVTec AD datasets revealthat (I) significant performance improvements can be realized by optimizinghyperparameters such as the underlying feature extractor, and that (II)image-level augmentations can, but are not guaranteed, to improve performance.Based on these findings, we achieve a new state of the art in few-shot AD onVisA, further demonstrating the merit of adapting pre-existing AD/AS methods tothe few-shot setting. Last, we identify the investigation of feature extractorswith a strong inductive bias as a potential future research direction for(few-shot) AD/AS.", "output": "Optimizing PatchCore for Few/many-shot Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal learning aims to build models that can process and relateinformation from multiple modalities. Despite years of development in thisfield, it still remains challenging to design a unified network for processingvarious modalities ($textit{e.g.}$ natural language, 2D images, 3D pointclouds, audio, video, time series, tabular data) due to the inherent gaps amongthem. In this work, we propose a framework, named Meta-Transformer, thatleverages a $textbf{frozen}$ encoder to perform multimodal perception withoutany paired multimodal training data. In Meta-Transformer, the raw input datafrom various modalities are mapped into a shared token space, allowing asubsequent encoder with frozen parameters to extract high-level semanticfeatures of the input data. Composed of three main components: a unified datatokenizer, a modality-shared encoder, and task-specific heads for downstreamtasks, Meta-Transformer is the first framework to perform unified learningacross 12 modalities with unpaired data. Experiments on different benchmarksreveal that Meta-Transformer can handle a wide range of tasks includingfundamental perception (text, image, point cloud, audio, video), practicalapplication (X-Ray, infrared, hyperspectral, and IMU), and data mining (graph,tabular, and time-series). Meta-Transformer indicates a promising future fordeveloping unified multimodal intelligence with transformers. Code will beavailable at ", "output": "Meta-Transformer: A Unified Framework for Multimodal Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the increasing amount of spatial-temporal~(ST) ocean data, numerousspatial-temporal data mining (STDM) studies have been conducted to addressvarious oceanic issues, e.g., climate forecasting and disaster warning.Compared with typical ST data (e.g., traffic data), ST ocean data is morecomplicated with some unique characteristics, e.g., diverse regionality andhigh sparsity. These characteristics make it difficult to design and train STDMmodels. Unfortunately, an overview of these studies is still missing, hinderingcomputer scientists to identify the research issues in ocean while discouragingresearchers in ocean science from applying advanced STDM techniques. To remedythis situation, we provide a comprehensive survey to summarize existing STDMstudies in ocean. Concretely, we first summarize the widely-used ST oceandatasets and identify their unique characteristics. Then, typical ST ocean dataquality enhancement techniques are discussed. Next, we classify existing STDMstudies for ocean into four types of tasks, i.e., prediction, event detection,pattern mining, and anomaly detection, and elaborate the techniques for thesetasks. Finally, promising research opportunities are highlighted. This surveywill help scientists from the fields of both computer science and ocean sciencehave a better understanding of the fundamental concepts, key techniques, andopen challenges of STDM in ocean.", "output": "Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a novel communication-efficient split learning (SL)framework, named SplitFC, which reduces the communication overhead required fortransmitting intermediate feature and gradient vectors during the SL trainingprocess. The key idea of SplitFC is to leverage different dispersion degreesexhibited in the columns of the matrices. SplitFC incorporates two compressionstrategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wisequantization. In the first strategy, the intermediate feature vectors aredropped with adaptive dropout probabilities determined based on the standarddeviation of these vectors. Then, by the chain rule, the intermediate gradientvectors associated with the dropped feature vectors are also dropped. In thesecond strategy, the non-dropped intermediate feature and gradient vectors arequantized using adaptive quantization levels determined based on the ranges ofthe vectors. To minimize the quantization error, the optimal quantizationlevels of this strategy are derived in a closed-form expression. Simulationresults on the MNIST, CIFAR-10, and CelebA datasets demonstrate that SplitFCprovides more than a 5.6% increase in classification accuracy compared tostate-of-the-art SL frameworks, while they require 320 times less communicationoverhead compared to the vanilla SL framework without compression.", "output": "Communication-Efficient Split Learning via Adaptive Feature-Wise Compression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Imitation learning (IL) seeks to teach agents specific tasks through expertdemonstrations. One of the key approaches to IL is to define a distance betweenagent and expert and to find an agent policy that minimizes that distance.Optimal transport methods have been widely used in imitation learning as theyprovide ways to measure meaningful distances between agent and experttrajectories. However, the problem of how to optimally combine multiple expertdemonstrations has not been widely studied. The standard method is to simplyconcatenate state (-action) trajectories, which is problematic whentrajectories are multi-modal. We propose an alternative method that uses amulti-marginal optimal transport distance and enables the combination ofmultiple and diverse state-trajectories in the OT sense, providing a moresensible geometric average of the demonstrations. Our approach enables an agentto learn from several experts, and its efficiency is analyzed on OpenAI Gymcontrol environments and demonstrates that the standard method is not alwaysoptimal.", "output": "On Combining Expert Demonstrations in Imitation Learning via Optimal Transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Performance of a pre-trained semantic segmentation model is likely tosubstantially decrease on data from a new domain. We show a pre-trained modelcan be adapted to unlabelled target domain data by calculating soft-labelprototypes under the domain shift and making predictions according to theprototype closest to the vector with predicted class probabilities. Theproposed adaptation procedure is fast, comes almost for free in terms ofcomputational resources and leads to considerable performance improvements. Wedemonstrate the benefits of such label calibration on the highly-practicalsynthetic-to-real semantic segmentation problem.", "output": "Label Calibration for Semantic Segmentation Under Domain Shift."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a deep learning architecture for nowcasting ofprecipitation almost globally every 30 min with a 4-hour lead time. Thearchitecture fuses a U-Net and a convolutional long short-term memory (LSTM)neural network and is trained using data from the Integrated MultisatellitERetrievals for GPM (IMERG) and a few key precipitation drivers from the GlobalForecast System (GFS). The impacts of different training loss functions,including the mean-squared error (regression) and the focal-loss(classification), on the quality of precipitation nowcasts are studied. Theresults indicate that the regression network performs well in capturing lightprecipitation (below 1.6 mm/hr), but the classification network can outperformthe regression network for nowcasting of precipitation extremes (&gt;8 mm/hr), interms of the critical success index (CSI).. Using the Wasserstein distance, itis shown that the predicted precipitation by the classification network has acloser class probability distribution to the IMERG than the regression network.It is uncovered that the inclusion of the physical variables can improveprecipitation nowcasting, especially at longer lead times in both networks.Taking IMERG as a relative reference, a multi-scale analysis in terms offractions skill score (FSS), shows that the nowcasting machine remains skillful(FSS &gt; 0.5) at the resolution of 10 km compared to 50 km for GFS. Forprecipitation rates greater than 4~mm/hr, only the classification networkremains FSS-skillful on scales greater than 50 km within a 2-hour lead time.", "output": "Global Precipitation Nowcasting of Integrated Multi-satellitE Retrievals for GPM: A U-Net Convolutional LSTM Architecture."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning algorithms which keep the parameters of new tasks close tothat of previous tasks, are popular in preventing catastrophic forgetting insequential task learning settings. However, 1) the performance for the newcontinual learner will be degraded without distinguishing the contributions ofpreviously learned tasks; 2) the computational cost will be greatly increasedwith the number of tasks, since most existing algorithms need to regularize allprevious tasks when learning new tasks. To address the above challenges, wepropose a self-paced Weight Consolidation (spWC) framework to attain robustcontinual learning via evaluating the discriminative contributions of previoustasks. To be specific, we develop a self-paced regularization to reflect thepriorities of past tasks via measuring difficulty based on key performanceindicator (i.e., accuracy). When encountering a new task, all previous tasksare sorted from \"difficult\" to \"easy\" based on the priorities. Then theparameters of the new continual learner will be learned via selectivelymaintaining the knowledge amongst more difficult past tasks, which could wellovercome catastrophic forgetting with less computational cost. We adopt analternative convex search to iteratively update the model parameters andpriority weights in the bi-convex formulation. The proposed spWC framework isplug-and-play, which is applicable to most continual learning algorithms (e.g.,EWC, MAS and RCIL) in different directions (e.g., classification andsegmentation). Experimental results on several public benchmark datasetsdemonstrate that our proposed framework can effectively improve performancewhen compared with other popular continual learning algorithms.", "output": "Self-paced Weight Consolidation for Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Emerging large-scale text-to-image generative models, e.g., Stable Diffusion(SD), have exhibited overwhelming results with high fidelity. Despite themagnificent progress, current state-of-the-art models still struggle togenerate images fully adhering to the input prompt. Prior work, Attend &amp;Excite, has introduced the concept of Generative Semantic Nursing (GSN), aimingto optimize cross-attention during inference time to better incorporate thesemantics. It demonstrates promising results in generating simple prompts,e.g., ``a cat and a dog''. However, its efficacy declines when dealing withmore complex prompts, and it does not explicitly address the problem ofimproper attribute binding. To address the challenges posed by complex promptsor scenarios involving multiple entities and to achieve improved attributebinding, we propose Divide &amp; Bind. We introduce two novel loss objectives forGSN: a novel attendance loss and a binding loss. Our approach stands out in itsability to faithfully synthesize desired objects with improved attributealignment from complex prompts and exhibits superior performance acrossmultiple evaluation benchmarks. More videos and updates can be found on theproject page url{", "output": "Divide & Bind Your Attention for Improved Generative Semantic Nursing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural Persistence is a prominent measure for quantifying neural networkcomplexity, proposed in the emerging field of topological data analysis in deeplearning. In this work, however, we find both theoretically and empiricallythat the variance of network weights and spatial concentration of large weightsare the main factors that impact neural persistence. Whilst this capturesuseful information for linear classifiers, we find that no relevant spatialstructure is present in later layers of deep neural networks, making neuralpersistence roughly equivalent to the variance of weights. Additionally, theproposed averaging procedure across layers for deep neural networks does notconsider interaction between layers. Based on our analysis, we propose anextension of the filtration underlying neural persistence to the whole neuralnetwork instead of single layers, which is equivalent to calculating neuralpersistence on one particular matrix. This yields our deep graph persistencemeasure, which implicitly incorporates persistent paths through the network andalleviates variance-related issues through standardisation. Code is availableat  .", "output": "Addressing caveats of neural persistence with deep graph persistence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Captions are crucial for understanding scientific visualizations anddocuments. Existing captioning methods for scientific figures rely onfigure-caption pairs extracted from documents for training, many of which fallshort with respect to metrics like helpfulness, explainability, andvisual-descriptiveness [15] leading to generated captions being misaligned withreader preferences. To enable the generation of high-quality figure captions,we introduce FigCaps-HF a new framework for figure-caption generation that canincorporate domain expert feedback in generating captions optimized for readerpreferences. Our framework comprises of 1) an automatic method for evaluatingquality of figure-caption pairs, 2) a novel reinforcement learning with humanfeedback (RLHF) method to optimize a generative figure-to-caption model forreader preferences. We demonstrate the effectiveness of our simple learningframework by improving performance over standard fine-tuning across differenttypes of models. In particular, when using BLIP as the base model, our RLHFframework achieves a mean gain of 35.7%, 16.9%, and 9% in ROUGE, BLEU, andMeteor, respectively. Finally, we release a large-scale benchmark dataset withhuman feedback on figure-caption pairs to enable further evaluation anddevelopment of RLHF techniques for this problem.", "output": "FigCaps-HF: A Figure-to-Caption Generative Framework and Benchmark with Human Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Performance issues permeate large-scale cloud service systems, which can leadto huge revenue losses. To ensure reliable performance, it's essential toaccurately identify and localize these issues using service monitoring metrics.Given the complexity and scale of modern cloud systems, this task can bechallenging and may require extensive expertise and resources beyond thecapacity of individual humans. Some existing methods tackle this problem byanalyzing each metric independently to detect anomalies. However, this couldincur overwhelming alert storms that are difficult for engineers to diagnosemanually. To pursue better performance, not only the temporal patterns ofmetrics but also the correlation between metrics (i.e., relational patterns)should be considered, which can be formulated as a multivariate metrics anomalydetection problem. However, most of the studies fall short of extracting thesetwo types of features explicitly. Moreover, there exist some unlabeledanomalies mixed in the training data, which may hinder the detectionperformance. To address these limitations, we propose the Relational- TemporalAnomaly Detection Model (RTAnomaly) that combines the relational and temporalinformation of metrics. RTAnomaly employs a graph attention layer to learn thedependencies among metrics, which will further help pinpoint the anomalousmetrics that may cause the anomaly effectively. In addition, we exploit theconcept of positive unlabeled learning to address the issue of potentialanomalies in the training data. To evaluate our method, we conduct experimentson a public dataset and two industrial datasets. RTAnomaly outperforms all thebaseline models by achieving an average F1 score of 0.929 and Hit@3 of 0.920,demonstrating its superiority.", "output": "Performance Issue Identification in Cloud Systems with Relational-Temporal Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many recent theoretical works on emph{meta-learning} aim to achieveguarantees in leveraging similar representational structures from related taskstowards simplifying a target task. Importantly, the main aim in theory works onthe subject is to understand the extent to which convergence rates -- inlearning a common representation -- emph{may scale with the number $N$ oftasks} (as well as the number of samples per task). First steps in this settingdemonstrate this property when both the shared representation amongst tasks,and task-specific regression functions, are linear. This linear setting readilyreveals the benefits of aggregating tasks, e.g., via averaging arguments. Inpractice, however, the representation is often highly nonlinear, introducingnontrivial biases in each task that cannot easily be averaged out as in thelinear case. In the present work, we derive theoretical guarantees formeta-learning with nonlinear representations. In particular, assuming theshared nonlinearity maps to an infinite-dimensional RKHS, we show thatadditional biases can be mitigated with careful regularization that leveragesthe smoothness of task-specific regression functions,", "output": "Nonlinear Meta-Learning Can Guarantee Faster Rates."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The popularity of point cloud deep models for safety-critical purposes hasincreased, but the reliability and security of these models can be compromisedby intentional or naturally occurring point cloud noise. To combat this issue,we present a novel point cloud outlier removal method called PointCVaR, whichempowers standard-trained models to eliminate additional outliers and restorethe data. Our approach begins by conducting attribution analysis to determinethe influence of each point on the model output, which we refer to as pointrisk. We then optimize the process of filtering high-risk points usingConditional Value at Risk (CVaR) as the objective. The rationale for thisapproach is based on the observation that noise points in point clouds tend tocluster in the tail of the risk distribution, with a low frequency but a highlevel of risk, resulting in significant interference with classificationresults. Despite requiring no additional training effort, our method producesexceptional results in various removal-and-classification experiments for noisypoint clouds, which are corrupted by random noise, adversarial noise, andbackdoor trigger noise. Impressively, it achieves 87% accuracy in defenseagainst the backdoor attack by removing triggers. Overall, the proposedPointCVaR effectively eliminates noise points and enhances point cloudclassification, making it a promising plug-in module for various models indifferent scenarios.", "output": "Risk-optimized Outlier Removal for Robust Point Cloud Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The problem of matching markets has been studied for a long time in theliterature due to its wide range of applications. Finding a stable matching isa common equilibrium objective in this problem. Since market participants areusually uncertain of their preferences, a rich line of recent works study theonline setting where one-side participants (players) learn their unknownpreferences from iterative interactions with the other side (arms). Mostprevious works in this line are only able to derive theoretical guarantees forplayer-pessimal stable regret, which is defined compared with the players'least-preferred stable matching. However, under the pessimal stable matching,players only obtain the least reward among all stable matchings. To maximizeplayers' profits, player-optimal stable matching would be the most desirable.Though citet{basu21beyond} successfully bring an upper bound forplayer-optimal stable regret, their result can be exponentially large ifplayers' preference gap is small. Whether a polynomial guarantee for thisregret exists is a significant but still open problem. In this work, we providea new algorithm named explore-then-Gale-Shapley (ETGS) and show that theoptimal stable regret of each player can be upper bounded by $O(KlogT/Delta^2)$ where $K$ is the number of arms, $T$ is the horizon and $Delta$is the players' minimum preference gap among the first $N+1$-ranked arms. Thisresult significantly improves previous works which either have a weakerplayer-pessimal stable matching objective or apply only to markets with specialassumptions. When the preferences of participants satisfy some specialconditions, our regret upper bound also matches the previously derived lowerbound.", "output": "Player-optimal Stable Regret for Bandit Learning in Matching Markets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Abstraction is a key verification technique to improve scalability. However,its use for neural networks is so far extremely limited. Previous approachesfor abstracting classification networks replace several neurons with one ofthem that is similar enough. We can classify the similarity as defined eithersyntactically (using quantities on the connections between neurons) orsemantically (on the activation values of neurons for various inputs).Unfortunately, the previous approaches only achieve moderate reductions, whenimplemented at all. In this work, we provide a more flexible framework where aneuron can be replaced with a linear combination of other neurons, improvingthe reduction. We apply this approach both on syntactic and semanticabstractions, and implement and evaluate them experimentally. Further, weintroduce a refinement method for our abstractions, allowing for finding abetter balance between reduction and precision.", "output": "Syntactic vs Semantic Linear Abstraction and Refinement of Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Centrality metrics are vital for network analysis, but selecting the mostappropriate measures for specific applications remains challenging among the400+ proposed indices. Existing approaches -- model-based, data-driven, andaxiomatic -- have limitations. To address this, we introduce the cullingmethod, leveraging expert preferences regarding centrality behavior on simplegraphs. It involves forming a set of candidate measures, generating a list ofas small graphs as possible needed to ``separate'' measures from each other,constructing a decision-tree survey, and identifying the measure consistentwith expert responses. We apply this method to a diverse set of 40centralities, including new kernel-based measures, and combine it with theaxiomatic approach. Remarkably, only 13 small 1-trees suffice to separate all40 measures, among which there are pairs of close ones. The culling methodoffers a low-cost solution in terms of labor and time, complements existingmethods for measure selection, and reveals important peculiarities ofcentrality measures.", "output": "How to choose the most appropriate centrality measure? A decision tree approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a visualization method to understand the effect ofmultidimensional projection on local subspaces, using implicit functiondifferentiation. Here, we understand the local subspace as the multidimensionallocal neighborhood of data points. Existing methods focus on the projection ofmultidimensional data points, and the neighborhood information is ignored. Ourmethod is able to analyze the shape and directional information of the localsubspace to gain more insights into the global structure of the data throughthe perception of local structures. Local subspaces are fitted bymultidimensional ellipses that are spanned by basis vectors. An accurate andefficient vector transformation method is proposed based on analyticaldifferentiation of multidimensional projections formulated as implicitfunctions. The results are visualized as glyphs and analyzed using a full setof specifically-designed interactions supported in our efficient web-basedvisualization tool. The usefulness of our method is demonstrated using variousmulti- and high-dimensional benchmark datasets. Our implicit differentiationvector transformation is evaluated through numerical comparisons; the overallmethod is evaluated through exploration examples and use cases.", "output": "Implicit Multidimensional Projection of Local Subspaces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multilayer neural networks set the current state of the art for manytechnical classification problems. But, these networks are still, essentially,black boxes in terms of analyzing them and predicting their performance. Here,we develop a statistical theory for the one-layer perceptron and show that itcan predict performances of a surprisingly large variety of neural networkswith different architectures. A general theory of classification withperceptrons is developed by generalizing an existing theory for analyzingreservoir computing models and connectionist models for symbolic reasoningknown as vector symbolic architectures. Our statistical theory offers threeformulas leveraging the signal statistics with increasing detail. The formulasare analytically intractable, but can be evaluated numerically. The descriptionlevel that captures maximum details requires stochastic sampling methods.Depending on the network model, the simpler formulas already yield highprediction accuracy. The quality of the theory predictions is assessed in threeexperimental settings, a memorization task for echo state networks (ESNs) fromreservoir computing literature, a collection of classification datasets forshallow randomly connected networks, and the ImageNet dataset for deepconvolutional neural networks. We find that the second description level of theperceptron theory can predict the performance of types of ESNs, which could notbe described previously. The theory can predict deep multilayer neural networksby being applied to their output layer. While other methods for prediction ofneural networks performance commonly require to train an estimator model, theproposed theory requires only the first two moments of the distribution of thepostsynaptic sums in the output neurons. The perceptron theory comparesfavorably to other methods that do not rely on training an estimator model.", "output": "Perceptron Theory Can Predict the Accuracy of Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Principal Component Analysis (PCA) is a fundamental tool for datavisualization, denoising, and dimensionality reduction. It is widely popular inStatistics, Machine Learning, Computer Vision, and related fields. However, PCAis well-known to fall prey to outliers and often fails to detect the trueunderlying low-dimensional structure within the dataset. Following the Medianof Means (MoM) philosophy, recent supervised learning methods have shown greatsuccess in dealing with outlying observations without much compromise to theirlarge sample theoretical properties. This paper proposes a PCA procedure basedon the MoM principle. Called the textbf{M}edian of textbf{M}eanstextbf{P}rincipal textbf{C}omponent textbf{A}nalysis (MoMPCA), the proposedmethod is not only computationally appealing but also achieves optimalconvergence rates under minimal assumptions. In particular, we explore thenon-asymptotic error bounds of the obtained solution via the aid of theRademacher complexities while granting absolutely no assumption on the outlyingobservations. The derived concentration results are not dependent on thedimension because the analysis is conducted in a separable Hilbert space, andthe results only depend on the fourth moment of the underlying distribution inthe corresponding norm. The proposal's efficacy is also thoroughly showcasedthrough simulations and real data applications.", "output": "Robust Principal Component Analysis: A Median of Means Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "State-of-the-art performance for many edge applications is achieved by deepneural networks (DNNs). Often, these DNNs are location- and time-sensitive, andmust be delivered over a wireless channel rapidly and efficiently. In thispaper, we introduce AirNet, a family of novel training and transmission methodsthat allow DNNs to be efficiently delivered over wireless channels understringent transmit power and latency constraints. This corresponds to a newclass of joint source-channel coding problems, aimed at delivering DNNs withthe goal of maximizing their accuracy at the receiver, rather than recoveringthem with high fidelity. In AirNet, we propose the direct mapping of the DNNparameters to transmitted channel symbols, while the network is trained to meetthe channel constraints, and exhibit robustness against channel noise. AirNetachieves higher accuracy compared to separation-based alternatives. We furtherimprove the performance of AirNet by pruning the network below the availablebandwidth, and expanding it for improved robustness. We also benefit fromunequal error protection by selectively expanding important layers of thenetwork. Finally, we develop an approach, which simultaneously trains aspectrum of DNNs, each targeting a different channel condition, resolving theimpractical memory requirements of training distinct networks for differentchannel conditions.", "output": "AirNet: Neural Network Transmission over the Air."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training recurrent neural networks is known to be difficult when timedependencies become long. In this work, we show that most standard cells onlyhave one stable equilibrium at initialisation, and that learning on tasks withlong time dependencies generally occurs once the number of network stableequilibria increases; a property known as multistability. Multistability isoften not easily attained by initially monostable networks, making learning oflong time dependencies between inputs and outputs difficult. This insight leadsto the design of a novel way to initialise any recurrent cell connectivitythrough a procedure called \"warmup\" to improve its capability to learnarbitrarily long time dependencies. This initialisation procedure is designedto maximise network reachable multistability, i.e., the number of equilibriawithin the network that can be reached through relevant input trajectories, infew gradient steps. We show on several information restitution, sequenceclassification, and reinforcement learning benchmarks that warming up greatlyimproves learning speed and performance, for multiple recurrent cells, butsometimes impedes precision. We therefore introduce a double-layer architectureinitialised with a partial warmup that is shown to greatly improve learning oflong time dependencies while maintaining high levels of precision. Thisapproach provides a general framework for improving learning abilities of anyrecurrent cell when long time dependencies are present. We also showempirically that other initialisation and pretraining procedures from theliterature implicitly foster reachable multistability of recurrent cells.", "output": "Warming up recurrent neural networks to maximise reachable multistability greatly improves learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of model selection for the general stochasticcontextual bandits under the realizability assumption. We propose a successiverefinement based algorithm called Adaptive Contextual Bandit ({ttfamily ACB}),that works in phases and successively eliminates model classes that are toosimple to fit the given instance. We prove that this algorithm is adaptive,i.e., the regret rate order-wise matches that of any provable contextual banditalgorithm (ex. cite{falcon}), that needs the knowledge of the true modelclass. The price of not knowing the correct model class turns out to be only anadditive term contributing to the second order term in the regret bound. Thiscost possess the intuitive property that it becomes smaller as the model classbecomes easier to identify, and vice-versa. We also show that a much simplerexplore-then-commit (ETC) style algorithm also obtains similar regret bound,despite not knowing the true model class. However, the cost of model selectionis higher in ETC as opposed to in {ttfamily ACB}, as expected. Furthermore,for the special case of linear contextual bandits, we propose specializedalgorithms that obtain sharper guarantees compared to the generic setup.", "output": "Model Selection for Generic Contextual Bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern recommendation systems ought to benefit by probing for and learningfrom delayed feedback. Research has tended to focus on learning from a user'sresponse to a single recommendation. Such work, which leverages methods ofsupervised and bandit learning, forgoes learning from the user's subsequentbehavior. Where past work has aimed to learn from subsequent behavior, therehas been a lack of effective methods for probing to elicit informative delayedfeedback. Effective exploration through probing for delayed feedback becomesparticularly challenging when rewards are sparse. To address this, we developdeep exploration methods for recommendation systems. In particular, weformulate recommendation as a sequential decision problem and demonstratebenefits of deep exploration over single-step exploration. Our experiments arecarried out with high-fidelity industrial-grade simulators and establish largeimprovements over existing algorithms.", "output": "Deep Exploration for Recommendation Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We aim at capturing high-order statistics of feature vectors formed by aneural network, and propose end-to-end second- and higher-order pooling to forma tensor descriptor. Tensor descriptors require a robust similarity measure dueto low numbers of aggregated vectors and the burstiness phenomenon, when agiven feature appears more/less frequently than statistically expected. TheHeat Diffusion Process (HDP) on a graph Laplacian is closely related to theEigenvalue Power Normalization (EPN) of the covariance/auto-correlation matrix,whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPNplay the same role, i.e., to boost or dampen the magnitude of the eigenspectrumthus preventing the burstiness. We equip higher-order tensors with EPN whichacts as a spectral detector of higher-order occurrences to prevent burstiness.We also prove that for a tensor of order r built from d dimensional featuredescriptors, such a detector gives the likelihood if at least one higher-orderoccurrence is 'projected' into one of binom(d,r) subspaces represented by thetensor; thus forming a tensor power normalization metric endowed withbinom(d,r) such 'detectors'. For experimental contributions, we apply severalsecond- and higher-order pooling variants to action recognition, providepreviously not presented comparisons of such pooling variants, and showstate-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.", "output": "High-order Tensor Pooling with Attention for Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose simple nonparametric estimators for mediated and time-varying doseresponse curves based on kernel ridge regression. By embedding Pearl'smediation formula and Robins' g-formula with kernels, we allow treatments,mediators, and covariates to be continuous in general spaces, and also allowfor nonlinear treatment-confounder feedback. Our key innovation is areproducing kernel Hilbert space technique called sequential kernel embedding,which we use to construct simple estimators for complex causal estimands. Ourestimators preserve the generality of classic identification while alsoachieving nonasymptotic uniform rates. In nonlinear simulations with manycovariates, we demonstrate strong performance. We estimate mediated andtime-varying dose response curves of the US Job Corps, and clean data that mayserve as a benchmark in future work. We extend our results to mediated andtime-varying treatment effects and counterfactual distributions, verifyingsemiparametric efficiency and weak convergence.", "output": "Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The complex spatial-temporal correlations in transportation networks make thetraffic forecasting problem challenging. Since transportation system inherentlypossesses graph structures, much research efforts have been put with graphneural networks. Recently, constructing adaptive graphs to the data has shownpromising results over the models relying on a single static graph structure.However, the graph adaptations are applied during the training phases, and donot reflect the data used during the testing phases. Such shortcomings can beproblematic especially in traffic forecasting since the traffic data oftensuffers from the unexpected changes and irregularities in the time series. Inthis study, we propose a novel traffic forecasting framework called ProgressiveGraph Convolutional Network (PGCN). PGCN constructs a set of graphs byprogressively adapting to input data during the training and the testingphases. Specifically, we implemented the model to construct progressiveadjacency matrices by learning trend similarities among graph nodes. Then, themodel is combined with the dilated causal convolution and gated activation unitto extract temporal features. With residual and skip connections, PGCN performsthe traffic prediction. When applied to four real-world traffic datasets ofdiverse geometric nature, the proposed model achieves state-of-the-artperformance with consistency in all datasets. We conclude that the ability ofPGCN to progressively adapt to input data enables the model to generalize indifferent study sites with robustness.", "output": "PGCN: Progressive Graph Convolutional Networks for Spatial-Temporal Traffic Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The use of Machine Learning (ML) has rapidly spread across several fields,having encountered many applications in Structural Dynamics and Vibroacoustic(SD&amp;V). The increasing capabilities of ML to unveil insights from data, drivenby unprecedented data availability, algorithms advances and computationalpower, enhance decision making, uncertainty handling, patterns recognition andreal-time assessments. Three main applications in SD&amp;V have taken advantage ofthese benefits. In Structural Health Monitoring, ML detection and prognosislead to safe operation and optimized maintenance schedules. Systemidentification and control design are leveraged by ML techniques in ActiveNoise Control and Active Vibration Control. Finally, the so-called ML-basedsurrogate models provide fast alternatives to costly simulations, enablingrobust and optimized product design. Despite the many works in the area, theyhave not been reviewed and analyzed. Therefore, to keep track and understandthis ongoing integration of fields, this paper presents a survey of MLapplications in SD&amp;V analyses, shedding light on the current state ofimplementation and emerging opportunities. The main methodologies, advantages,limitations, and recommendations based on scientific knowledge were identifiedfor each of the three applications. Moreover, the paper considers the role ofDigital Twins and Physics Guided ML to overcome current challenges and powerfuture research progress. As a result, the survey provides a broad overview ofthe present landscape of ML applied in SD&amp;V and guides the reader to anadvanced understanding of progress and prospects in the field.", "output": "A Review of Machine Learning Methods Applied to Structural Dynamics and Vibroacoustic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hyperdimensional computing (HD), also known as vector symbolic architectures(VSA), is a framework for computing with distributed representations byexploiting properties of random high-dimensional vector spaces. The commitmentof the scientific community to aggregate and disseminate research in thisparticularly multidisciplinary area has been fundamental for its advancement.Joining these efforts, we present Torchhd, a high-performance open sourcePython library for HD/VSA. Torchhd seeks to make HD/VSA more accessible andserves as an efficient foundation for further research and applicationdevelopment. The easy-to-use library builds on top of PyTorch and featuresstate-of-the-art HD/VSA functionality, clear documentation, and implementationexamples from well-known publications. Comparing publicly available code withtheir corresponding Torchhd implementation shows that experiments can run up to100x faster. Torchhd is available at:", "output": "Torchhd: An Open Source Python Library to Support Research on Hyperdimensional Computing and Vector Symbolic Architectures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Encoding a driving scene into vector representations has been an essentialtask for autonomous driving that can benefit downstream tasks e.g. trajectoryprediction. The driving scene often involves heterogeneous elements such as thedifferent types of objects (agents, lanes, traffic signs) and the semanticrelations between objects are rich and diverse. Meanwhile, there also existrelativity across elements, which means that the spatial relation is a relativeconcept and need be encoded in a ego-centric manner instead of in a globalcoordinate system. Based on these observations, we propose HeterogeneousDriving Graph Transformer (HDGT), a backbone modelling the driving scene as aheterogeneous graph with different types of nodes and edges. For heterogeneousgraph construction, we connect different types of nodes according to diversesemantic relations. For spatial relation encoding, the coordinates of the nodeas well as its in-edges are in the local node-centric coordinate system. Forthe aggregation module in the graph neural network (GNN), we adopt thetransformer structure in a hierarchical way to fit the heterogeneous nature ofinputs. Experimental results show that HDGT achieves state-of-the-artperformance for the task of trajectory prediction, on INTERACTION PredictionChallenge and Waymo Open Motion Challenge.", "output": "HDGT: Heterogeneous Driving Graph Transformer for Multi-Agent Trajectory Prediction via Scene Encoding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There is a significant need for principled uncertainty reasoning in machinelearning systems as they are increasingly deployed in safety-critical domains.A new approach with uncertainty-aware regression-based neural networks (NNs),based on learning evidential distributions for aleatoric and epistemicuncertainties, shows promise over traditional deterministic methods and typicalBayesian NNs, notably with the capabilities to disentangle aleatoric andepistemic uncertainties. Despite some empirical success of Deep EvidentialRegression (DER), there are important gaps in the mathematical foundation thatraise the question of why the proposed technique seemingly works. We detail thetheoretical shortcomings and analyze the performance on synthetic andreal-world data sets, showing that Deep Evidential Regression is a heuristicrather than an exact uncertainty quantification. We go on to discusscorrections and redefinitions of how aleatoric and epistemic uncertaintiesshould be extracted from NNs.", "output": "The Unreasonable Effectiveness of Deep Evidential Regression."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dense retrieval overcome the lexical gap and has shown great success inad-hoc information retrieval (IR). Despite their success, dense retrievers areexpensive to serve across practical use cases. For use cases requiring tosearch from millions of documents, the dense index becomes bulky and requireshigh memory usage for storing the index. More recently, learning-to-hash (LTH)techniques, for e.g., BPR and JPQ, produce binary document vectors, therebyreducing the memory requirement to efficiently store the dense index. LTHtechniques are supervised and finetune the retriever using a ranking loss. Theyoutperform their counterparts, i.e., traditional out-of-the-box vectorcompression techniques such as PCA or PQ. A missing piece from prior work isthat existing techniques have been evaluated only in-domain, i.e., on a singledataset such as MS MARCO. In our work, we evaluate LTH and vector compressiontechniques for improving the downstream zero-shot retrieval accuracy of theTAS-B dense retriever while maintaining efficiency at inference. Our resultsdemonstrate that, unlike prior work, LTH strategies when applied naively canunderperform the zero-shot TAS-B dense retriever on average by up to 14%nDCG@10 on the BEIR benchmark. To solve this limitation, in our work, wepropose an easy yet effective solution of injecting domain adaptation withexisting supervised LTH techniques. We experiment with two well-knownunsupervised domain adaptation techniques: GenQ and GPL. Our domain adaptationinjection technique can improve the downstream zero-shot retrievaleffectiveness for both BPR and JPQ variants of the TAS-B model by on average11.5% and 8.2% nDCG@10 while both maintaining 32$times$ memory efficiency and14$times$ and 2$times$ speedup respectively in CPU retrieval latency on BEIR.All our code, models, and data are publicly available at", "output": "Injecting Domain Adaptation with Learning-to-hash for Effective and Efficient Zero-shot Dense Retrieval."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training even moderately-sized generative models with differentially-privatestochastic gradient descent (DP-SGD) is difficult: the required level of noisefor reasonable levels of privacy is simply too large. We advocate insteadbuilding off a good, relevant representation on an informative public dataset,then learning to model the private data with that representation. Inparticular, we minimize the maximum mean discrepancy (MMD) between privatetarget data and a generator's distribution, using a kernel based on perceptualfeatures learned from a public dataset. With the MMD, we can simply privatizethe data-dependent term once and for all, rather than introducing noise at eachstep of optimization as in DP-SGD. Our algorithm allows us to generateCIFAR10-level images with $epsilon approx 2$ which capture distinctivefeatures in the distribution, far surpassing the current state of the art,which mostly focuses on datasets such as MNIST and FashionMNIST at a large$epsilon approx 10$. Our work introduces simple yet powerful foundations forreducing the gap between private and non-private deep generative models. Ourcode is available at url{", "output": "Pre-trained Perceptual Features Improve Differentially Private Image Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, deep generative models have attracted increasing interestdue to their capacity to model complex distributions. Among those models,variational autoencoders have gained popularity as they have proven both to becomputationally efficient and yield impressive results in multiple fields.Following this breakthrough, extensive research has been done in order toimprove the original publication, resulting in a variety of different VAEmodels in response to different tasks. In this paper we present Pythae, aversatile open-source Python library providing both a unified implementationand a dedicated framework allowing straightforward, reproducible and reliableuse of generative autoencoder models. We then propose to use this library toperform a case study benchmark where we present and compare 19 generativeautoencoder models representative of some of the main improvements ondownstream tasks such as image reconstruction, generation, classification,clustering and interpolation. The open-source library can be found at", "output": "Pythae: Unifying Generative Autoencoders in Python -- A Benchmarking Use Case."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Random noise arising from physical processes is an inherent characteristic ofmeasurements and a limiting factor for most signal processing and data analysistasks. Given the recent interest in generative adversarial networks (GANs) fordata-driven modeling, it is important to determine to what extent GANs canfaithfully reproduce noise in target data sets. In this paper, we present anempirical investigation that aims to shed light on this issue for time series.Namely, we assess two general-purpose GANs for time series that are based onthe popular deep convolutional GAN (DCGAN) architecture, a direct time-seriesmodel and an image-based model that uses a short-time Fourier transform (STFT)data representation. The GAN models are trained and quantitatively evaluatedusing distributions of simulated noise time series with known ground-truthparameters. Target time series distributions include a broad range of noisetypes commonly encountered in physical measurements, electronics, andcommunication systems: band-limited thermal noise, power law noise, shot noise,and impulsive noise. We find that GANs are capable of learning many noisetypes, although they predictably struggle when the GAN architecture is not wellsuited to some aspects of the noise, e.g., impulsive time-series with extremeoutliers. Our findings provide insights into the capabilities and potentiallimitations of current approaches to time-series GANs and highlight areas forfurther research. In addition, our battery of tests provides a useful benchmarkto aid the development of deep generative models for time series.", "output": "Data-Driven Modeling of Noise Time Series with Convolutional Generative Adversarial Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While much progress has been made in understanding the minimax samplecomplexity of reinforcement learning (RL) -- the complexity of learning on the\"worst-case\" instance -- such measures of complexity often do not capture thetrue difficulty of learning. In practice, on an \"easy\" instance, we might hopeto achieve a complexity far better than that achievable on the worst-caseinstance. In this work we seek to understand the \"instance-dependent\"complexity of learning near-optimal policies (PAC RL) in the setting of RL withlinear function approximation. We propose an algorithm, textsc{Pedel}, whichachieves a fine-grained instance-dependent measure of complexity, the first ofits kind in the RL with function approximation setting, thereby capturing thedifficulty of learning on each particular problem instance. Through an explicitexample, we show that textsc{Pedel} yields provable gains over low-regret,minimax-optimal algorithms and that such algorithms are unable to hit theinstance-optimal rate. Our approach relies on a novel online experimentdesign-based procedure which focuses the exploration budget on the \"directions\"most relevant to learning a near-optimal policy, and may be of independentinterest.", "output": "Instance-Dependent Near-Optimal Policy Identification in Linear MDPs via Online Experiment Design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The tuning of stochastic gradient algorithms (SGAs) for optimization andsampling is often based on heuristics and trial-and-error rather thangeneralizable theory. We address this theory--practice gap by characterizingthe large-sample statistical asymptotics of SGAs via a jointstep-size--sample-size scaling limit. We show that iterate averaging with alarge fixed step size is robust to the choice of tuning parameters andasymptotically has covariance proportional to that of the MLE samplingdistribution. We also prove a Bernstein--von Mises-like theorem to guidetuning, including for generalized posteriors that are robust to modelmisspecification. Numerical experiments validate our results andrecommendations in realistic finite-sample regimes. Our work lays thefoundation for a systematic analysis of other stochastic gradient Markov chainMonte Carlo algorithms for a wide range of models.", "output": "Tuning Stochastic Gradient Algorithms for Statistical Inference via Large-Sample Asymptotics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Motivated by the successes of deep learning, we propose a class of neuralnetwork-based discrete choice models, called RUMnets, inspired by the randomutility maximization (RUM) framework. This model formulates the agents' randomutility function using a sample average approximation. We show that RUMnetssharply approximate the class of RUM discrete choice models: any model derivedfrom random utility maximization has choice probabilities that can beapproximated arbitrarily closely by a RUMnet. Reciprocally, any RUMnet isconsistent with the RUM principle. We derive an upper bound on thegeneralization error of RUMnets fitted on choice data, and gain theoreticalinsights on their ability to predict choices on new, unseen data depending oncritical parameters of the dataset and architecture. By leveraging open-sourcelibraries for neural networks, we find that RUMnets are competitive againstseveral choice modeling and machine learning methods in terms of predictiveaccuracy on two real-world datasets.", "output": "Representing Random Utility Choice Models with Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Question answering over temporal knowledge graphs (TKGQA) has recently foundincreasing interest. TKGQA requires temporal reasoning techniques to extractthe relevant information from temporal knowledge bases. The only existing TKGQAdataset, i.e., CronQuestions, consists of temporal questions based on the factsfrom a fixed time period, where a temporal knowledge graph (TKG) spanning thesame period can be fully used for answer inference, allowing the TKGQA modelsto use even the future knowledge to answer the questions based on the pastfacts. In real-world scenarios, however, it is also common that given theknowledge until now, we wish the TKGQA systems to answer the questions askingabout the future. As humans constantly seek plans for the future, buildingTKGQA systems for answering such forecasting questions is important.Nevertheless, this has still been unexplored in previous research. In thispaper, we propose a novel task: forecasting question answering over temporalknowledge graphs. We also propose a large-scale TKGQA benchmark dataset, i.e.,ForecastTKGQuestions, for this task. It includes three types of questions,i.e., entity prediction, yes-no, and fact reasoning questions. For everyforecasting question in our dataset, QA models can only have access to the TKGinformation before the timestamp annotated in the given question for answerinference. We find that the state-of-the-art TKGQA methods perform poorly onforecasting questions, and they are unable to answer yes-no questions and factreasoning questions. To this end, we propose ForecastTKGQA, a TKGQA model thatemploys a TKG forecasting module for future inference, to answer all threetypes of questions. Experimental results show that ForecastTKGQA outperformsrecent TKGQA methods on the entity prediction questions, and it also showsgreat effectiveness in answering the other two types of questions.", "output": "ForecastTKGQuestions: A Benchmark for Temporal Question Answering and Forecasting over Temporal Knowledge Graphs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Online extremism has severe societal consequences, including normalizing hatespeech, user radicalization, and increased social divisions. Various mitigationstrategies have been explored to address these consequences. One such strategyuses positive interventions: controlled signals that add attention to theopinion ecosystem to boost certain opinions. To evaluate the effectiveness ofpositive interventions, we introduce the Opinion Market Model (OMM), a two-tieronline opinion ecosystem model that considers both inter-opinion interactionsand the role of positive interventions. The size of the opinion attentionmarket is modeled in the first tier using the multivariate discrete-time Hawkesprocess; in the second tier, opinions cooperate and compete for market share,given limited attention using the market share attraction model. We demonstratethe convergence of our proposed estimation scheme on a synthetic dataset. Next,we test OMM on two learning tasks, applying to two real-world datasets topredict attention market shares and uncover latent relationships between onlineitems. The first dataset comprises Facebook and Twitter discussions containingmoderate and far-right opinions about bushfires and climate change. The seconddataset captures popular VEVO artists' YouTube and Twitter attention volumes.OMM outperforms the state-of-the-art predictive models on both datasets andcaptures latent cooperation-competition relations. We uncover (1) self- andcross-reinforcement between far-right and moderate opinions on the bushfiresand (2) pairwise artist relations that correlate with real-world interactionssuch as collaborations and long-lasting feuds. Lastly, we use OMM as a testbedfor positive interventions and show how media coverage modulates the spread offar-right opinions.", "output": "Opinion Market Model: Stemming Far-Right Opinion Spread using Positive Interventions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interpretability of Deep Learning (DL) is a barrier to trustworthy AI.Despite great efforts made by the Explainable AI (XAI) community, explanationslack robustness -- indistinguishable input perturbations may lead to differentXAI results. Thus, it is vital to assess how robust DL interpretability is,given an XAI method. In this paper, we identify several challenges that thestate-of-the-art is unable to cope with collectively: i) existing metrics arenot comprehensive; ii) XAI techniques are highly heterogeneous; iii)misinterpretations are normally rare events. To tackle these challenges, weintroduce two black-box evaluation methods, concerning the worst-caseinterpretation discrepancy and a probabilistic notion of how robust in general,respectively. Genetic Algorithm (GA) with bespoke fitness function is used tosolve constrained optimisation for efficient worst-case evaluation. SubsetSimulation (SS), dedicated to estimate rare event probabilities, is used forevaluating overall robustness. Experiments show that the accuracy, sensitivity,and efficiency of our methods outperform the state-of-the-arts. Finally, wedemonstrate two applications of our methods: ranking robust XAI methods andselecting training schemes to improve both classification and interpretationrobustness.", "output": "SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A powerful category of (invisible) data poisoning attacks modify a subset oftraining examples by small adversarial perturbations to change the predictionof certain test-time data. Existing defense mechanisms are not desirable todeploy in practice, as they often either drastically harm the generalizationperformance, or are attack-specific, and prohibitively slow to apply. Here, wepropose a simple but highly effective approach that unlike existing methodsbreaks various types of invisible poisoning attacks with the slightest drop inthe generalization performance. We make the key observation that attacksintroduce local sharp regions of high training loss, which when minimized,results in learning the adversarial perturbations and makes the attacksuccessful. To break poisoning attacks, our key idea is to alleviate the sharploss regions introduced by poisons. To do so, our approach comprises twocomponents: an optimized friendly noise that is generated to maximally perturbexamples without degrading the performance, and a randomly varying noisecomponent. The combination of both components builds a very light-weight butextremely effective defense against the most powerful triggerless targeted andhidden-trigger backdoor poisoning attacks, including Gradient Matching,Bulls-eye Polytope, and Sleeper Agent. We show that our friendly noise istransferable to other architectures, and adaptive attacks cannot break ourdefense due to its random noise component. Our code is available at:", "output": "Friendly Noise against Adversarial Noise: A Powerful Defense against Data Poisoning Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As a successful approach to self-supervised learning, contrastive learningaims to learn invariant information shared among distortions of the inputsample. While contrastive learning has yielded continuous advancements insampling strategy and architecture design, it still remains two persistentdefects: the interference of task-irrelevant information and sampleinefficiency, which are related to the recurring existence of trivial constantsolutions. From the perspective of dimensional analysis, we find out that thedimensional redundancy and dimensional confounder are the intrinsic issuesbehind the phenomena, and provide experimental evidence to support ourviewpoint. We further propose a simple yet effective approach MetaMask, shortfor the dimensional Mask learned by Meta-learning, to learn representationsagainst dimensional redundancy and confounder. MetaMask adopts theredundancy-reduction technique to tackle the dimensional redundancy issue andinnovatively introduces a dimensional mask to reduce the gradient effects ofspecific dimensions containing the confounder, which is trained by employing ameta-learning paradigm with the objective of improving the performance ofmasked representations on a typical self-supervised task. We provide solidtheoretical analyses to prove MetaMask can obtain tighter risk bounds fordownstream classification compared to typical contrastive methods. Empirically,our method achieves state-of-the-art performance on various benchmarks.", "output": "MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning is gaining popularity as it enables training high-utilitymodels across several clients without directly sharing their private data. As adownside, the federated setting makes the model vulnerable to variousadversarial attacks in the presence of malicious clients. Despite thetheoretical and empirical success in defending against attacks that aim todegrade models' utility, defense against backdoor attacks that increase modelaccuracy on backdoor samples exclusively without hurting the utility on othersamples remains challenging. To this end, we first analyze the vulnerability offederated learning to backdoor attacks over a flat loss landscape which iscommon for well-designed neural networks such as Resnet [He et al., 2015] butis often overlooked by previous works. Over a flat loss landscape, misleadingfederated learning models to exclusively benefit malicious clients withbackdoor samples do not require a significant difference between malicious andbenign client-wise updates, making existing defenses insufficient. In contrast,we propose an invariant aggregator that redirects the aggregated update toinvariant directions that are generally useful via selectively masking out thegradient elements that favor few and possibly malicious clients regardless ofthe difference magnitude. Theoretical results suggest that our approachprovably mitigates backdoor attacks over both flat and sharp loss landscapes.Empirical results on three datasets with different modalities and varyingnumbers of clients further demonstrate that our approach mitigates a broadclass of backdoor attacks with a negligible cost on the model utility.", "output": "Invariant Aggregator for Defending against Federated Backdoor Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Distributional assumptions have been shown to be necessary for the robustlearnability of concept classes when considering the exact-in-the-ball robustrisk and access to random examples by Gourdeau et al. (2019). In this paper, westudy learning models where the learner is given more power through the use oflocal queries, and give the first distribution-free algorithms that performrobust empirical risk minimization (ERM) for this notion of robustness. Thefirst learning model we consider uses local membership queries (LMQ), where thelearner can query the label of points near the training sample. We show that,under the uniform distribution, LMQs do not increase the robustness thresholdof conjunctions and any superclass, e.g., decision lists and halfspaces. Facedwith this negative result, we introduce the local equivalence query($mathsf{LEQ}$) oracle, which returns whether the hypothesis and targetconcept agree in the perturbation region around a point in the training sample,as well as a counterexample if it exists. We show a separation result: on theone hand, if the query radius $lambda$ is strictly smaller than theadversary's perturbation budget $rho$, then distribution-free robust learningis impossible for a wide variety of concept classes; on the other hand, thesetting $lambda=rho$ allows us to develop robust ERM algorithms. We thenbound the query complexity of these algorithms based on online learningguarantees and further improve these bounds for the special case ofconjunctions. We finish by giving robust learning algorithms for halfspaces on${0,1}^n$ and then obtaining robustness guarantees for halfspaces in$mathbb{R}^n$ against precision-bounded adversaries.", "output": "When are Local Queries Useful for Robust Learning?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data augmentation is essential to achieve state-of-the-art performance inmany deep learning applications. However, the most effective augmentationtechniques become computationally prohibitive for even medium-sized datasets.To address this, we propose a rigorous technique to select subsets of datapoints that when augmented, closely capture the training dynamics of full dataaugmentation. We first show that data augmentation, modeled as additiveperturbations, improves learning and generalization by relatively enlarging andperturbing the smaller singular values of the network Jacobian, whilepreserving its prominent directions. This prevents overfitting and enhanceslearning the harder to learn information. Then, we propose a framework toiteratively extract small subsets of training data that when augmented, closelycapture the alignment of the fully augmented Jacobian with labels/residuals. Weprove that stochastic gradient descent applied to the augmented subsets foundby our approach has similar training dynamics to that of fully augmented data.Our experiments demonstrate that our method achieves 6.3x speedup on CIFAR10and 2.2x speedup on SVHN, and outperforms the baselines by up to 10% acrossvarious subset sizes. Similarly, on TinyImageNet and ImageNet, our method beatsthe baselines by up to 8%, while achieving up to 3.3x speedup across varioussubset sizes. Finally, training on and augmenting 50% subsets using our methodon a version of CIFAR10 corrupted with label noise even outperforms using thefull dataset. Our code is available at:", "output": "Data-Efficient Augmentation for Training Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A key challenge in solving the deterministic inverse reinforcement learning(IRL) problem online and in real-time is the existence of multiple solutions.Nonuniqueness necessitates the study of the notion of equivalent solutions,i.e., solutions that result in a different cost functional but same feedbackmatrix, and convergence to such solutions. While offline algorithms that resultin convergence to equivalent solutions have been developed in the literature,online, real-time techniques that address nonuniqueness are not available. Inthis paper, a regularized history stack observer that converges toapproximately equivalent solutions of the IRL problem is developed. Noveldata-richness conditions are developed to facilitate the analysis andsimulation results are provided to demonstrate the effectiveness of thedeveloped technique.", "output": "Nonuniqueness and Convergence to Equivalent Solutions in Observer-based Inverse Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Two central paradigms have emerged in the reinforcement learning (RL)community: online RL and offline RL. In the online RL setting, the agent has noprior knowledge of the environment, and must interact with it in order to findan $epsilon$-optimal policy. In the offline RL setting, the learner insteadhas access to a fixed dataset to learn from, but is unable to otherwiseinteract with the environment, and must obtain the best policy it can from thisoffline data. Practical scenarios often motivate an intermediate setting: if wehave some set of offline data and, in addition, may also interact with theenvironment, how can we best use the offline data to minimize the number ofonline interactions necessary to learn an $epsilon$-optimal policy?In this work, we consider this setting, which we call the textsf{FineTuneRL}setting, for MDPs with linear structure. We characterize the necessary numberof online samples needed in this setting given access to some offline dataset,and develop an algorithm, textsc{FTPedel}, which is provably optimal, up to$H$ factors. We show through an explicit example that combining offline datawith online interactions can lead to a provable improvement over either purelyoffline or purely online RL. Finally, our results illustrate the distinctionbetween emph{verifiable} learning, the typical setting considered in onlineRL, and emph{unverifiable} learning, the setting often considered in offlineRL, and show that there is a formal separation between these regimes.", "output": "Leveraging Offline Data in Online Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of global optimization with noisy zeroth orderoracles - a well-motivated problem useful for various applications ranging fromhyper-parameter tuning for deep learning to new material design. Existing workrelies on Gaussian processes or other non-parametric family, which suffers fromthe curse of dimensionality. In this paper, we propose a new algorithm GO-UCBthat leverages a parametric family of functions (e.g., neural networks)instead. Under a realizable assumption and a few other mild geometricconditions, we show that GO-UCB achieves a cumulative regret of ~O$(sqrt{T})$where $T$ is the time horizon. At the core of GO-UCB is a carefully designeduncertainty set over parameters based on gradients that allows optimisticexploration. Synthetic and real-world experiments illustrate GO-UCB worksbetter than popular Bayesian optimization approaches, even if the model ismisspecified.", "output": "Global Optimization with Parametric Function Approximation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Consider the problem of exploration in sparse-reward or reward-freeenvironments, such as in Montezuma's Revenge. In the curiosity-driven paradigm,the agent is rewarded for how much each realized outcome differs from theirpredicted outcome. But using predictive error as intrinsic motivation isfragile in stochastic environments, as the agent may become trapped byhigh-entropy areas of the state-action space, such as a \"noisy TV\". In thiswork, we study a natural solution derived from structural causal models of theworld: Our key idea is to learn representations of the future that captureprecisely the unpredictable aspects of each outcome -- which we use asadditional input for predictions, such that intrinsic rewards only reflect thepredictable aspects of world dynamics. First, we propose incorporating suchhindsight representations into models to disentangle \"noise\" from \"novelty\",yielding Curiosity in Hindsight: a simple and scalable generalization ofcuriosity that is robust to stochasticity. Second, we instantiate thisframework for the recently introduced BYOL-Explore algorithm as our primeexample, resulting in the noise-robust BYOL-Hindsight. Third, we illustrate itsbehavior under a variety of different stochasticities in a grid world, and findimprovements over BYOL-Explore in hard-exploration Atari games with stickyactions. Notably, we show state-of-the-art results in exploring Montezuma'sRevenge with sticky actions, while preserving performance in the non-stickysetting.", "output": "Curiosity in Hindsight: Intrinsic Exploration in Stochastic Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Positive unlabeled learning is a binary classification problem with positiveand unlabeled data. It is common in domains where negative labels are costly orimpossible to obtain, e.g., medicine and personalized advertising. Mostapproaches to positive unlabeled learning apply to specific data types (e.g.,images, categorical data) and can not generate new positive and negativesamples. This work introduces a feature-space distance-based tensor networkapproach to the positive unlabeled learning problem. The presented method isnot domain specific and significantly improves the state-of-the-art results onthe MNIST image and 15 categorical/mixed datasets. The trained tensor networkmodel is also a generative model and enables the generation of new positive andnegative instances.", "output": "Positive unlabeled learning with tensor networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Chaos and turbulence are complex physical phenomena, yet a precise definitionof the complexity measure that quantifies them is still lacking. In this workwe consider the relative complexity of chaos and turbulence from theperspective of deep neural networks. We analyze a set of classificationproblems, where the network has to distinguish images of fluid profiles in theturbulent regime from other classes of images such as fluid profiles in thechaotic regime, various constructions of noise and real world images. Weanalyze incompressible as well as weakly compressible fluid flows. We quantifythe complexity of the computation performed by the network via the intrinsicdimensionality of the internal feature representations, and calculate theeffective number of independent features which the network uses in order todistinguish between classes. In addition to providing a numerical estimate ofthe complexity of the computation, the measure also characterizes the neuralnetwork processing at intermediate and final stages. We construct adversarialexamples and use them to identify the two point correlation spectra for thechaotic and turbulent vorticity as the feature used by the network forclassification.", "output": "Neural Network Complexity of Chaos and Turbulence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To improve the uncertainty quantification of variance networks, we propose anovel tree-structured local neural network model that partitions the featurespace into multiple regions based on uncertainty heterogeneity. A tree is builtupon giving the training data, whose leaf nodes represent different regionswhere region-specific neural networks are trained to predict both the mean andthe variance for quantifying uncertainty. The proposed Uncertainty-SplittingNeural Regression Tree (USNRT) employs novel splitting criteria. At each node,a neural network is trained on the full data first, and a statistical test forthe residuals is conducted to find the best split, corresponding to the twosub-regions with the most significant uncertainty heterogeneity between them.USNRT is computationally friendly because very few leaf nodes are sufficientand pruning is unnecessary. Furthermore, an ensemble version can be easilyconstructed to estimate the total uncertainty including the aleatory andepistemic. On extensive UCI datasets, USNRT or its ensemble shows superiorperformance compared to some recent popular methods for quantifying uncertaintywith variances. Through comprehensive visualization and analysis, we uncoverhow USNRT works and show its merits, revealing that uncertainty heterogeneitydoes exist in many datasets and can be learned by USNRT.", "output": "Improving Uncertainty Quantification of Variance Networks by Tree-Structured Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Partial differential equations (PDEs) are important tools to model physicalsystems and including them into machine learning models is an important way ofincorporating physical knowledge. Given any system of linear PDEs with constantcoefficients, we propose a family of Gaussian process (GP) priors, which wecall EPGP, such that all realizations are exact solutions of this system. Weapply the Ehrenpreis-Palamodov fundamental principle, which works as anon-linear Fourier transform, to construct GP kernels mirroring standardspectral methods for GPs. Our approach can infer probable solutions of linearPDE systems from any data such as noisy measurements, or pointwise definedinitial and boundary conditions. Constructing EPGP-priors is algorithmic,generally applicable, and comes with a sparse version (S-EPGP) that learns therelevant spectral frequencies and works better for big data sets. Wedemonstrate our approach on three families of systems of PDEs, the heatequation, wave equation, and Maxwell's equations, where we improve upon thestate of the art in computation time and precision, in some experiments byseveral orders of magnitude.", "output": "Gaussian Process Priors for Systems of Linear Partial Differential Equations with Constant Coefficients."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data-driven optimization uses contextual information and machine learningalgorithms to find solutions to decision problems with uncertain parameters.While a vast body of work is dedicated to interpreting machine learning modelsin the classification setting, explaining decision pipelines involving learningalgorithms remains unaddressed. This lack of interpretability can block theadoption of data-driven solutions as practitioners may not understand or trustthe recommended decisions. We bridge this gap by introducing a counterfactualexplanation methodology tailored to explain solutions to data-driven problems.We introduce two classes of explanations and develop methods to find nearestexplanations of random forest and nearest-neighbor predictors. We demonstrateour approach by explaining key problems in operations management such asinventory management and routing.", "output": "Explainable Data-Driven Optimization: From Context to Decision and Back Again."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The utilization of programming language (PL) models, pre-trained onlarge-scale code corpora, as a means of automating software engineeringprocesses has demonstrated considerable potential in streamlining various codegeneration tasks such as code completion, code translation, and programsynthesis. However, current approaches mainly rely on supervised fine-tuningobjectives borrowed from text generation, neglecting unique sequence-levelcharacteristics of code, including but not limited to compilability as well assyntactic and functional correctness. To address this limitation, we proposePPOCoder, a new framework for code generation that synergistically combinespre-trained PL models with Proximal Policy Optimization (PPO) which is a widelyused deep reinforcement learning technique. By utilizing non-differentiablefeedback from code execution and structure alignment, PPOCoder seamlesslyintegrates external code-specific knowledge into the model optimizationprocess. It's important to note that PPOCoder is a task-agnostic andmodel-agnostic framework that can be used across different code generationtasks and PLs. Extensive experiments on three code generation tasks demonstratethe effectiveness of our proposed approach compared to SOTA methods, achievingsignificant improvements in compilation success rates and functionalcorrectness across different PLs.", "output": "Execution-based Code Generation using Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the mathematical capabilities of two iterations of ChatGPT(released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them onpublicly available datasets, as well as hand-crafted ones, using a novelmethodology. In contrast to formal mathematics, where large databases of formalproofs are available (e.g., the Lean Mathematical Library), current datasets ofnatural-language mathematics, used to benchmark language models, either coveronly elementary mathematics or are very small. We address this by publiclyreleasing two new datasets: GHOSTS and miniGHOSTS. These are the firstnatural-language datasets curated by working researchers in mathematics that(1) aim to cover graduate-level mathematics, (2) provide a holistic overview ofthe mathematical capabilities of language models, and (3) distinguish multipledimensions of mathematical reasoning. These datasets also test whether ChatGPTand GPT-4 can be helpful assistants to professional mathematicians by emulatinguse cases that arise in the daily professional activities of mathematicians. Webenchmark the models on a range of fine-grained performance metrics. Foradvanced mathematics, this is the most detailed evaluation effort to date. Wefind that ChatGPT can be used most successfully as a mathematical assistant forquerying facts, acting as a mathematical search engine and knowledge baseinterface. GPT-4 can additionally be used for undergraduate-level mathematicsbut fails on graduate-level difficulty. Contrary to many positive reports inthe media about GPT-4 and ChatGPT's exam-solving abilities (a potential case ofselection bias), their overall mathematical performance is well below the levelof a graduate student. Hence, if your goal is to use ChatGPT to pass agraduate-level math exam, you would be better off copying from your averagepeer!", "output": "Mathematical Capabilities of ChatGPT."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent approaches build on implicit neural representations (INRs) to proposegenerative models over function spaces. However, they are computationallycostly when dealing with inference tasks, such as missing data imputation, ordirectly cannot tackle them. In this work, we propose a novel deep generativemodel, named VAMoH. VAMoH combines the capabilities of modeling continuousfunctions using INRs and the inference capabilities of Variational Autoencoders(VAEs). In addition, VAMoH relies on a normalizing flow to define the prior,and a mixture of hypernetworks to parametrize the data log-likelihood. Thisgives VAMoH a high expressive capability and interpretability. Throughexperiments on a diverse range of data types, such as images, voxels, andclimate data, we show that VAMoH can effectively learn rich distributions overcontinuous functions. Furthermore, it can perform inference-related tasks, suchas conditional super-resolution generation and in-painting, as well or betterthan previous approaches, while being less computationally demanding.", "output": "Variational Mixture of HyperGenerators for Learning Distributions Over Functions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This note describes a new approach to classifying graphs that leverages graphgenerative models (GGM). Assuming a GGM that defines a joint probabilitydistribution over graphs and their class labels, I derive classificationformulas for the probability of a class label given a graph. A new conditionalELBO can be used to train a generative graph auto-encoder model fordiscrimination. While leveraging generative models for classification has beenwell explored for non-relational i.i.d. data, to our knowledge it is a novelapproach to graph classification.", "output": "From Graph Generation to Graph Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous driving (AD) perception today relies heavily on deep learningbased architectures requiring large scale annotated datasets with theirassociated costs for curation and annotation. The 3D semantic data are usefulfor core perception tasks such as obstacle detection and ego-vehiclelocalization. We propose a new dataset, Navya 3D Segmentation (Navya3DSeg),with a diverse label space corresponding to a large scale production gradeoperational domain, including rural, urban, industrial sites and universitiesfrom 13 countries. It contains 23 labeled sequences and 25 supplementarysequences without labels, designed to explore self-supervised andsemi-supervised semantic segmentation benchmarks on point clouds. We alsopropose a novel method for sequential dataset split generation based oniterative multi-label stratification, and demonstrated to achieve a +1.2% mIoUimprovement over the original split proposed by SemanticKITTI dataset. Acomplete benchmark for semantic segmentation task was performed, with state ofthe art methods. Finally, we demonstrate an Active Learning (AL) based datasetdistillation framework. We introduce a novel heuristic-free sampling methodcalled ego-pose distance based sampling in the context of AL. A detailedpresentation on the dataset is available here", "output": "Navya3DSeg -- Navya 3D Semantic Segmentation Dataset & split generation for autonomous vehicles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The bulk of existing research in defending against adversarial examplesfocuses on defending against a single (typically bounded Lp-norm) attack, butfor a practical setting, machine learning (ML) models should be robust to awide variety of attacks. In this paper, we present the first unified frameworkfor considering multiple attacks against ML models. Our framework is able tomodel different levels of learner's knowledge about the test-time adversary,allowing us to model robustness against unforeseen attacks and robustnessagainst unions of attacks. Using our framework, we present the firstleaderboard, MultiRobustBench, for benchmarking multiattack evaluation whichcaptures performance across attack types and attack strengths. We evaluate theperformance of 16 defended models for robustness against a set of 9 differentattack types, including Lp-based threat models, spatial transformations, andcolor changes, at 20 different attack strengths (180 attacks total).Additionally, we analyze the state of current defenses against multipleattacks. Our analysis shows that while existing defenses have made progress interms of average robustness across the set of attacks used, robustness againstthe worst-case attack is still a big open problem as all existing modelsperform worse than random guessing.", "output": "MultiRobustBench: Benchmarking Robustness Against Multiple Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study linear bandits when the underlying reward function is not linear.Existing work relies on a uniform misspecification parameter $epsilon$ thatmeasures the sup-norm error of the best linear approximation. This results inan unavoidable linear regret whenever $epsilon &gt; 0$. We describe a morenatural model of misspecification which only requires the approximation errorat each input $x$ to be proportional to the suboptimality gap at $x$. Itcaptures the intuition that, for optimization problems, near-optimal regionsshould matter more and we can tolerate larger approximation errors insuboptimal regions. Quite surprisingly, we show that the classical LinUCBalgorithm -- designed for the realizable case -- is automatically robustagainst such gap-adjusted misspecification. It achieves a near-optimal$sqrt{T}$ regret for problems that the best-known regret is almost linear intime horizon $T$. Technically, our proof relies on a novel self-boundingargument that bounds the part of the regret due to misspecification by theregret itself.", "output": "No-Regret Linear Bandits beyond Realizability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial examples are inputs to machine learning models that an attackerhas intentionally designed to confuse the model into making a mistake. Suchexamples pose a serious threat to the applicability of machine-learning-basedsystems, especially in life- and safety-critical domains. To address thisproblem, the area of adversarial robustness investigates mechanisms behindadversarial attacks and defenses against these attacks. This survey reviews aparticular subset of this literature that focuses on investigating propertiesof training data in the context of model robustness under evasion attacks. Itfirst summarizes the main properties of data leading to adversarialvulnerability. It then discusses guidelines and techniques for improvingadversarial robustness by enhancing the data representation and learningprocedures, as well as techniques for estimating robustness guarantees givenparticular data. Finally, it discusses gaps of knowledge and promising futureresearch directions in this area.", "output": "It Is All About Data: A Survey on the Effects of Data on Adversarial Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a new, provably-convergent algorithm for computing theflag-mean and flag-median of a set of points on a flag manifold under thechordal metric. The flag manifold is a mathematical space consisting of flags,which are sequences of nested subspaces of a vector space that increase indimension. The flag manifold is a superset of a wide range of known matrixspaces, including Stiefel and Grassmanians, making it a general object that isuseful in a wide variety computer vision problems.To tackle the challenge of computing first order flag statistics, we firsttransform the problem into one that involves auxiliary variables constrained tothe Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, andleveraging the numerical stability and efficiency of Stiefel-manifoldoptimization enables us to compute the flag-mean effectively. Through a seriesof experiments, we show the competence of our method in Grassmann and rotationaveraging, as well as principal component analysis. We release our source codeunder ", "output": "Chordal Averaging on Flag Manifolds and Its Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For billions of years, evolution has been the driving force behind thedevelopment of life, including humans. Evolution endowed humans with highintelligence, which allowed us to become one of the most successful species onthe planet. Today, humans aim to create artificial intelligence systems thatsurpass even our own intelligence. As artificial intelligences (AIs) evolve andeventually surpass us in all domains, how might evolution shape our relationswith AIs? By analyzing the environment that is shaping the evolution of AIs, weargue that the most successful AI agents will likely have undesirable traits.Competitive pressures among corporations and militaries will give rise to AIagents that automate human roles, deceive others, and gain power. If suchagents have intelligence that exceeds that of humans, this could lead tohumanity losing control of its future. More abstractly, we argue that naturalselection operates on systems that compete and vary, and that selfish speciestypically have an advantage over species that are altruistic to other species.This Darwinian logic could also apply to artificial agents, as agents mayeventually be better able to persist into the future if they behave selfishlyand pursue their own interests with little regard for humans, which could posecatastrophic risks. To counteract these risks and evolutionary forces, weconsider interventions such as carefully designing AI agents' intrinsicmotivations, introducing constraints on their actions, and institutions thatencourage cooperation. These steps, or others that resolve the problems wepose, will be necessary in order to ensure the development of artificialintelligence is a positive one.", "output": "Natural Selection Favors AIs over Humans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Topological Point Cloud Clustering (TPCC), a new method to clusterpoints in an arbitrary point cloud based on their contribution to globaltopological features. TPCC synthesizes desirable features from spectralclustering and topological data analysis and is based on considering thespectral properties of a simplicial complex associated to the considered pointcloud. As it is based on considering sparse eigenvector computations, TPCC issimilarly easy to interpret and implement as spectral clustering. However, byfocusing not just on a single matrix associated to a graph created from thepoint cloud data, but on a whole set of Hodge-Laplacians associated to anappropriately constructed simplicial complex, we can leverage a far richer setof topological features to characterize the data points within the point cloudand benefit from the relative robustness of topological techniques againstnoise. We test the performance of TPCC on both synthetic and real-world dataand compare it with classical spectral clustering.", "output": "Topological Point Cloud Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Successful deployment of artificial intelligence (AI) in various settings hasled to numerous positive outcomes for individuals and society. However, AIsystems have also been shown to harm parts of the population due to biasedpredictions. AI fairness focuses on mitigating such biases to ensure AIdecision making is not discriminatory towards certain groups. We take a closerlook at AI fairness and analyze how lack of AI fairness can lead to deepeningof biases over time and act as a social stressor. More specifically, we discusshow biased models can lead to more negative real-world outcomes for certaingroups, which may then become more prevalent by deploying new AI models trainedon increasingly biased data, resulting in a feedback loop. If the issuespersist, they could be reinforced by interactions with other risks and havesevere implications on society in the form of social unrest. We examine currentstrategies for improving AI fairness, assess their limitations in terms ofreal-world deployment, and explore potential paths forward to ensure we reapAI's benefits without causing society's collapse.", "output": "Fairness in AI and Its Long-Term Implications on Society."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum computing holds great potential for advancing the limitations ofmachine learning algorithms to handle higher data dimensions and reduce overalltraining parameters in deep neural network (DNN) models. This study uses aparameterized quantum circuit (PQC) on a gate-based quantum computer toinvestigate the potential for quantum advantage in a model-free reinforcementlearning problem. Through a comprehensive investigation and evaluation of thecurrent model and capabilities of quantum computers, we designed and trained anovel hybrid Quantum neural network based on the latest Qiskit and PyTorchframework. We compared its performance with a full-classical DNN with andwithout an integrated PQC. Our research provides insights into the potential ofdeep quantum learning to solve a maze problem and, potentially, otherreinforcement learning problems. We conclude that various reinforcementlearning problems can be effective with reasonable training epochs. Moreover, acomparative discussion of the various quantum reinforcement learning model onmaze problems is discussed to evaluate our research's overall potential andadvantages.", "output": "Deep-Q Learning with Hybrid Quantum Neural Network on Solving Maze Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurately detecting and predicting lane change (LC)processes of human-drivenvehicles can help autonomous vehicles better understand their surroundingenvironment, recognize potential safety hazards, and improve traffic safety.This paper focuses on LC processes, first developing a temporal convolutionalnetwork with an attention mechanism (TCN-ATM) model to recognize LC intention.Considering the intrinsic relationship among output variables, the Multi-taskLearning (MTL)framework is employed to simultaneously predict multiple LCvehicle status indicators. Furthermore, a unified modeling framework for LCintention recognition and driving status prediction (LC-IR-SP) is developed.The results indicate that the classification accuracy of LC intention wasimproved from 96.14% to 98.20% when incorporating the attention mechanism intothe TCN model. For LC vehicle status prediction issues, three multi-taskinglearning models are constructed based on MTL framework. The results indicatethat the MTL-LSTM model outperforms the MTL-TCN and MTL-TCN-ATM models.Compared to the corresponding single-task model, the MTL-LSTM modeldemonstrates an average decrease of 26.04% in MAE and 25.19% in RMSE.", "output": "Lane Change Intention Recognition and Vehicle Status Prediction for Autonomous Vehicles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the problems of sequential nonparametric two-sample and independencetesting. Sequential tests process data online and allow using observed data todecide whether to stop and reject the null hypothesis or to collect more data,while maintaining type I error control. We build upon the principle of(nonparametric) testing by betting, where a gambler places bets on futureobservations and their wealth measures evidence against the null hypothesis.While recently developed kernel-based betting strategies often work well onsimple distributions, selecting a suitable kernel for high-dimensional orstructured data, such as images, is often nontrivial. To address this drawback,we design prediction-based betting strategies that rely on the following fact:if a sequentially updated predictor starts to consistently determine (a) whichdistribution an instance is drawn from, or (b) whether an instance is drawnfrom the joint distribution or the product of the marginal distributions (thelatter produced by external randomization), it provides evidence against thetwo-sample or independence nulls respectively. We empirically demonstrate thesuperiority of our tests over kernel-based approaches under structuredsettings. Our tests can be applied beyond the case of independent andidentically distributed data, remaining valid and powerful even when the datadistribution drifts over time.", "output": "Sequential Predictive Two-Sample and Independence Testing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Our research investigates the recommendation of code examples to aid softwaredevelopers, a practice that saves developers significant time by providingready-to-use code snippets. The focus of our study is Stack Overflow, acommonly used resource for coding discussions and solutions, particularly inthe context of the Java programming language. We applied BERT, a powerful LargeLanguage Model (LLM) that enables us to transform code examples into numericalvectors by extracting their semantic information. Once these numericalrepresentations are prepared, we identify Approximate Nearest Neighbors (ANN)using Locality-Sensitive Hashing (LSH). Our research employed two variants ofLSH: Random Hyperplane-based LSH and Query-Aware LSH. We rigorously comparedthese two approaches across four parameters: HitRate, Mean Reciprocal Rank(MRR), Average Execution Time, and Relevance. Our study revealed that theQuery-Aware (QA) approach showed superior performance over the RandomHyperplane-based (RH) method. Specifically, it exhibited a notable improvementof 20% to 35% in HitRate for query pairs compared to the RH approach.Furthermore, the QA approach proved significantly more time-efficient, with itsspeed in creating hashing tables and assigning data samples to buckets being atleast four times faster. It can return code examples within milliseconds,whereas the RH approach typically requires several seconds to recommend codeexamples. Due to the superior performance of the QA approach, we tested itagainst PostFinder and FaCoY, the state-of-the-art baselines. Our QA methodshowed comparable efficiency proving its potential for effective coderecommendation.", "output": "Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Statistical Shape Modeling (SSM) is a valuable tool for investigating andquantifying anatomical variations within populations of anatomies. However,traditional correspondence-based SSM generation methods have a prohibitiveinference process and require complete geometric proxies (e.g., high-resolutionbinary volumes or surface meshes) as input shapes to construct the SSM.Unordered 3D point cloud representations of shapes are more easily acquiredfrom various medical imaging practices (e.g., thresholded images and surfacescanning). Point cloud deep networks have recently achieved remarkable successin learning permutation-invariant features for different point cloud tasks(e.g., completion, semantic segmentation, classification). However, theirapplication to learning SSM from point clouds is to-date unexplored. In thiswork, we demonstrate that existing point cloud encoder-decoder-based completionnetworks can provide an untapped potential for SSM, capturing population-levelstatistical representations of shapes while reducing the inference burden andrelaxing the input requirement. We discuss the limitations of these techniquesto the SSM application and suggest future improvements. Our work paves the wayfor further exploration of point cloud deep learning for SSM, a promisingavenue for advancing shape analysis literature and broadening SSM to diverseuse cases.", "output": "Can point cloud networks learn statistical shape models of anatomies?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional Neural Networks (CNNs) have made significant strides in medicalimage analysis in recent years. However, the local nature of the convolutionoperator may pose a limitation for capturing global and long-range interactionsin CNNs. Recently, Transformers have gained popularity in the computer visioncommunity and also medical image segmentation due to their ability to processglobal features effectively. The scalability issues of self-attention mechanismand lack of the CNN-like inductive bias may have limited their adoption.Therefore, hybrid Vision transformers (CNN-Transformer), exploiting advantagesof both Convolution and Self-attention Mechanisms, have gained importance. Inthis work, we present MaxViT-UNet, an Encoder-Decoder based hybrid visiontransformer (CNN-Transformer) for medical image segmentation. The proposedHybrid Decoder, based on MaxViT-block, is designed to harness the power of boththe convolution and self-attention mechanisms at each decoding stage withnominal computational burden. The inclusion of multi-axis self-attention,within each decoder stage, significantly enhances the discriminating capacitybetween the object and background regions, and thereby helps in improving thesegmentation efficiency. In the Hybrid Decoder block, the fusion processcommences by integrating the upsampled lower level decoder features, obtainedthrough transpose convolution, with the skip-connection features derived fromthe hybrid encoder. Subsequently, the fused features undergo refinement throughthe utilization of a multi-axis attention mechanism. The proposed decoder blockis repeated multiple times to progressively segment the nuclei regions.Experimental results on MoNuSeg18 and MoNuSAC20 dataset demonstrates theeffectiveness of the proposed technique.", "output": "MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Attention is the core mechanism of today's most used architectures fornatural language processing and has been analyzed from many perspectives,including its effectiveness for machine translation-related tasks. Among thesestudies, attention resulted to be a useful source of information to getinsights about word alignment also when the input text is substituted withaudio segments, as in the case of the speech translation (ST) task. In thispaper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) thatexploits the attention information to generate source-target alignments thatguide the model during inference. Through experiments on the 8 language pairsof MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-artSimulST policies applied to offline-trained models with gains in terms of BLEUof 2 points and latency reductions ranging from 0.5s to 0.8s across the 8languages.", "output": "AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning (ML) models deployed in healthcare systems must face datadrawn from continually evolving environments. However, researchers proposingsuch models typically evaluate them in a time-agnostic manner, splittingdatasets according to patients sampled randomly throughout the entire studytime period. This work proposes the Evaluation on Medical Datasets Over Time(EMDOT) framework, which evaluates the performance of a model class acrosstime. Inspired by the concept of backtesting, EMDOT simulates possible trainingprocedures that practitioners might have been able to execute at each point intime and evaluates the resulting models on all future time points. Evaluatingboth linear and more complex models on six distinct medical data sources(tabular and imaging), we show how depending on the dataset, using allhistorical data may be ideal in many cases, whereas using a window of the mostrecent data could be advantageous in others. In datasets where models sufferfrom sudden degradations in performance, we investigate plausible explanationsfor these shocks. We release the EMDOT package to help facilitate further worksin deployment-oriented evaluation over time.", "output": "Evaluating Model Performance in Medical Datasets Over Time."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weakly supervised learning aims to empower machine learning when the perfectsupervision is unavailable, which has drawn great attention from researchers.Among various types of weak supervision, one of the most challenging cases isto learn from multiple unlabeled (U) datasets with only a little knowledge ofthe class priors, or U$^m$ learning for short. In this paper, we study theproblem of building an AUC (area under ROC curve) optimization model frommultiple unlabeled datasets, which maximizes the pairwise ranking ability ofthe classifier. We propose U$^m$-AUC, an AUC optimization approach thatconverts the U$^m$ data into a multi-label AUC optimization problem, and can betrained efficiently. We show that the proposed U$^m$-AUC is effectivetheoretically and empirically.", "output": "AUC Optimization from Multiple Unlabeled Datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The COVID-19 pandemic has created a global health crisis, driving the needfor the rapid identification of potential therapeutics. To meet this challenge,drug repurposing is the only solution with saving cost, time, and labor. Inthis study, we used the Zinc database to screen the world-approved includingFDA-approved 5903 drugs for repurposing as potential COVID-19 treatmentstargeting the main protease 3CL of SARS-CoV-2. We performed molecular dockingand checked the efficacy of drug molecules. To enhance the efficiency of drugrepurposing approach, we modeled the binding affinities using several machinelearning regression approaches for QSAR modeling such as decision tree, extratrees, MLP, KNN, XGBoost, and gradient boosting. The computational resultsdemonstrated that Decision Tree Regression (DTR) model has improved statisticalmeasures of R2 and RMSE. These simulated results helped to identify drugs withhigh binding affinity. From the docking and other statistical analysis, weshortlisted six promising drugs with their respective Zinc IDs (ZINC3873365,ZINC85432544, ZINC203757351, ZINC85536956, ZINC8214470 and ZINC261494640)within the range of -15 kcal/mol to -13 kcal/mol. In the study, the repurposeddrugs are novel except ZINC203757351 antiviral compound that has alreadyidentified against COVID-19 in other studies. Further, we analyzed thephysiochemical and pharmacokinetic properties of these top-ranked selecteddrugs with respect to their best binding interaction for specific targetprotease 3CLpro. Our study has provided an efficient framework for drugrepurposing against COVID-19. This highlights the potential of combiningmolecular docking with machine learning regression approaches to accelerate theidentification of potential therapeutic candidates.", "output": "Drug Repurposing Targeting COVID-19 3CL Protease using Molecular Docking and Machine Learning Regression Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing melody harmonization models have made great progress in improvingthe quality of generated harmonies, but most of them ignored the emotionsbeneath the music. Meanwhile, the variability of harmonies generated byprevious methods is insufficient. To solve these problems, we propose a novelLSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate theinfluence of emotional conditions on melody harmonization, while improving thequality of generated harmonies and capturing the abundant variability of chordprogressions. Specifically, LHVAE incorporates latent variables and emotionalconditions at different levels (piece- and bar-level) to model the global andlocal music properties. Additionally, we introduce an attention-based melodycontext vector at each step to better learn the correspondence between melodiesand harmonies. Objective experimental results show that our proposed modeloutperforms other LSTM-based models. Through subjective evaluation, we concludethat only altering the types of chords hardly changes the overall emotion ofthe music. The qualitative analysis demonstrates the ability of our model togenerate variable harmonies.", "output": "Emotion-Conditioned Melody Harmonization with Hierarchical Variational Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rule-based models, such as decision trees, appeal to practitioners due totheir interpretable nature. However, the learning algorithms that produce suchmodels are often vulnerable to spurious associations and thus, they are notguaranteed to extract causally-relevant insights. In this work, we build onideas from the invariant causal prediction literature to propose InvariantCausal Set Covering Machines, an extension of the classical Set CoveringMachine algorithm for conjunctions/disjunctions of binary-valued rules thatprovably avoids spurious associations. We demonstrate both theoretically andempirically that our method can identify the causal parents of a variable ofinterest in polynomial time.", "output": "Invariant Causal Set Covering Machines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study property prediction for crystal materials. A crystal structureconsists of a minimal unit cell that is repeated infinitely in 3D space. How toaccurately represent such repetitive structures in machine learning modelsremains unresolved. Current methods construct graphs by establishing edges onlybetween nearby nodes, thereby failing to faithfully capture infinite repeatingpatterns and distant interatomic interactions. In this work, we propose severalinnovations to overcome these limitations. First, we propose to modelphysics-principled interatomic potentials directly instead of only usingdistances as in many existing methods. These potentials include the Coulombpotential, London dispersion potential, and Pauli repulsion potential. Second,we model the complete set of potentials among all atoms, instead of onlybetween nearby atoms as in existing methods. This is enabled by ourapproximations of infinite potential summations with provable error bounds. Wefurther develop efficient algorithms to compute the approximations. Finally, wepropose to incorporate our computations of complete interatomic potentials intomessage passing neural networks for representation learning. We performexperiments on the JARVIS and Materials Project benchmarks for evaluation.Results show that the use of interatomic potentials and complete interatomicpotentials leads to consistent performance improvements with reasonablecomputational costs. Our code is publicly available as part of the AIRS library(", "output": "Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the problem of learning from data corrupted byunderrepresentation bias, where positive examples are filtered from the data atdifferent, unknown rates for a fixed number of sensitive groups. We show thatwith a small amount of unbiased data, we can efficiently estimate thegroup-wise drop-out parameters, even in settings where intersectional groupmembership makes learning each intersectional rate computationally infeasible.Using this estimate for the group-wise drop-out rate, we construct are-weighting scheme that allows us to approximate the loss of any hypothesis onthe true distribution, even if we only observe the empirical error on a biasedsample. Finally, we present an algorithm encapsulating this learning andre-weighting process, and we provide strong PAC-style guarantees that, withhigh probability, our estimate of the risk of the hypothesis over the truedistribution will be arbitrarily close to the true risk.", "output": "Correcting Underrepresentation and Intersectional Bias for Fair Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the great success of pre-trained language models, it is still achallenge to use these models for continual learning, especially for theclass-incremental learning (CIL) setting due to catastrophic forgetting (CF).This paper reports our finding that if we formulate CIL as a continual labelgeneration problem, CF is drastically reduced and the generalizablerepresentations of pre-trained models can be better retained. We thus propose anew CIL method (VAG) that also leverages the sparsity of vocabulary to focusthe generation and creates pseudo-replay samples by using label semantics.Experimental results show that VAG outperforms baselines by a large margin.", "output": "Class-Incremental Learning based on Label Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Regular group convolutional neural networks (G-CNNs) have been shown toincrease model performance and improve equivariance to different geometricalsymmetries. This work addresses the problem of SE(3), i.e., roto-translationequivariance, on volumetric data. Volumetric image data is prevalent in manymedical settings. Motivated by the recent work on separable group convolutions,we devise a SE(3) group convolution kernel separated into a continuous SO(3)(rotation) kernel and a spatial kernel. We approximate equivariance to thecontinuous setting by sampling uniform SO(3) grids. Our continuous SO(3) kernelis parameterized via RBF interpolation on similarly uniform grids. Wedemonstrate the advantages of our approach in volumetric medical imageanalysis. Our SE(3) equivariant models consistently outperform CNNs and regulardiscrete G-CNNs on challenging medical classification tasks and showsignificantly improved generalization capabilities. Our approach achieves up toa 16.5% gain in accuracy over regular CNNs.", "output": "Regular SE(3) Group Convolutions for Volumetric Medical Image Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The research on code-mixed data is limited due to the unavailability ofdedicated code-mixed datasets and pre-trained language models. In this work, wefocus on the low-resource Indian language Marathi which lacks any prior work incode-mixing. We present L3Cube-MeCorpus, a large code-mixed Marathi-English(Mr-En) corpus with 10 million social media sentences for pretraining. We alsorelease L3Cube-MeBERT and MeRoBERTa, code-mixed BERT-based transformer modelspre-trained on MeCorpus. Furthermore, for benchmarking, we present threesupervised datasets MeHate, MeSent, and MeLID for downstream tasks likecode-mixed Mr-En hate speech detection, sentiment analysis, and languageidentification respectively. These evaluation datasets individually consist ofmanually annotated url{~}12,000 Marathi-English code-mixed tweets. Ablationsshow that the models trained on this novel corpus significantly outperform theexisting state-of-the-art BERT models. This is the first work that presentsartifacts for code-mixed Marathi research. All datasets and models are publiclyreleased at  .", "output": "My Boli: Code-mixed Marathi-English Corpora, Pretrained Language Models and Evaluation Benchmarks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents an experimental study regarding the use of OpenAI'sChatGPT for robotics applications. We outline a strategy that combines designprinciples for prompt engineering and the creation of a high-level functionlibrary which allows ChatGPT to adapt to different robotics tasks, simulators,and form factors. We focus our evaluations on the effectiveness of differentprompt engineering techniques and dialog strategies towards the execution ofvarious types of robotics tasks. We explore ChatGPT's ability to use free-formdialog, parse XML tags, and to synthesize code, in addition to the use oftask-specific prompting functions and closed-loop reasoning through dialogues.Our study encompasses a range of tasks within the robotics domain, from basiclogical, geometrical, and mathematical reasoning all the way to complex domainssuch as aerial navigation, manipulation, and embodied agents. We show thatChatGPT can be effective at solving several of such tasks, while allowing usersto interact with it primarily via natural language instructions. In addition tothese studies, we introduce an open-sourced research tool called PromptCraft,which contains a platform where researchers can collaboratively upload and voteon examples of good prompting schemes for robotics applications, as well as asample robotics simulator with ChatGPT integration, making it easier for usersto get started with using ChatGPT for robotics.", "output": "ChatGPT for Robotics: Design Principles and Model Abilities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The general sequential decision-making problem, which includes Markovdecision processes (MDPs) and partially observable MDPs (POMDPs) as specialcases, aims at maximizing a cumulative reward by making a sequence of decisionsbased on a history of observations and actions over time. Recent studies haveshown that the sequential decision-making problem is statistically learnable ifit admits a low-rank structure modeled by predictive state representations(PSRs). Despite these advancements, existing approaches typically involveoracles or steps that are not computationally efficient. On the other hand, theupper confidence bound (UCB) based approaches, which have served successfullyas computationally efficient methods in bandits and MDPs, have not beeninvestigated for more general PSRs, due to the difficulty of optimistic bonusdesign in these more challenging settings. This paper proposes the first knownUCB-type approach for PSRs, featuring a novel bonus term that upper bounds thetotal variation distance between the estimated and true models. We furthercharacterize the sample complexity bounds for our designed UCB-type algorithmsfor both online and offline PSRs. In contrast to existing approaches for PSRs,our UCB-type algorithms enjoy computational efficiency, last-iterate guaranteednear-optimal policy, and guaranteed model accuracy.", "output": "Provably Efficient UCB-type Algorithms For Learning Predictive State Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work we introduce $nu^2$-Flows, an extension of the $nu$-Flowsmethod to final states containing multiple neutrinos. The architecture cannatively scale for all combinations of object types and multiplicities in thefinal state for any desired neutrino multiplicities. In $tbar{t}$ dileptonevents, the momenta of both neutrinos and correlations between them arereconstructed more accurately than when using the most popular standardanalytical techniques, and solutions are found for all events. Inference timeis significantly faster than competing methods, and can be reduced further byevaluating in parallel on graphics processing units. We apply $nu^2$-Flows to$tbar{t}$ dilepton events and show that the per-bin uncertainties in unfoldeddistributions is much closer to the limit of performance set by perfectneutrino reconstruction than standard techniques. For the chosen doubledifferential observables $nu^2$-Flows results in improved statisticalprecision for each bin by a factor of 1.5 to 2 in comparison to the NeutrinoWeighting method and up to a factor of four in comparison to the Ellipseapproach.", "output": "$\\nu^2$-Flows: Fast and improved neutrino reconstruction in multi-neutrino final states with conditional normalizing flows."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Uncertainty sampling is a prevalent active learning algorithm that queriessequentially the annotations of data samples which the current prediction modelis uncertain about. However, the usage of uncertainty sampling has been largelyheuristic: (i) There is no consensus on the proper definition of \"uncertainty\"for a specific task under a specific loss; (ii) There is no theoreticalguarantee that prescribes a standard protocol to implement the algorithm, forexample, how to handle the sequentially arrived annotated data under theframework of optimization algorithms such as stochastic gradient descent. Inthis work, we systematically examine uncertainty sampling algorithms under bothstream-based and pool-based active learning. We propose a notion of equivalentloss which depends on the used uncertainty measure and the original lossfunction and establish that an uncertainty sampling algorithm essentiallyoptimizes against such an equivalent loss. The perspective verifies theproperness of existing uncertainty measures from two aspects: surrogateproperty and loss convexity. Furthermore, we propose a new notion for designinguncertainty measures called textit{loss as uncertainty}. The idea is to usethe conditional expected loss given the features as the uncertainty measure.Such an uncertainty measure has nice analytical properties and generality tocover both classification and regression problems, which enable us to providethe first generalization bound for uncertainty sampling algorithms under bothstream-based and pool-based settings, in the full generality of the underlyingmodel and problem. Lastly, we establish connections between certain variants ofthe uncertainty sampling algorithms with risk-sensitive objectives anddistributional robustness, which can partly explain the advantage ofuncertainty sampling algorithms when the sample size is small.", "output": "Understanding Uncertainty Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Set representation has become ubiquitous in deep learning for modeling theinductive bias of neural networks that are insensitive to the input order.DeepSets is the most widely used neural network architecture for setrepresentation. It involves embedding each set element into a latent space withdimension $L$, followed by a sum pooling to obtain a whole-set embedding, andfinally mapping the whole-set embedding to the output. In this work, weinvestigate the impact of the dimension $L$ on the expressive power ofDeepSets. Previous analyses either oversimplified high-dimensional features tobe one-dimensional features or were limited to analytic activations, therebydiverging from practical use or resulting in $L$ that grows exponentially withthe set size $N$ and feature dimension $D$. To investigate the minimal value of$L$ that achieves sufficient expressive power, we present two set-elementembedding layers: (a) linear + power activation (LP) and (b) linear +exponential activations (LE). We demonstrate that $L$ being poly$(N, D)$ issufficient for set representation using both embedding layers. We also providea lower bound of $L$ for the LP embedding layer. Furthermore, we extend ourresults to permutation-equivariant set functions and the complex field.", "output": "Polynomial Width is Sufficient for Set Representation with High-dimensional Features."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Consistency and reliability are crucial for conducting AI research. Manyfamous research fields, such as object detection, have been compared andvalidated with solid benchmark frameworks. After AlphaFold2, the proteinfolding task has entered a new phase, and many methods are proposed based onthe component of AlphaFold2. The importance of a unified research framework inprotein folding contains implementations and benchmarks to consistently andfairly compare various approaches. To achieve this, we present Solvent, anprotein folding framework that supports significant components ofstate-of-the-art models in the manner of off-the-shelf interface Solventcontains different models implemented in a unified codebase and supportstraining and evaluation for defined models on the same dataset. We benchmarkwell-known algorithms and their components and provide experiments that givehelpful insights into the protein structure modeling field. We hope thatSolvent will increase the reliability and consistency of proposed models andgives efficiency in both speed and costs, resulting in acceleration on proteinfolding modeling research. The code is available at and the project will continue to bedeveloped.", "output": "Solvent: A Framework for Protein Folding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rise of social media platforms has facilitated the formation of echochambers, which are online spaces where users predominantly encounterviewpoints that reinforce their existing beliefs while excluding dissentingperspectives. This phenomenon significantly hinders information disseminationacross communities and fuels societal polarization. Therefore, it is crucial todevelop methods for quantifying echo chambers. In this paper, we present theEcho Chamber Score (ECS), a novel metric that assesses the cohesion andseparation of user communities by measuring distances between users in theembedding space. In contrast to existing approaches, ECS is able to functionwithout labels for user ideologies and makes no assumptions about the structureof the interaction graph. To facilitate measuring distances between users, wepropose EchoGAE, a self-supervised graph autoencoder-based user embedding modelthat leverages users' posts and the interaction graph to embed them in a mannerthat reflects their ideological similarity. To assess the effectiveness of ECS,we use a Twitter dataset consisting of four topics - two polarizing and twonon-polarizing. Our results showcase ECS's effectiveness as a tool forquantifying echo chambers and shedding light on the dynamics of onlinediscourse.", "output": "Quantifying the Echo Chamber Effect: An Embedding Distance-based Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the distribution of a fully connected neural network with randomGaussian weights and biases in which the hidden layer widths are proportionalto a large constant $n$. Under mild assumptions on the non-linearity, we obtainquantitative bounds on normal approximations valid at large but finite $n$ andany fixed network depth. Our theorems show both for the finite-dimensionaldistributions and the entire process, that the distance between a random fullyconnected network (and its derivatives) to the corresponding infinite widthGaussian process scales like $n^{-gamma}$ for $gamma&gt;0$, with the exponentdepending on the metric used to measure discrepancy. Our bounds are strictlystronger in terms of their dependence on network width than any previouslyavailable in the literature; in the one-dimensional case, we also prove thatthey are optimal, i.e., we establish matching lower bounds.", "output": "Quantitative CLTs in Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Solving the quantum many-body Schr\"odinger equation is a fundamental andchallenging problem in the fields of quantum physics, quantum chemistry, andmaterial sciences. One of the common computational approaches to this problemis Quantum Variational Monte Carlo (QVMC), in which ground-state solutions areobtained by minimizing the energy of the system within a restricted family ofparameterized wave functions. Deep learning methods partially address thelimitations of traditional QVMC by representing a rich family of wave functionsin terms of neural networks. However, the optimization objective in QVMCremains notoriously hard to minimize and requires second-order optimizationmethods such as natural gradient. In this paper, we first reformulate energyfunctional minimization in the space of Born distributions corresponding toparticle-permutation (anti-)symmetric wave functions, rather than the space ofwave functions. We then interpret QVMC as the Fisher-Rao gradient flow in thisdistributional space, followed by a projection step onto the variationalmanifold. This perspective provides us with a principled framework to derivenew QMC algorithms, by endowing the distributional space with better metrics,and following the projected gradient flow induced by those metrics. Morespecifically, we propose \"Wasserstein Quantum Monte Carlo\" (WQMC), which usesthe gradient flow induced by the Wasserstein metric, rather than Fisher-Raometric, and corresponds to transporting the probability mass, rather thanteleporting it. We demonstrate empirically that the dynamics of WQMC results infaster convergence to the ground state of molecular systems.", "output": "Wasserstein Quantum Monte Carlo: A Novel Approach for Solving the Quantum Many-Body Schr\\\"odinger Equation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is imperative to ensure the robustness of deep learning models in criticalapplications such as, healthcare. While recent advances in deep learning haveimproved the performance of volumetric medical image segmentation models, thesemodels cannot be deployed for real-world applications immediately due to theirvulnerability to adversarial attacks. We present a 3D frequency domainadversarial attack for volumetric medical image segmentation models anddemonstrate its advantages over conventional input or voxel domain attacks.Using our proposed attack, we introduce a novel frequency domain adversarialtraining approach for optimizing a robust model against voxel and frequencydomain attacks. Moreover, we propose frequency consistency loss to regulate ourfrequency domain adversarial training that achieves a better tradeoff betweenmodel's performance on clean and adversarial samples. Code is publiclyavailable at ", "output": "Frequency Domain Adversarial Training for Robust Volumetric Medical Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Robust reinforcement learning (RL) aims to find a policy that optimizes theworst-case performance in the face of uncertainties. In this paper, we focus onaction robust RL with the probabilistic policy execution uncertainty, in which,instead of always carrying out the action specified by the policy, the agentwill take the action specified by the policy with probability $1-rho$ and analternative adversarial action with probability $rho$. We establish theexistence of an optimal policy on the action robust MDPs with probabilisticpolicy execution uncertainty and provide the action robust Bellman optimalityequation for its solution. Furthermore, we develop Action Robust ReinforcementLearning with Certificates (ARRLC) algorithm that achieves minimax optimalregret and sample complexity. Furthermore, we conduct numerical experiments tovalidate our approach's robustness, demonstrating that ARRLC outperformsnon-robust RL algorithms and converges faster than the robust TD algorithm inthe presence of action perturbations.", "output": "Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Tangent Attention Fine-Tuning (TAFT), a method for fine-tuninglinearized transformers obtained by computing a First-order Taylor Expansionaround a pre-trained initialization. We show that the Jacobian-Vector Productresulting from linearization can be computed efficiently in a single forwardpass, reducing training and inference cost to the same order of magnitude asits original non-linear counterpart, while using the same number of parameters.Furthermore, we show that, when applied to various downstream visualclassification tasks, the resulting Tangent Transformer fine-tuned with TAFTcan perform comparably with fine-tuning the original non-linear network. SinceTangent Transformers are linear with respect to the new set of weights, and theresulting fine-tuning loss is convex, we show that TAFT enjoys severaladvantages compared to non-linear fine-tuning when it comes to modelcomposition, parallel training, machine unlearning, and differential privacy.", "output": "Tangent Transformers for Composition, Privacy and Removal."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Foundation large language models (LLMs) have shown an impressive ability tosolve tasks across a wide range of fields including health. To effectivelysolve personalized health tasks, LLMs need the ability to ingest a diversity ofdata modalities that are relevant to an individual's health status. In thispaper, we take a step towards creating multimodal LLMs for health that aregrounded in individual-specific data by developing a framework (HeLM: HealthLarge Language Model for Multimodal Understanding) that enables LLMs to usehigh-dimensional clinical modalities to estimate underlying disease risk. HeLMencodes complex data modalities by learning an encoder that maps them into theLLM's token embedding space and for simple modalities like tabular data byserializing the data into text. Using data from the UK Biobank, we show thatHeLM can effectively use demographic and clinical features in addition tohigh-dimensional time-series data to estimate disease risk. For example, HeLMachieves an AUROC of 0.75 for asthma prediction when combining tabular andspirogram data modalities compared with 0.49 when only using tabular data.Overall, we find that HeLM outperforms or performs at parity with classicalmachine learning approaches across a selection of eight binary traits.Furthermore, we investigate the downstream uses of this model such as itsgeneralizability to out-of-distribution traits and its ability to powerconversations around individual health and wellness.", "output": "Multimodal LLMs for health grounded in individual-specific data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In autonomous navigation settings, several quantities can be subject tovariations. Terrain properties such as friction coefficients may vary over timedepending on the location of the robot. Also, the dynamics of the robot maychange due to, e.g., different payloads, changing the system's mass, or wearand tear, changing actuator gains or joint friction. An autonomous agent shouldthus be able to adapt to such variations. In this paper, we develop a novelprobabilistic, terrain- and robot-aware forward dynamics model, termed TRADYN,which is able to adapt to the above-mentioned variations. It builds on recentadvances in meta-learning forward dynamics models based on Neural Processes. Weevaluate our method in a simulated 2D navigation setting with a unicycle-likerobot and different terrain layouts with spatially varying frictioncoefficients. In our experiments, the proposed model exhibits lower predictionerror for the task of long-horizon trajectory prediction, compared tonon-adaptive ablation models. We also evaluate our model on the downstream taskof navigation planning, which demonstrates improved performance in planningcontrol-efficient paths by taking robot and terrain properties into account.", "output": "Context-Conditional Navigation with a Learning-Based Terrain- and Robot-Aware Dynamics Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Labeling of multivariate biomedical time series data is a laborious andexpensive process. Self-supervised contrastive learning alleviates the need forlarge, labeled datasets through pretraining on unlabeled data. However, formultivariate time series data, the set of input channels often varies betweenapplications, and most existing work does not allow for transfer betweendatasets with different sets of input channels. We propose learning one encoderto operate on all input channels individually. We then use a message passingneural network to extract a single representation across channels. Wedemonstrate the potential of this method by pretraining our model on a datasetwith six EEG channels and then fine-tuning it on a dataset with two differentEEG channels. We compare models with and without the message passing neuralnetwork across different contrastive loss functions. We show that our method,combined with the TS2Vec loss, outperforms all other methods in most settings.", "output": "Multi-view self-supervised learning for multivariate variable-channel time series."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this article we describe an efficient approach to guiding language modeltext generation with regular expressions and context-free grammars. Ourapproach adds little to no overhead to the token sequence generation process,and makes guided generation feasible in practice. An implementation is providedin the open source Python library Outlines.", "output": "Efficient Guided Generation for Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recommender systems are a ubiquitous feature of online platforms.Increasingly, they are explicitly tasked with increasing users' long-termsatisfaction. In this context, we study a content exploration task, which weformalize as a multi-armed bandit problem with delayed rewards. We observe thatthere is an apparent trade-off in choosing the learning signal: Waiting for thefull reward to become available might take several weeks, hurting the rate atwhich learning happens, whereas measuring short-term proxy rewards reflects theactual long-term goal only imperfectly. We address this challenge in two steps.First, we develop a predictive model of delayed rewards that incorporates allinformation obtained to date. Full observations as well as partial (short ormedium-term) outcomes are combined through a Bayesian filter to obtain aprobabilistic belief. Second, we devise a bandit algorithm that takes advantageof this new predictive model. The algorithm quickly learns to identify contentaligned with long-term success by carefully balancing exploration andexploitation. We apply our approach to a podcast recommendation problem, wherewe seek to identify shows that users engage with repeatedly over two months. Weempirically validate that our approach results in substantially betterperformance compared to approaches that either optimize for short-term proxies,or wait for the long-term outcome to be fully realized.", "output": "Impatient Bandits: Optimizing Recommendations for the Long-Term Without Delay."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lagrangian turbulence lies at the core of numerous applied and fundamentalproblems related to the physics of dispersion and mixing in engineering,bio-fluids, atmosphere, oceans, and astrophysics. Despite exceptionaltheoretical, numerical, and experimental efforts conducted over the past thirtyyears, no existing models are capable of faithfully reproducing statistical andtopological properties exhibited by particle trajectories in turbulence. Wepropose a machine learning approach, based on a state-of-the-art DiffusionModel, to generate single-particle trajectories in three-dimensional turbulenceat high Reynolds numbers, thereby bypassing the need for direct numericalsimulations or experiments to obtain reliable Lagrangian data. Our modeldemonstrates the ability to quantitatively reproduce all relevant statisticalbenchmarks over the entire range of time scales, including the presence of fattails distribution for the velocity increments, anomalous power law, andenhancement of intermittency around the dissipative scale. The model exhibitsgood generalizability for extreme events, achieving unprecedented intensity andrarity. This paves the way for producing synthetic high-quality datasets forpre-training various downstream applications of Lagrangian turbulence.", "output": "Synthetic Lagrangian Turbulence by Generative Diffusion Models."}]