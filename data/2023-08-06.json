[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Computer-aided diagnosis (CAD), a vibrant medical imaging research field, isexpanding quickly. Because errors in medical diagnostic systems might lead toseriously misleading medical treatments, major efforts have been made in recentyears to improve computer-aided diagnostics applications. The use of machinelearning in computer-aided diagnosis is crucial. A simple equation may resultin a false indication of items like organs. Therefore, learning from examplesis a vital component of pattern recognition. Pattern recognition and machinelearning in the biomedical area promise to increase the precision of diseasedetection and diagnosis. They also support the decision-making process'sobjectivity. Machine learning provides a practical method for creating elegantand autonomous algorithms to analyze high-dimensional and multimodalbio-medical data. This review article examines machine-learning algorithms fordetecting diseases, including hepatitis, diabetes, liver disease, dengue fever,and heart disease. It draws attention to the collection of machine learningtechniques and algorithms employed in studying conditions and the ensuingdecision-making process.", "output": "Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT-like models have revolutionized various applications in artificialintelligence, from summarization and coding to translation, matching or evensurpassing human performance. However, the current landscape lacks anaccessible, efficient, and cost-effective end-to-end RLHF (ReinforcementLearning with Human Feedback) training pipeline for these powerful models,particularly when training at the scale of billions of parameters. This paperintroduces DeepSpeed-Chat, a novel system that democratizes RLHF training,making it accessible to the AI community. DeepSpeed-Chat offers three keycapabilities: an easy-to-use training and inference experience for ChatGPT-likemodels, a DeepSpeed-RLHF pipeline that replicates the training pipeline fromInstructGPT, and a robust DeepSpeed-RLHF system that combines variousoptimizations for training and inference in a unified way. The system deliversunparalleled efficiency and scalability, enabling training of models withhundreds of billions of parameters in record time and at a fraction of thecost. With this development, DeepSpeed-Chat paves the way for broader access toadvanced RLHF training, even for data scientists with limited resources,thereby fostering innovation and further development in the field of AI.", "output": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Embedding learning transforms discrete data entities into continuousnumerical representations, encoding features/properties of the entities.Despite the outstanding performance reported from different embedding learningalgorithms, few efforts were devoted to structurally interpreting how featuresare encoded in the learned embedding space. This work proposes EmbeddingTree, ahierarchical embedding exploration algorithm that relates the semantics ofentity features with the less-interpretable embedding vectors. An interactivevisualization tool is also developed based on EmbeddingTree to explorehigh-dimensional embeddings. The tool helps users discover nuance features ofdata entities, perform feature denoising/injecting in embedding training, andgenerate embeddings for unseen entities. We demonstrate the efficacy ofEmbeddingTree and our visualization tool through embeddings generated forindustry-scale merchant data and the public 30Music listening/playlistsdataset.", "output": "EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Over the past four decades, efforts have been made to develop and evaluatemodels for Empirical Translation Process Research (TPR), yet a comprehensiveframework remains elusive. This article traces the evolution of empirical TPRwithin the CRITT TPR-DB tradition and proposes the Free Energy Principle (FEP)and Active Inference (AIF) as a framework for modeling deeply embeddedtranslation processes. It introduces novel approaches for quantifyingfundamental concepts of Relevance Theory (relevance, s-mode, i-mode), andestablishes their relation to the Monitor Model, framing relevance maximizationas a special case of minimizing free energy. FEP/AIF provides a mathematicallyrigorous foundation that enables modeling of deep temporal architectures inwhich embedded translation processes unfold on different timelines. Thisframework opens up exciting prospects for future research in predictive TPR,likely to enrich our comprehension of human translation processes, and makingvaluable contributions to the wider realm of translation studies and the designof cognitive architectures.", "output": "Empirical Translation Process Research: Past and Possible Future Perspectives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Navigating automated driving systems (ADSs) through complex drivingenvironments is difficult. Predicting the driving behavior of surroundinghuman-driven vehicles (HDVs) is a critical component of an ADS. This paperproposes an enhanced motion-planning approach for an ADS in a highway-mergingscenario. The proposed enhanced approach utilizes the results of two aspects:the driving behavior and long-term trajectory of surrounding HDVs, which arecoupled using a hierarchical model that is used for the motion planning of anADS to improve driving safety.", "output": "An enhanced motion planning approach by integrating driving heterogeneity and long-term trajectory prediction for automated driving systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Causal probabilistic graph-based models have gained widespread utility,enabling the modeling of cause-and-effect relationships across diverse domains.With their rising adoption in new areas, such as automotive system safety andmachine learning, the need for an integrated lifecycle framework akin to DevOpsand MLOps has emerged. Currently, a process reference for organizationsinterested in employing causal engineering is missing. To address this gap andfoster widespread industrial adoption, we propose CausalOps, a novel lifecycleframework for causal model development and application. By defining keyentities, dependencies, and intermediate artifacts generated during causalengineering, we establish a consistent vocabulary and workflow model. This workcontextualizes causal model usage across different stages and stakeholders,outlining a holistic view of creating and maintaining them. CausalOps' aim isto drive the adoption of causal methods in practical applications withininterested organizations and the causality community.", "output": "CausalOps -- Towards an Industrial Lifecycle for Causal Probabilistic Graphical Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce OpenFlamingo, a family of autoregressive vision-language modelsranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to producean open-source replication of DeepMind's Flamingo models. On sevenvision-language datasets, OpenFlamingo models average between 80 - 89% ofcorresponding Flamingo performance. This technical report describes our models,training data, hyperparameters, and evaluation suite. We share our models andcode at ", "output": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To interact with humans in the world, agents need to understand the diversetypes of language that people use, relate them to the visual world, and actbased on them. While current agents learn to execute simple languageinstructions from task rewards, we aim to build agents that leverage diverselanguage that conveys general knowledge, describes the state of the world,provides interactive feedback, and more. Our key idea is that language helpsagents predict the future: what will be observed, how the world will behave,and which situations will be rewarded. This perspective unifies languageunderstanding with future prediction as a powerful self-supervised learningobjective. We present Dynalang, an agent that learns a multimodal world modelthat predicts future text and image representations and learns to act fromimagined model rollouts. Unlike traditional agents that use language only topredict actions, Dynalang acquires rich language understanding by using pastlanguage also to predict future language, video, and rewards. In addition tolearning from online interaction in an environment, Dynalang can be pretrainedon datasets of text, video, or both without actions or rewards. From usinglanguage hints in grid worlds to navigating photorealistic scans of homes,Dynalang utilizes diverse types of language to improve task performance,including environment descriptions, game rules, and instructions.", "output": "Learning to Model the World with Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer-based models have revolutionized the performance of a wide rangeof language tasks. Intuitively, one might expect text classification, whichdoes not necessitate as many high-level representations as generative tasks, tobe comprehensively addressed with the powerful representation capabilities ofTransformers. However, in reality, there remains significant potential forenhancement, particularly in the areas of multi-class and multi-labelclassification of lengthy textual documents and other large files. Theperformance of Transformer-based models is mainly hindered by a majorlimitation: a restricted input length, e.g., 512 tokens for BERT. While anincrease in GPU memory can marginally extend this limit, practical real-worldapplications often operate under constrained GPU resources. In this work, wetackle the input limit problem from the perspective of correlated multipleinstance learning. The proposed approach, LaFiCMIL, serves as a versatileframework applicable to various large file classification tasks coveringbinary, multi-class, and multi-label classification tasks, spanning variousdomains including Natural Language Processing, Programming Language Processing,and Android Analysis. To evaluate its effectiveness, we employ eight benchmarkdatasets pertaining to Long Document Classification, Code Defect Detection, andAndroid Malware Detection. Leveraging BERT-family models as feature extractors,our experimental results demonstrate that LaFiCMIL achieves newstate-of-the-art performance across all benchmark datasets. This is largelyattributable to its capability of scaling BERT up to nearly 20K tokens, runningon a single Tesla V-100 GPU with 32G of memory.", "output": "LaFiCMIL: Rethinking Large File Classification from the Perspective of Correlated Multiple Instance Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Renewable energy is important for achieving carbon neutrality goal. With thegreat success of Large Language Models (LLMs) like ChatGPT in automatic contentgeneration, LLMs are playing an increasingly important role. However, there hasnot been a specially designed LLM for renewable energy. Meanwhile, there hasnot been any dataset of renewable energy for training LLMs. Therefore, thispaper published the first open-source Renewable Energy Academic Paper (REAP)dataset for non-commercial LLM research of renewable energy. REAP dataset iscollected through searching the title and abstract of 1,168,970 academicliteratures from Web of Science. Based on REAP dataset, HouYi model, the firstLLM for renewable energy, is developed through finetuning general LLMs. HouYidemonstrated powerful academic paper paragraph generation ability in renewableenergy field. Experiments show that its ability to generate academic papers onrenewable energy is comparable to ChatGPT, slightly outperforms Claude, ERNIEBot and SparkDesk, and significantly outperforms open-source LLaMA-13B model.", "output": "HouYi: An open-source large language model specially designed for renewable energy and carbon neutrality field."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "At the beginning era of large language model, it is quite critical togenerate a high-quality financial dataset to fine-tune a large language modelfor financial related tasks. Thus, this paper presents a carefully designeddata creation pipeline for this purpose. Particularly, we initiate a dialoguebetween an AI investor and financial expert using ChatGPT and incorporate thefeedback of human financial experts, leading to the refinement of the dataset.This pipeline yielded a robust instruction tuning dataset comprised of 103kmulti-turn chats. Extensive experiments have been conducted on this dataset toevaluate the model's performance by adopting an external GPT-4 as the judge.The promising experimental results verify that our approach led to significantadvancements in generating accurate, relevant, and financial-style responsesfrom AI models, and thus providing a powerful tool for applications within thefinancial sector.", "output": "An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatMOF is an autonomous Artificial Intelligence (AI) system that is built topredict and generate of metal-organic frameworks (MOFs). By leveraging alarge-scale language model (gpt-3.5-turbo), ChatMOF extracts key details fromtextual inputs and delivers appropriate responses, thus eliminating thenecessity for rigid structured queries. The system is comprised of three corecomponents (i.e. an agent, a toolkit, and an evaluator) and it forms a robustpipeline that manages a variety of tasks, including data retrieval, propertyprediction, and structure generation. The study further explores the merits andconstraints of using large language models (LLMs) AI system in materialsciences using and showcases its transformative potential for futureadvancements.", "output": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cost-effective sensors are capable of real-time capturing a variety of airquality-related modalities from different pollutant concentrations toindoor/outdoor humidity and temperature. Machine learning (ML) models arecapable of performing air-quality \"ahead-of-time\" approximations. Undoubtedly,accurate indoor air quality approximation significantly helps provide a healthyindoor environment, optimize associated energy consumption, and offer humancomfort. However, it is crucial to design an ML architecture to capture thedomain knowledge, so-called problem physics. In this study, we propose sixnovel physics-based ML models for accurate indoor pollutant concentrationapproximations. The proposed models include an adroit combination ofstate-space concepts in physics, Gated Recurrent Units, and Decompositiontechniques. The proposed models were illustrated using data collected from fiveoffices in a commercial building in California. The proposed models are shownto be less complex, computationally more efficient, and more accurate thansimilar state-of-the-art transformer-based models. The superiority of theproposed models is due to their relatively light architecture (computationalefficiency) and, more importantly, their ability to capture the underlyinghighly nonlinear patterns embedded in the often contaminated sensor-collectedindoor air quality temporal data.", "output": "Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural networks (GNNs) have brought superb performance to variousapplications utilizing graph structural data, such as social analysis and frauddetection. The graph links, e.g., social relationships and transaction history,are sensitive and valuable information, which raises privacy concerns whenusing GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novelgraph poisoning attack that increases the effectiveness of graph link stealingby amplifying the link connectivity leakage. To infer node adjacency moreaccurately, we propose an attention mechanism that can be embedded into thelink detection network. Our experiments demonstrate that VertexSerumsignificantly outperforms the SOTA link inference attack, improving the AUCscores by an average of $9.8%$ across four real-world datasets and threedifferent GNN structures. Furthermore, our experiments reveal the effectivenessof VertexSerum in both black-box and online learning settings, furthervalidating its applicability in real-world scenarios.", "output": "VertexSerum: Poisoning Graph Neural Networks for Link Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A self-driving vehicle (SDV) must be able to perceive its surroundings andpredict the future behavior of other traffic participants. Existing workseither perform object detection followed by trajectory forecasting of thedetected objects, or predict dense occupancy and flow grids for the wholescene. The former poses a safety concern as the number of detections needs tobe kept low for efficiency reasons, sacrificing object recall. The latter iscomputationally expensive due to the high-dimensionality of the output grid,and suffers from the limited receptive field inherent to fully convolutionalnetworks. Furthermore, both approaches employ many computational resourcespredicting areas or objects that might never be queried by the motion planner.This motivates our unified approach to perception and future prediction thatimplicitly represents occupancy and flow over time with a single neuralnetwork. Our method avoids unnecessary computation, as it can be directlyqueried by the motion planner at continuous spatio-temporal locations.Moreover, we design an architecture that overcomes the limited receptive fieldof previous explicit occupancy prediction methods by adding an efficient yeteffective global attention mechanism. Through extensive experiments in bothurban and highway settings, we demonstrate that our implicit model outperformsthe current state-of-the-art. For more information, visit the project website:", "output": "Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For Industry 4.0 Revolution, cooperative autonomous mobility systems arewidely used based on multi-agent reinforcement learning (MARL). However, theMARL-based algorithms suffer from huge parameter utilization and convergencedifficulties with many agents. To tackle these problems, a quantum MARL (QMARL)algorithm based on the concept of actor-critic network is proposed, which isbeneficial in terms of scalability, to deal with the limitations in the noisyintermediate-scale quantum (NISQ) era. Additionally, our QMARL is alsobeneficial in terms of efficient parameter utilization and fast convergence dueto quantum supremacy. Note that the reward in our QMARL is defined as taskprecision over computation time in multiple agents, thus, multi-agentcooperation can be realized. For further improvement, an additional techniquefor scalability is proposed, which is called projection value measure (PVM).Based on PVM, our proposed QMARL can achieve the highest reward, by reducingthe action dimension into a logarithmic-scale. Finally, we can conclude thatour proposed QMARL with PVM outperforms the other algorithms in terms ofefficient parameter utilization, fast convergence, and scalability.", "output": "Quantum Multi-Agent Reinforcement Learning for Autonomous Mobility Cooperation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Disordered many-body systems exhibit a wide range of emergent phenomenaacross different scales. These complex behaviors can be utilized for variousinformation processing tasks such as error correction, learning, andoptimization. Despite the empirical success of utilizing these systems forintelligent tasks, the underlying principles that govern their emergentintelligent behaviors remain largely unknown. In this thesis, we aim tocharacterize such emergent intelligence in disordered systems throughstatistical physics. We chart a roadmap for our efforts in this thesis based ontwo axes: learning mechanisms (long-term memory vs. working memory) andlearning dynamics (artificial vs. natural). Throughout our journey, we uncoverrelationships between learning mechanisms and physical dynamics that couldserve as guiding principles for designing intelligent systems. We hope that ourinvestigation into the emergent intelligence of seemingly disparate learningsystems can expand our current understanding of intelligence beyond neuralsystems and uncover a wider range of computational substrates suitable for AIapplications.", "output": "Non-equilibrium physics: from spin glasses to machine and neural learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We explore AI-powered upscaling as a design assistance tool in the context ofcreating 2D game levels. Deep neural networks are used to upscale artificiallydownscaled patches of levels from the puzzle platformer game Lode Runner. Thetrained networks are incorporated into a web-based editor, where the user cancreate and edit levels at three different levels of resolution: 4x4, 8x8, and16x16. An edit at any resolution instantly transfers to the other resolutions.As upscaling requires inventing features that might not be present at lowerresolutions, we train neural networks to reproduce these features. We introducea neural network architecture that is capable of not only learning upscalingbut also giving higher priority to less frequent tiles. To investigate thepotential of this tool and guide further development, we conduct a qualitativestudy with 3 designers to understand how they use it. Designers enjoyedco-designing with the tool, liked its underlying concept, and provided feedbackfor further improvement.", "output": "Lode Enhancer: Level Co-creation Through Scaling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have shown promising results in cross-modal generationtasks, including text-to-image and text-to-audio generation. However,generating music, as a special type of audio, presents unique challenges due tolimited availability of music data and sensitive issues related to copyrightand plagiarism. In this paper, to tackle these challenges, we first construct astate-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusionand AudioLDM architectures to the music domain. We achieve this by retrainingthe contrastive language-audio pretraining model (CLAP) and the Hifi-GANvocoder, as components of MusicLDM, on a collection of music data samples.Then, to address the limitations of training data and to avoid plagiarism, weleverage a beat tracking model and propose two different mixup strategies fordata augmentation: beat-synchronous audio mixup and beat-synchronous latentmixup, which recombine training audio directly or via a latent embeddingsspace, respectively. Such mixup strategies encourage the model to interpolatebetween musical training samples and generate new music within the convex hullof the training data, making the generated music more diverse while stillstaying faithful to the corresponding style. In addition to popular evaluationmetrics, we design several new evaluation metrics based on CLAP score todemonstrate that our proposed MusicLDM and beat-synchronous mixup strategiesimprove both the quality and novelty of generated music, as well as thecorrespondence between input text and generated music.", "output": "MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a Pre-Training Deep Reinforcement Learning(DRL) foravoidance navigation without map for mobile robots which map raw sensor data tocontrol variable and navigate in an unknown environment. The efficient offlinetraining strategy is proposed to speed up the inefficient random explorationsin early stage and we also collect a universal dataset including expertexperience for offline training, which is of some significance for othernavigation training work. The pre-training and prioritized expert experienceare proposed to reduce 80% training time and has been verified to improve the2 times reward of DRL. The advanced simulation gazebo with real physicalmodelling and dynamic equations reduce the gap between sim-to-real. We trainour model a corridor environment, and evaluate the model in differentenvironment getting the same effect. Compared to traditional method navigation,we can confirm the trained model can be directly applied into differentscenarios and have the ability to no collision navigate. It was demonstratedthat our DRL model have universal general capacity in different environment.", "output": "Avoidance Navigation Based on Offline Pre-Training Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This research paper delves into the integration of OpenAI's ChatGPT intoembodied agent systems, evaluating its influence on interactive decision-makingbenchmark. Drawing a parallel to the concept of people assuming roles accordingto their unique strengths, we introduce InterAct. In this approach, we feedChatGPT with varied prompts, assigning it a numerous roles like a checker and asorter, then integrating them with the original language model. Our researchshows a remarkable success rate of 98% in AlfWorld, which consists of 6different tasks in a simulated household environment, emphasizing thesignificance of proficient prompt engineering. The results highlight ChatGPT'scompetence in comprehending and performing intricate tasks effectively inreal-world settings, thus paving the way for further advancements in taskplanning.", "output": "InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Aiming at the prediction problem of transport capacity risk caused by themismatch between the carrying capacity of rail transit network and passengerflow demand, this paper proposes an explainable prediction method of railtransit network transport capacity risk based on linear Gaussian Bayesiannetwork. This method obtains the training data of the prediction model based onthe simulation model of the rail transit system with a three-layer structureincluding rail transit network, train flow and passenger flow. A Bayesiannetwork structure construction method based on the topology of the rail transitnetwork is proposed, and the MLE (Maximum Likelihood Estimation) method is usedto realize the parameter learning of the Bayesian network. Finally, theeffectiveness of the proposed method is verified by simulation examples.", "output": "A Global Transport Capacity Risk Prediction Method for Rail Transit Based on Gaussian Bayesian Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning priors on trajectory distributions can help accelerate robot motionplanning optimization. Given previously successful plans, learning trajectorygenerative models as priors for a new planning problem is highly desirable.Prior works propose several ways on utilizing this prior to bootstrapping themotion planning problem. Either sampling the prior for initializations or usingthe prior distribution in a maximum-a-posterior formulation for trajectoryoptimization. In this work, we propose learning diffusion models as priors. Wethen can sample directly from the posterior trajectory distribution conditionedon task goals, by leveraging the inverse denoising process of diffusion models.Furthermore, diffusion has been recently shown to effectively encode datamultimodality in high-dimensional settings, which is particularly well-suitedfor large trajectory dataset. To demonstrate our method efficacy, we compareour proposed method - Motion Planning Diffusion - against several baselines insimulated planar robot and 7-dof robot arm manipulator environments. To assessthe generalization capabilities of our method, we test it in environments withpreviously unseen obstacles. Our experiments show that diffusion models arestrong priors to encode high-dimensional trajectory distributions of robotmotions.", "output": "Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised representation learning approaches aim to learn discriminativefeature representations from unlabeled data, without the requirement ofannotating every sample. Enabling unsupervised representation learning isextremely crucial for time series data, due to its unique annotation bottleneckcaused by its complex characteristics and lack of visual cues compared withother data modalities. In recent years, unsupervised representation learningtechniques have advanced rapidly in various domains. However, there is a lackof systematic analysis of unsupervised representation learning approaches fortime series. To fill the gap, we conduct a comprehensive literature review ofexisting rapidly evolving unsupervised representation learning approaches fortime series. Moreover, we also develop a unified and standardized library,named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fastimplementations and unified evaluations on various models. With ULTS, weempirically evaluate state-of-the-art approaches, especially the rapidlyevolving contrastive learning methods, on 9 diverse real-world datasets. Wefurther discuss practical considerations as well as open research challenges onunsupervised representation learning for time series to facilitate futureresearch in this field.", "output": "Unsupervised Representation Learning for Time Series: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Twenty-seven years ago, E. Freuder highlighted that \"Constraint programmingrepresents one of the closest approaches computer science has yet made to theHoly Grail of programming: the user states the problem, the computer solvesit\". Nowadays, CP users have great modeling tools available (like Minizinc andCPMpy), allowing them to formulate the problem and then let a solver do therest of the job, getting closer to the stated goal. However, this stillrequires the CP user to know the formalism and respect it. Another significantchallenge lies in the expertise required to effectively model combinatorialproblems. All this limits the wider adoption of CP. In this position paper, weinvestigate a possible approach to leverage pre-trained Large Language Modelsto extract models from textual problem descriptions. More specifically, we takeinspiration from the Natural Language Processing for Optimization (NL4OPT)challenge and present early results with a decomposition-based promptingapproach to GPT Models.", "output": "Holy Grail 2.0: From Natural Language to Constraint Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "DOLCE, the first top-level (foundational) ontology to be axiomatized, hasremained stable for twenty years and today is broadly used in a variety ofdomains. DOLCE is inspired by cognitive and linguistic considerations and aimsto model a commonsense view of reality, like the one human beings exploit ineveryday life in areas as diverse as socio-technical systems, manufacturing,financial transactions and cultural heritage. DOLCE clearly lists theontological choices it is based upon, relies on philosophical principles, isrichly formalized, and is built according to well-established ontologicalmethodologies, e.g. OntoClean. Because of these features, it has inspired mostof the existing top-level ontologies and has been used to develop or improvestandards and public domain resources (e.g. CIDOC CRM, DBpedia and WordNet).Being a foundational ontology, DOLCE is not directly concerned with domainknowledge. Its purpose is to provide the general categories and relationsneeded to give a coherent view of reality, to integrate domain knowledge, andto mediate across domains. In these 20 years DOLCE has shown that appliedontologies can be stable and that interoperability across reference and domainontologies is a reality. This paper briefly introduces the ontology and showshow to use it on a few modeling cases.", "output": "DOLCE: A Descriptive Ontology for Linguistic and Cognitive Engineering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the advancement of DNNs into safety-critical applications, testingapproaches for such models have gained more attention. A current direction isthe search for and identification of systematic weaknesses that put safetyassumptions based on average performance values at risk. Such weaknesses cantake on the form of (semantically coherent) subsets or areas in the input spacewhere a DNN performs systematically worse than its expected average. However,it is non-trivial to attribute the reason for such observed low performances tothe specific semantic features that describe the subset. For instance,inhomogeneities within the data w.r.t. other (non-considered) attributes mightdistort results. However, taking into account all (available) attributes andtheir interaction is often computationally highly expensive. Inspired bycounterfactual explanations, we propose an effective and computationally cheapalgorithm to validate the semantic attribution of existing subsets, i.e., tocheck whether the identified attribute is likely to have caused the degradedperformance. We demonstrate this approach on an example from the autonomousdriving domain using highly annotated simulated data, where we show for asemantic segmentation model that (i) performance differences among thedifferent pedestrian assets exist, but (ii) only in some cases is the assettype itself the reason for this reduction in the performance.", "output": "Assessing Systematic Weaknesses of DNNs using Counterfactuals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, dominant Multi-object tracking (MOT) and segmentation (MOTS)methods mainly follow the tracking-by-detection paradigm. Transformer-basedend-to-end (E2E) solutions bring some ideas to MOT and MOTS, but they cannotachieve a new state-of-the-art (SOTA) performance in major MOT and MOTSbenchmarks. Detection and association are two main modules of thetracking-by-detection paradigm. Association techniques mainly depend on thecombination of motion and appearance information. As deep learning has beenrecently developed, the performance of the detection and appearance model israpidly improved. These trends made us consider whether we can achieve SOTAbased on only high-performance detection and appearance model. Our paper mainlyfocuses on exploring this direction based on CBNetV2 with Swin-B as a detectionmodel and MoCo-v2 as a self-supervised appearance model. Motion information andIoU mapping were removed during the association. Our method wins 1st place onthe MOTS track and wins 2nd on the MOT track in the CVPR2023 WAD workshop. Wehope our simple and effective method can give some insights to the MOT and MOTSresearch community. Source code will be released under this git repository", "output": "ReIDTrack: Multi-Object Track and Segmentation Without Motion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An attractive book cover is important for the success of a book. In thispaper, we apply Generative Adversarial Networks (GANs) to the book coversdomain, using different methods for training in order to obtain bettergenerated images. We interleave GANs with knowledge graphs to alter the inputtitle to obtain multiple possible options for any given title, which are thenused as an augmented input to the generator. Finally, we use the discriminatorobtained during the training phase to select the best images generated with newtitles. Our method performed better at generating book covers than previousattempts, and the knowledge graph gives better options to the book author oreditor compared to using GANs alone.", "output": "Interleaving GANs with knowledge graphs to support design creativity for book covers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Wind resistance control is an essential feature for quadcopters to maintaintheir position to avoid deviation from target position and prevent collisionswith obstacles. Conventionally, cascaded PID controller is used for the controlof quadcopters for its simplicity and ease of tuning its parameters. However,it is weak against wind disturbances and the quadcopter can easily deviate fromtarget position. In this work, we propose a residual reinforcement learningbased approach to build a wind resistance controller of a quadcopter. Bylearning only the residual that compensates the disturbance, we can continueusing the cascaded PID controller as the base controller of the quadcopter butimprove its performance against wind disturbances. To avoid unexpected crashesand destructions of quadcopters, our method does not require real hardware fordata collection and training. The controller is trained only on a simulator anddirectly applied to the target hardware without extra finetuning process. Wedemonstrate the effectiveness of our approach through various experimentsincluding an experiment in an outdoor scene with wind speed greater than 13m/s. Despite its simplicity, our controller reduces the position deviation byapproximately 50% compared to the quadcopter controlled with the conventionalcascaded PID controller. Furthermore, trained controller is robust andpreserves its performance even though the quadcopter's mass and propeller'slift coefficient is changed between 50% to 150% from original training time.", "output": "Improving Wind Resistance Performance of Cascaded PID Controlled Quadcopters using Residual Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Maintaining a balance between the supply and demand of products by optimizingreplenishment decisions is one of the most important challenges in the supplychain industry. This paper presents a novel reinforcement learning frameworkcalled MARLIM, to address the inventory management problem for a single-echelonmulti-products supply chain with stochastic demands and lead-times. Within thiscontext, controllers are developed through single or multiple agents in acooperative setting. Numerical experiments on real data demonstrate thebenefits of reinforcement learning methods over traditional baselines.", "output": "MARLIM: Multi-Agent Reinforcement Learning for Inventory Management."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bias in textual data can lead to skewed interpretations and outcomes when thedata is used. These biases could perpetuate stereotypes, discrimination, orother forms of unfair treatment. An algorithm trained on biased data ends upmaking decisions that disproportionately impact a certain group of people.Therefore, it is crucial to detect and remove these biases to ensure the fairand ethical use of data. To this end, we develop a comprehensive and robustframework textsc{Nbias} that consists of a data layer, corpus contruction,model development layer and an evaluation layer. The dataset is constructed bycollecting diverse data from various fields, including social media,healthcare, and job hiring portals. As such, we applied a transformer-basedtoken classification model that is able to identify bias words/ phrases througha unique named entity. In the assessment procedure, we incorporate a blend ofquantitative and qualitative evaluations to gauge the effectiveness of ourmodels. We achieve accuracy improvements ranging from 1% to 8% compared tobaselines. We are also able to generate a robust understanding of the modelfunctioning, capturing not only numerical data but also the quality andintricacies of its performance. The proposed approach is applicable to avariety of biases and contributes to the fair and ethical use of textual data.", "output": "NBIAS: A Natural Language Processing Framework for Bias Identification in Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Machine Learning (GML) has numerous applications, such as node/graphclassification and link prediction, in real-world domains. Providinghuman-understandable explanations for GML models is a challenging yetfundamental task to foster their adoption, but validating explanations for linkprediction models has received little attention. In this paper, we providequantitative metrics to assess the quality of link prediction explanations,with or without ground-truth. State-of-the-art explainability methods for GraphNeural Networks are evaluated using these metrics. We discuss how underlyingassumptions and technical details specific to the link prediction task, such asthe choice of distance between node embeddings, can influence the quality ofthe explanations.", "output": "Evaluating Link Prediction Explanations for Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D panoptic segmentation is a challenging perception task that requires bothsemantic segmentation and instance segmentation. In this task, we notice thatimages could provide rich texture, color, and discriminative information, whichcan complement LiDAR data for evident performance improvement, but their fusionremains a challenging problem. To this end, we propose LCPS, the firstLiDAR-Camera Panoptic Segmentation network. In our approach, we conductLiDAR-Camera fusion in three stages: 1) an Asynchronous Compensation PixelAlignment (ACPA) module that calibrates the coordinate misalignment caused byasynchronous problems between sensors; 2) a Semantic-Aware Region Alignment(SARA) module that extends the one-to-one point-pixel mapping to one-to-manysemantic relations; 3) a Point-to-Voxel feature Propagation (PVP) module thatintegrates both geometric and semantic fusion information for the entire pointcloud. Our fusion strategy improves about 6.9% PQ performance over theLiDAR-only baseline on NuScenes dataset. Extensive quantitative and qualitativeexperiments further demonstrate the effectiveness of our novel framework. Thecode will be released at ", "output": "LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Feature selection could be defined as an optimization problem and solved bybio-inspired algorithms. Bees Algorithm (BA) shows decent performance infeature selection optimization tasks. On the other hand, Local PhaseQuantization (LPQ) is a frequency domain feature which has excellentperformance on Depth images. Here, after extracting LPQ features out of RGB(colour) and Depth images from the Iranian Kinect Face Database (IKFDB), theBees feature selection algorithm applies to select the desired number offeatures for final classification tasks. IKFDB is recorded with Kinect sensorV.2 and contains colour and depth images for facial and facialmicro-expressions recognition purposes. Here five facial expressions of Anger,Joy, Surprise, Disgust and Fear are used for final validation. The proposedBees LPQ method is compared with Particle Swarm Optimization (PSO) LPQ, PCALPQ, Lasso LPQ, and just LPQ features for classification tasks with SupportVector Machines (SVM), K-Nearest Neighbourhood (KNN), Shallow Neural Networkand Ensemble Subspace KNN. Returned results, show a decent performance of theproposed algorithm (99 % accuracy) in comparison with others.", "output": "Bees Local Phase Quantization Feature Selection for RGB-D Facial Expressions Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces an approach that combines the language reasoningcapabilities of large language models (LLMs) with the benefits of localtraining to tackle complex, domain-specific tasks. Specifically, the authorsdemonstrate their approach by extracting structured condition codes frompathology reports. The proposed approach utilizes local LLMs, which can befine-tuned to respond to specific generative instructions and providestructured outputs. The authors collected a dataset of over 150k uncuratedsurgical pathology reports, containing gross descriptions, final diagnoses, andcondition codes. They trained different model architectures, including LLaMA,BERT and LongFormer and evaluated their performance. The results show that theLLaMA-based models significantly outperform BERT-style models across allevaluated metrics, even with extremely reduced precision. The LLaMA modelsperformed especially well with large datasets, demonstrating their ability tohandle complex, multi-label tasks. Overall, this work presents an effectiveapproach for utilizing LLMs to perform domain-specific tasks using accessiblehardware, with potential applications in the medical domain, where complex dataextraction and classification are required.", "output": "Local Large Language Models for Complex Structured Medical Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a retrospective overview of a decade of research in ourdepartment towards self-organizing personal knowledge assistants in evolvingcorporate memories. Our research is typically inspired by real-world problemsand often conducted in interdisciplinary collaborations with research andindustry partners. We summarize past experiments and results comprising topicslike various ways of knowledge graph construction in corporate and personalsettings, Managed Forgetting and (Self-organizing) Context Spaces as a novelapproach to Personal Information Management (PIM) and knowledge work support.Past results are complemented by an overview of related work and some of ourlatest findings not published so far. Last, we give an overview of our relatedindustry use cases including a detailed look into CoMem, a Corporate Memorybased on our presented research already in productive use and providingchallenges for further research. Many contributions are only first steps in newdirections with still a lot of untapped potential, especially with regard tofurther increasing the automation in PIM and knowledge work support.", "output": "Towards Self-organizing Personal Knowledge Assistants in Evolving Corporate Memories."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the widespread application of personalized online services,click-through rate (CTR) prediction has received more and more attention andresearch. The most prominent features of CTR prediction are its multi-fieldcategorical data format, and vast and daily-growing data volume. The largecapacity of neural models helps digest such massive amounts of data under thesupervised learning paradigm, yet they fail to utilize the substantial data toits full potential, since the 1-bit click signal is not sufficient to guide themodel to learn capable representations of features and instances. Theself-supervised learning paradigm provides a more promising pretrain-finetunesolution to better exploit the large amount of user click logs, and learn moregeneralized and effective representations. However, self-supervised learningfor CTR prediction is still an open question, since current works on this lineare only preliminary and rudimentary. To this end, we propose a Model-agnosticpretraining (MAP) framework that applies feature corruption and recovery onmulti-field categorical data, and more specifically, we derive two practicalalgorithms: masked feature prediction (MFP) and replaced feature detection(RFD). MFP digs into feature interactions within each instance through maskingand predicting a small portion of input features, and introduces noisecontrastive estimation (NCE) to handle large feature spaces. RFD further turnsMFP into a binary classification mode through replacing and detecting changesin input features, making it even simpler and more effective for CTRpretraining. Our extensive experiments on two real-world large-scale datasets(i.e., Avazu, Criteo) demonstrate the advantages of these two methods onseveral strong backbones (e.g., DCNv2, DeepFM), and achieve newstate-of-the-art performance in terms of both effectiveness and efficiency forCTR prediction.", "output": "MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Job scheduling is a well-known Combinatorial Optimization problem withendless applications. Well planned schedules bring many benefits in the contextof automated systems: among others, they limit production costs and waste.Nevertheless, the NP-hardness of this problem makes it essential to useheuristics whose design is difficult, requires specialized knowledge and oftenproduces methods tailored to the specific task. This paper presents an originalend-to-end Deep Reinforcement Learning approach to scheduling thatautomatically learns dispatching rules. Our technique is inspired by naturallanguage encoder-decoder models for sequence processing and has never beenused, to the best of our knowledge, for scheduling purposes. We applied andtested our method in particular to some benchmark instances of Job ShopProblem, but this technique is general enough to be potentially used to tackleother different optimal job scheduling tasks with minimal intervention. Resultsdemonstrate that we outperform many classical approaches exploiting prioritydispatching rules and show competitive results on state-of-the-art DeepReinforcement Learning ones.", "output": "Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fine-grained image classification (FGIC) is a challenging task in computervision for due to small visual differences among inter-subcategories, but,large intra-class variations. Deep learning methods have achieved remarkablesuccess in solving FGIC. In this paper, we propose a fusion approach to addressFGIC by combining global texture with local patch-based information. The firstpipeline extracts deep features from various fixed-size non-overlapping patchesand encodes features by sequential modelling using the long short-term memory(LSTM). Another path computes image-level textures at multiple scales using thelocal binary patterns (LBP). The advantages of both streams are integrated torepresent an efficient feature vector for image classification. The method istested on eight datasets representing the human faces, skin lesions, fooddishes, marine lives, etc. using four standard backbone CNNs. Our method hasattained better classification accuracy over existing methods with notablemargins.", "output": "Deep Neural Networks Fused with Textures for Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial training (AT) is widely considered the state-of-the-art techniquefor improving the robustness of deep neural networks (DNNs) against adversarialexamples (AE). Nevertheless, recent studies have revealed that adversariallytrained models are prone to unfairness problems, restricting theirapplicability. In this paper, we empirically observe that this limitation maybe attributed to serious adversarial confidence overfitting, i.e., certainadversarial examples with overconfidence. To alleviate this problem, we proposeHAM, a straightforward yet effective framework via adaptive Hard Adversarialexample Mining.HAM concentrates on mining hard adversarial examples whilediscarding the easy ones in an adaptive fashion. Specifically, HAM identifieshard AEs in terms of their step sizes needed to cross the decision boundarywhen calculating loss value. Besides, an early-dropping mechanism isincorporated to discard the easy examples at the initial stages of AEgeneration, resulting in efficient AT. Extensive experimental results onCIFAR-10, SVHN, and Imagenette demonstrate that HAM achieves significantimprovement in robust fairness while reducing computational cost compared toseveral state-of-the-art adversarial training methods. The code will be madepublicly available.", "output": "Hard Adversarial Example Mining for Improving Robust Fairness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic representations in higher sensory cortices form the basis forrobust, yet flexible behavior. These representations are acquired over thecourse of development in an unsupervised fashion and continuously maintainedover an organism's lifespan. Predictive learning theories propose that theserepresentations emerge from predicting or reconstructing sensory inputs.However, brains are known to generate virtual experiences, such as duringimagination and dreaming, that go beyond previously experienced inputs. Here,we suggest that virtual experiences may be just as relevant as actual sensoryinputs in shaping cortical representations. In particular, we discuss twocomplementary learning principles that organize representations through thegeneration of virtual experiences. First, \"adversarial dreaming\" proposes thatcreative dreams support a cortical implementation of adversarial learning inwhich feedback and feedforward pathways engage in a productive game of tryingto fool each other. Second, \"contrastive dreaming\" proposes that the invarianceof neuronal representations to irrelevant factors of variation is acquired bytrying to map similar virtual experiences together via a contrastive learningprocess. These principles are compatible with known cortical structure anddynamics and the phenomenology of sleep thus providing promising directions toexplain cortical learning beyond the classical predictive learning paradigm.", "output": "Learning beyond sensations: how dreams organize neuronal representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The current work investigates the capability of Large language models (LLMs)that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2)to predict psychiatric functioning from patient interviews and clinicaldescriptions without being trained to do so. To assess this, n = 145 depressionand n =115 PTSD assessments and n = 46 clinical case studies across highprevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, traumaand stress, Addictive disorders) were analyzed using prompts to extractestimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 iscapable of assessing psychiatric functioning across a range of psychiatricconditions with the strongest performance being the prediction of depressionscores based on standardized assessments (Accuracy range= 0.80 - 0.84) whichwere statistically indistinguishable from human clinical raters t(1,144) =1.20; p = 0.23. Results show the potential for general clinical language modelsto flexibly predict psychiatric risk based on free descriptions of functioningfrom both patients and clinicians.", "output": "The Capability of Large Language Models to Measure Psychiatric Functioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning models are known to be vulnerable to adversarial evasionattacks as illustrated by image classification models. Thoroughly understandingsuch attacks is critical in order to ensure the safety and robustness ofcritical AI tasks. However, most evasion attacks are difficult to deployagainst a majority of AI systems because they have focused on image domain withonly few constraints. An image is composed of homogeneous, numerical,continuous, and independent features, unlike many other input types to AIsystems used in practice. Furthermore, some input types include additionalsemantic and functional constraints that must be observed to generate realisticadversarial inputs. In this work, we propose a new framework to enable thegeneration of adversarial inputs irrespective of the input type and taskdomain. Given an input and a set of pre-defined input transformations, ourframework discovers a sequence of transformations that result in a semanticallycorrect and functional adversarial input. We demonstrate the generality of ourapproach on several diverse machine learning tasks with various inputrepresentations. We also show the importance of generating adversarial examplesas they enable the deployment of mitigation techniques.", "output": "URET: Universal Robustness Evaluation Toolkit (for Evasion)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-motion generation has gained increasing attention, but most existingmethods are limited to generating short-term motions that correspond to asingle sentence describing a single action. However, when a text streamdescribes a sequence of continuous motions, the generated motions correspondingto each sentence may not be coherently linked. Existing long-term motiongeneration methods face two main issues. Firstly, they cannot directly generatecoherent motions and require additional operations such as interpolation toprocess the generated actions. Secondly, they generate subsequent actions in anautoregressive manner without considering the influence of future actions onprevious ones. To address these issues, we propose a novel approach thatutilizes a past-conditioned diffusion model with two optional coherent samplingmethods: Past Inpainting Sampling and Compositional Transition Sampling. PastInpainting Sampling completes subsequent motions by treating previous motionsas conditions, while Compositional Transition Sampling models the distributionof the transition as the composition of two adjacent motions guided bydifferent text prompts. Our experimental results demonstrate that our proposedmethod is capable of generating compositional and coherent long-term 3D humanmotions controlled by a user-instructed long text stream. The code is availableathref{", "output": "Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we make the first attempt to evaluate LLMs in a morechallenging code generation scenario, i.e. class-level code generation. Wefirst manually construct the first class-level code generation benchmarkClassEval of 100 class-level Python code generation tasks with approximately500 person-hours. Based on it, we then perform the first study of 11state-of-the-art LLMs on class-level code generation. Based on our results, wehave the following main findings. First, we find that all existing LLMs showmuch worse performance on class-level code generation compared to on standalonemethod-level code generation benchmarks like HumanEval; and the method-levelcoding ability cannot equivalently reflect the class-level coding ability amongLLMs. Second, we find that GPT-4 and GPT-3.5 still exhibit dominate superiorthan other LLMs on class-level code generation, and the second-tier modelsincludes Instruct-Starcoder, Instruct-Codegen, and Wizardcoder with verysimilar performance. Third, we find that generating the entire class all atonce (i.e. holistic generation strategy) is the best generation strategy onlyfor GPT-4 and GPT-3.5, while method-by-method generation (i.e. incremental andcompositional) is better strategies for the other models with limited abilityof understanding long instructions and utilizing the middle information.Lastly, we find the limited model ability of generating method-dependent codeand discuss the frequent error types in generated classes. Our benchmark isavailable at ", "output": "ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-adventure games and text role-playing games are grand challenges forreinforcement learning game playing agents. Text role-playing games areopen-ended environments where an agent must faithfully play a particularcharacter. We consider the distinction between characters and actors, where anactor agent has the ability to play multiple characters. We present a frameworkwe call a thespian agent that can learn to emulate multiple characters alongwith a soft prompt that can be used to direct it as to which character to playat any time. We further describe an attention mechanism that allows the agentto learn new characters that are based on previously learned characters in afew-shot fashion. We show that our agent outperforms the state of the art agentframework in multi-character learning and few-shot learning.", "output": "Thespian: Multi-Character Text Role-Playing Game Agents."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning seeks to enable deep learners to train on a series oftasks of unknown length without suffering from the catastrophic forgetting ofprevious tasks. One effective solution is replay, which involves storing fewprevious experiences in memory and replaying them when learning the currenttask. However, there is still room for improvement when it comes to selectingthe most informative samples for storage and determining the optimal number ofsamples to be stored. This study aims to address these issues with a novelcomparison of the commonly used reservoir sampling to various alternativepopulation strategies and providing a novel detailed analysis of how to findthe optimal number of stored samples.", "output": "Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Preprints play an increasingly critical role in academic communities. Thereare many reasons driving researchers to post their manuscripts to preprintservers before formal submission to journals or conferences, but the use ofpreprints has also sparked considerable controversy, especially surrounding theclaim of priority. In this paper, a case study of computer science preprintssubmitted to arXiv from 2008 to 2017 is conducted to quantify how manypreprints have eventually been printed in peer-reviewed venues. Among thosepublished manuscripts, some are published under different titles and without anupdate to their preprints on arXiv. In the case of these manuscripts, thetraditional fuzzy matching method is incapable of mapping the preprint to thefinal published version. In view of this issue, we introduce a semantics-basedmapping method with the employment of Bidirectional Encoder Representationsfrom Transformers (BERT). With this new mapping method and a plurality of datasources, we find that 66% of all sampled preprints are published underunchanged titles and 11% are published under different titles and with othermodifications. A further analysis was then performed to investigate why thesepreprints but not others were accepted for publication. Our comparison revealsthat in the field of computer science, published preprints feature adequaterevisions, multiple authorship, detailed abstract and introduction, extensiveand authoritative references and available source code.", "output": "How many preprints have actually been printed and why: a case study of computer science preprints on arXiv."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Depth completion, which aims to generate high-quality dense depth maps fromsparse depth maps, has attracted increasing attention in recent years. Previouswork usually employs RGB images as guidance, and introduces iterative spatialpropagation to refine estimated coarse depth maps. However, most of thepropagation refinement methods require several iterations and suffer from afixed receptive field, which may contain irrelevant and useless informationwith very sparse input. In this paper, we address these two challengessimultaneously by revisiting the idea of deformable convolution. We propose aneffective architecture that leverages deformable kernel convolution as asingle-pass refinement module, and empirically demonstrate its superiority. Tobetter understand the function of deformable convolution and exploit it fordepth completion, we further systematically investigate a variety ofrepresentative strategies. Our study reveals that, different from prior work,deformable convolution needs to be applied on an estimated depth map with arelatively high density for better performance. We evaluate our model on thelarge-scale KITTI dataset and achieve state-of-the-art level performance inboth accuracy and inference speed. Our code is available at", "output": "Revisiting Deformable Convolution for Depth Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have revolutionized NLP by solving downstreamtasks with little to no labeled data. Despite their versatile abilities, thelarger question of their ability to reason remains ill-understood. This paperaddresses reasoning in math word problems (MWPs) by studying symbolic versionsof the numeric problems, since a symbolic expression is a \"concise explanation\"of the numeric answer. We create and use a symbolic version of the SVAMPdataset and find that GPT-3's davinci-002 model also has good zero-shotaccuracy on symbolic MWPs. To evaluate the faithfulness of the model'sreasoning, we go beyond accuracy and additionally evaluate the alignmentbetween the final answer and the outputted reasoning, which correspond tonumeric and symbolic answers respectively for MWPs. We explore a self-promptingapproach to encourage the symbolic reasoning to align with the numeric answer,thus equipping the LLM with the ability to provide a concise and verifiablereasoning and making it more interpretable. Surprisingly, self-prompting alsoimproves the symbolic accuracy to be higher than both the numeric and symbolicaccuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will bereleased for future research on symbolic math problems.", "output": "Reasoning in Large Language Models Through Symbolic Math Word Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Albeit being a prevalent architecture searching approach, differentiablearchitecture search (DARTS) is largely hindered by its substantial memory costsince the entire supernet resides in the memory. This is where the single-pathDARTS comes in, which only chooses a single-path submodel at each step. Whilebeing memory-friendly, it also comes with low computational costs. Nonetheless,we discover a critical issue of single-path DARTS that has not been primarilynoticed. Namely, it also suffers from severe performance collapse since toomany parameter-free operations like skip connections are derived, just likeDARTS does. In this paper, we propose a new algorithm called RObustifyingMemory-Efficient NAS (ROME) to give a cure. First, we disentangle the topologysearch from the operation search to make searching and evaluation consistent.We then adopt Gumbel-Top2 reparameterization and gradient accumulation torobustify the unwieldy bi-level optimization. We verify ROME extensively across15 benchmarks to demonstrate its effectiveness and robustness.", "output": "ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-adaptive software systems continuously adapt in response to internal andexternal changes in their execution environment, captured as contexts. The COPparadigm posits a technique for the development of self-adaptive systems,capturing their main characteristics with specialized programming languageconstructs. COP adaptations are specified as independent modules composed inand out of the base system as contexts are activated and deactivated inresponse to sensed circumstances from the surrounding environment. However, thedefinition of adaptations, their contexts and associated specialized behavior,need to be specified at design time. In complex CPS this is intractable due tonew unpredicted operating conditions. We propose Auto-COP, a new technique toenable generation of adaptations at run time. Auto-COP uses RL options to buildaction sequences, based on the previous instances of the system execution.Options are explored in interaction with the environment, and the most suitableoptions for each context are used to generate adaptations exploiting COP. Tovalidate Auto-COP, we present two case studies exhibiting different systemcharacteristics and application domains: a driving assistant and a robotdelivery system. We present examples of Auto-COP code generated at run time, toillustrate the types of circumstances (contexts) requiring adaptation, and thecorresponding generated adaptations for each context. We confirm that thegenerated adaptations exhibit correct system behavior measured bydomain-specific performance metrics, while reducing the number of requiredexecution/actuation steps by a factor of two showing that the adaptations areregularly selected by the running system as adaptive behavior is moreappropriate than the execution of primitive actions.", "output": "Auto-COP: Adaptation Generation in Context-Oriented Programming using Reinforcement Learning Options."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A longstanding goal in reinforcement learning is to build intelligent agentsthat show fast learning and a flexible transfer of skills akin to humans andanimals. This paper investigates the integration of two frameworks for tacklingthose goals: episodic control and successor features. Episodic control is acognitively inspired approach relying on episodic memory, an instance-basedmemory model of an agent's experiences. Meanwhile, successor features andgeneralized policy improvement (SF&amp;GPI) is a meta and transfer learningframework allowing to learn policies for tasks that can be efficiently reusedfor later tasks which have a different reward function. Individually, these twotechniques have shown impressive results in vastly improving sample efficiencyand the elegant reuse of previously learned policies. Thus, we outline acombination of both approaches in a single reinforcement learning framework andempirically illustrate its benefits.", "output": "Successor Feature Neural Episodic Control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As one of the most pervasive applications of machine learning, recommendersystems are playing an important role on assisting human decision making. Thesatisfaction of users and the interests of platforms are closely related to thequality of the generated recommendation results. However, as a highlydata-driven system, recommender system could be affected by data or algorithmicbias and thus generate unfair results, which could weaken the reliance of thesystems. As a result, it is crucial to address the potential unfairnessproblems in recommendation settings. Recently, there has been growing attentionon fairness considerations in recommender systems with more and more literatureon approaches to promote fairness in recommendation. However, the studies arerather fragmented and lack a systematic organization, thus making it difficultto penetrate for new researchers to the domain. This motivates us to provide asystematic survey of existing works on fairness in recommendation. This surveyfocuses on the foundations for fairness in recommendation literature. It firstpresents a brief introduction about fairness in basic machine learning taskssuch as classification and ranking in order to provide a general overview offairness research, as well as introduce the more complex situations andchallenges that need to be considered when studying fairness in recommendersystems. After that, the survey will introduce fairness in recommendation witha focus on the taxonomies of current fairness definitions, the typicaltechniques for improving fairness, as well as the datasets for fairness studiesin recommendation. The survey also talks about the challenges and opportunitiesin fairness research with the hope of promoting the fair recommendationresearch area and beyond.", "output": "Fairness in Recommendation: Foundations, Methods and Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In peer review systems, reviewers are often asked to evaluate variousfeatures of submissions, such as technical quality or novelty. A score is givento each of the predefined features and based on these the reviewer has toprovide an overall quantitative recommendation. It may be assumed that eachreviewer has her own mapping from the set of features to a recommendation, andthat different reviewers have different mappings in mind. This introduces anelement of arbitrariness known as commensuration bias. In this paper we discussa framework, introduced by Noothigattu, Shah and Procaccia, and then applied bythe organizers of the AAAI 2022 conference. Noothigattu, Shah and Procacciaproposed to aggregate reviewer's mapping by minimizing certain loss functions,and studied axiomatic properties of this approach, in the sense of socialchoice theory. We challenge several of the results and assumptions used intheir work and report a number of negative results. On the one hand, we study atrade-off between some of the axioms proposed and the ability of the method toproperly capture agreements of the majority of reviewers. On the other hand, weshow that dropping a certain unrealistic assumption has dramatic effects,including causing the method to be discontinuous.", "output": "No Agreement Without Loss: Learning and Social Choice in Peer Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Clustering analysis of sequence data continues to address many applicationsin engineering design, aided with the rapid growth of machine learning inapplied science. This paper presents an unsupervised machine learning algorithmto extract defining characteristics of earthquake ground-motion spectra, alsocalled latent features, to aid in ground-motion selection (GMS). In thiscontext, a latent feature is a low-dimensional machine-discovered spectralcharacteristic learned through nonlinear relationships of a neural networkautoencoder. Machine discovered latent features can be combined withtraditionally defined intensity measures and clustering can be performed toselect a representative subgroup from a large ground-motion suite. Theobjective of efficient GMS is to choose characteristic records representativeof what the structure will probabilistically experience in its lifetime. Threeexamples are presented to validate this approach, including the use ofsynthetic and field recorded ground-motion datasets. The presented deepembedding clustering of ground-motion spectra has three main advantages: 1.defining characteristics the represent the sparse spectral content ofground-motions are discovered efficiently through training of the autoencoder,2. domain knowledge is incorporated into the machine learning framework withconditional variables in the deep embedding scheme, and 3. method exhibitsexcellent performance when compared to a benchmark seismic hazard analysis.", "output": "An Unsupervised Machine Learning Approach for Ground-Motion Spectra Clustering and Selection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale pre-training has shown promising results on thevision-and-language navigation (VLN) task. However, most existing pre-trainingmethods employ discrete panoramas to learn visual-textual associations. Thisrequires the model to implicitly correlate incomplete, duplicate observationswithin the panoramas, which may impair an agent's spatial understanding. Thus,we propose a new map-based pre-training paradigm that is spatial-aware for usein VLN. Concretely, we build a local metric map to explicitly aggregateincomplete observations and remove duplicates, while modeling navigationdependency in a global topological map. This hybrid design can balance thedemand of VLN for both short-term reasoning and long-term planning. Then, basedon the hybrid map, we devise a pre-training framework to learn a multimodal maprepresentation, which enhances spatial-aware cross-modal reasoning therebyfacilitating the language-guided navigation goal. Extensive experimentsdemonstrate the effectiveness of the map-based pre-training route for VLN, andthe proposed method achieves state-of-the-art on four VLN benchmarks.", "output": "BEVBert: Multimodal Map Pre-training for Language-guided Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to quickly learn a new task with minimal instruction - known asfew-shot learning - is a central aspect of intelligent agents. Classicalfew-shot benchmarks make use of few-shot samples from a single modality, butsuch samples may not be sufficient to characterize an entire concept class. Incontrast, humans use cross-modal information to learn new concepts efficiently.In this work, we demonstrate that one can indeed build a better ${bf visual}$dog classifier by ${bf read}$ing about dogs and ${bf listen}$ing to thembark. To do so, we exploit the fact that recent multimodal foundation modelssuch as CLIP are inherently cross-modal, mapping different modalities to thesame representation space. Specifically, we propose a simple cross-modaladaptation approach that learns from few-shot examples spanning differentmodalities. By repurposing class names as additional one-shot training samples,we achieve SOTA results with an embarrassingly simple linear classifier forvision-language adaptation. Furthermore, we show that our approach can benefitexisting methods such as prefix tuning, adapters, and classifier ensembling.Finally, to explore other modalities beyond vision and language, we constructthe first (to our knowledge) audiovisual few-shot benchmark and use cross-modaltraining to improve the performance of both image and audio classification.", "output": "Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The goal of continual learning is to improve the performance of recognitionmodels in learning sequentially arrived data. Although most existing works areestablished on the premise of learning from scratch, growing efforts have beendevoted to incorporating the benefits of pre-training. However, how toadaptively exploit the pre-trained knowledge for each incremental task whilemaintaining its generalizability remains an open question. In this work, wepresent an extensive analysis for continual learning on a pre-trained model(CLPM), and attribute the key challenge to a progressive overfitting problem.Observing that selectively reducing the learning rate can almost resolve thisissue in the representation layer, we propose a simple but extremely effectiveapproach named Slow Learner with Classifier Alignment (SLCA), which furtherimproves the classification layer by modeling the class-wise distributions andaligning the classification layers in a post-hoc fashion. Across a variety ofscenarios, our proposal provides substantial improvements for CLPM (e.g., up to49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, SplitCUB-200 and Split Cars-196, respectively), and thus outperformsstate-of-the-art approaches by a large margin. Based on such a strong baseline,critical factors and promising directions are analyzed in-depth to facilitatesubsequent research. Code has been made available at:", "output": "SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The foraging behavior of animals is a paradigm of target search in nature.Understanding which foraging strategies are optimal and how animals learn themare central challenges in modeling animal foraging. While the question ofoptimality has wide-ranging implications across fields such as economy,physics, and ecology, the question of learnability is a topic of ongoing debatein evolutionary biology. Recognizing the interconnected nature of thesechallenges, this work addresses them simultaneously by exploring optimalforaging strategies through a reinforcement learning framework. To this end, wemodel foragers as learning agents. We first prove theoretically that maximizingrewards in our reinforcement learning model is equivalent to optimizingforaging efficiency. We then show with numerical experiments that, in theparadigmatic model of non-destructive search, our agents learn foragingstrategies which outperform the efficiency of some of the best known strategiessuch as L'evy walks. These findings highlight the potential of reinforcementlearning as a versatile framework not only for optimizing search strategies butalso to model the learning process, thus shedding light on the role of learningin natural optimization processes.", "output": "Optimal foraging strategies can be learned."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we revisit the efficacy of knowledge distillation as a functionmatching and metric learning problem. In doing so we verify three importantdesign decisions, namely the normalisation, soft maximum function, andprojection layers as key ingredients. We theoretically show that the projectorimplicitly encodes information on past examples, enabling relational gradientsfor the student. We then show that the normalisation of representations istightly coupled with the training dynamics of this projector, which can have alarge impact on the students performance. Finally, we show that a simple softmaximum function can be used to address any significant capacity gap problems.Experimental results on various benchmark datasets demonstrate that using theseinsights can lead to superior or comparable performance to state-of-the-artknowledge distillation techniques, despite being much more computationallyefficient. In particular, we obtain these results across image classification(CIFAR100 and ImageNet), object detection (COCO2017), and on more difficultdistillation objectives, such as training data efficient transformers, wherebywe attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet.", "output": "A closer look at the training dynamics of knowledge distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human intelligence excels at combining basic skills to solve complex tasks.This capability is vital for Artificial Intelligence (AI) and should beembedded in comprehensive intelligent models, enabling them to harness expertmodels for complex task-solving towards Artificial General Intelligence (AGI).Large Language Models (LLMs) show promising learning and reasoning abilities,and can effectively use external models, tools or APIs to tackle complexproblems. In this work, we introduce OpenAGI, an open-source AGI researchplatform designed for multi-step, real-world tasks. Specifically, OpenAGI usesa dual strategy, integrating standard benchmark tasks for benchmarking andevaluation, and open-ended tasks including more expandable models, tools orAPIs for creative problem-solving. Tasks are presented as natural languagequeries to the LLM, which then selects and executes appropriate models. We alsopropose a Reinforcement Learning from Task Feedback (RLTF) mechanism that usestask results to improve the LLM's ability, which creates a self-improving AIfeedback loop. While we acknowledge that AGI is a broad and multifacetedresearch challenge with no singularly defined solution path, the integration ofLLMs with domain-specific expert models, inspired by mirroring the blend ofgeneral and specialized intelligence in humans, offers a promising approachtowards AGI. We are open-sourcing the OpenAGI project's code, dataset,benchmarks, evaluation methods, and demo to foster community involvement in AGIadvancement: ", "output": "OpenAGI: When LLM Meets Domain Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Emergent chain-of-thought (CoT) reasoning capabilities promise to improveperformance and explainability of large language models (LLMs). However,uncertainties remain about how reasoning strategies formulated for previousmodel generations generalize to new model generations and different datasets.In this small-scale study, we compare different reasoning strategies induced byzero-shot prompting across six recently released LLMs (davinci-002,davinci-003, GPT-3.5-turbo, GPT-4, Flan-T5-xxl and Cohere command-xlarge) on amixture of six question-answering datasets, including datasets from scientificand medical domains. Our findings demonstrate that while some variations ineffectiveness occur, gains from CoT reasoning strategies remain robust acrossdifferent models and datasets. GPT-4 has the most benefit from currentstate-of-the-art reasoning strategies and exhibits the best performance byapplying a prompt previously discovered through automated discovery.", "output": "An automatically discovered chain-of-thought prompt generalizes to novel models and datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, significant advancements have been made in time-series forecastingresearch, with an increasing focus on analyzing the nature of time-series data,e.g, channel-independence (CI) and channel-dependence (CD), rather than solelyfocusing on designing sophisticated forecasting models. However, currentresearch has primarily focused on either CI or CD in isolation, and thechallenge of effectively combining these two opposing properties to achieve asynergistic effect remains an unresolved issue. In this paper, we carefullyexamine the opposing properties of CI and CD, and raise a practical questionthat has not been effectively answered, e.g.,\"How to effectively mix the CI andCD properties of time series to achieve better predictive performance?\" Toanswer this question, we propose Mlinear (MIX-Linear), a simple yet effectivemethod based mainly on linear layers. The design philosophy of Mlinear mainlyincludes two aspects:(1) dynamically tuning the CI and CD properties based onthe time semantics of different input time series, and (2) providing deepsupervision to adjust the individual performance of the \"CI predictor\" and \"CDpredictor\". In addition, empirically, we introduce a new loss function thatsignificantly outperforms the widely used mean squared error (MSE) on multipledatasets. Experiments on time-series datasets covering multiple fields andwidely used have demonstrated the superiority of our method over PatchTST whichis the lateset Transformer-based method in terms of the MSE and MAE metrics on7 datasets with identical sequence inputs (336 or 512). Specifically, ourmethod significantly outperforms PatchTST with a ratio of 21:3 at 336 sequencelength input and 29:10 at 512 sequence length input. Additionally, our approachhas a 10 $times$ efficiency advantage at the unit level, taking into accountboth training and inference times.", "output": "Mlinear: Rethink the Linear Model for Time-series Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a latent variable generalisation of neural network softmaxclassification trained with cross-entropy loss, referred to as variationalclassification (VC). Our approach offers a novel probabilistic perspective onthe highly familiar softmax classification model, to which it relates similarlyto how variational and traditional autoencoders relate. We derive a trainingobjective based on the evidence lower bound (ELBO) that is non-trivial tooptimize, and therefore propose an adversarial approach to maximise it. We showthat VC addresses an inherent inconsistency within softmax classification,whilst also allowing more flexible choices of prior distributions in the latentspace in place of implicit assumptions revealed within off-the-shelf softmaxclassifiers. Empirical evaluation on image and text classification datasetsdemonstrates that variational classification maintains prediction accuracywhile improving other desirable properties such as calibration and adversarialrobustness, particularly under distribution shift and low data settings.", "output": "Variational Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rapidly evolving fields of e-commerce and metaverse continue to seekinnovative approaches to enhance the consumer experience. At the same time,recent advancements in the development of diffusion models have enabledgenerative networks to create remarkably realistic images. In this context,image-based virtual try-on, which consists in generating a novel image of atarget model wearing a given in-shop garment, has yet to capitalize on thepotential of these powerful generative solutions. This work introducesLaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for theVirtual Try-ON task. The proposed architecture relies on a latent diffusionmodel extended with a novel additional autoencoder module that exploitslearnable skip connections to enhance the generation process preserving themodel's characteristics. To effectively maintain the texture and details of thein-shop garment, we propose a textual inversion component that can map thevisual features of the garment to the CLIP token embedding space and thusgenerate a set of pseudo-word token embeddings capable of conditioning thegeneration process. Experimental results on Dress Code and VITON-HD datasetsdemonstrate that our approach outperforms the competitors by a consistentmargin, achieving a significant milestone for the task. Source code and trainedmodels are publicly available at: ", "output": "LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks(GNNs) on a given graph topology by dynamically learning it. However, most ofLGI methods assume to have a (noisy, incomplete, improvable, ...) input graphto rewire and can solely learn regular graph topologies. In the wake of thesuccess of Topological Deep Learning (TDL), we study Latent Topology Inference(LTI) for learning higher-order cell complexes (with sparse and not regulartopology) describing multi-way interactions between data points. To this aim,we introduce the Differentiable Cell Complex Module (DCM), a novel learnablefunction that computes cell probabilities in the complex to improve thedownstream task. We show how to integrate DCM with cell complex message passingnetworks layers and train it in a end-to-end fashion, thanks to a two-stepinference procedure that avoids an exhaustive search across all possible cellsin the input, thus maintaining scalability. Our model is tested on severalhomophilic and heterophilic graph datasets and it is shown to outperform otherstate-of-the-art techniques, offering significant improvements especially incases where an input graph is not provided.", "output": "From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extensive research has been conducted on fault diagnosis of planetarygearboxes using vibration signals and deep learning (DL) approaches. However,DL-based methods are susceptible to the domain shift problem caused by varyingoperating conditions of the gearbox. Although domain adaptation and datasynthesis methods have been proposed to overcome such domain shifts, they areoften not directly applicable in real-world situations where only healthy datais available in the target domain. To tackle the challenge of extreme domainshift scenarios where only healthy data is available in the target domain, thispaper proposes two novel domain knowledge-informed data synthesis methodsutilizing the health data map (HDMap). The two proposed approaches are referredto as scaled CutPaste and FaultPaste. The HDMap is used to physically representthe vibration signal of the planetary gearbox as an image-like matrix, allowingfor visualization of fault-related features. CutPaste and FaultPaste are thenapplied to generate faulty samples based on the healthy data in the targetdomain, using domain knowledge and fault signatures extracted from the sourcedomain, respectively. In addition to generating realistic faults, the proposedmethods introduce scaling of fault signatures for controlled synthesis offaults with various severity levels. A case study is conducted on a planetarygearbox testbed to evaluate the proposed approaches. The results show that theproposed methods are capable of accurately diagnosing faults, even in cases ofextreme domain shift, and can estimate the severity of faults that have notbeen previously observed in the target domain.", "output": "Domain knowledge-informed Synthetic fault sample generation with Health Data Map for cross-domain Planetary Gearbox Fault Diagnosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Non-Fungible-Token (NFT) market has experienced explosive growth inrecent years. According to DappRadar, the total transaction volume on OpenSea,the largest NFT marketplace, reached 34.7 billion dollars in February 2023.However, the NFT market is mostly unregulated and there are significantconcerns about money laundering, fraud and wash trading. The lack ofindustry-wide regulations, and the fact that amateur traders and retailinvestors comprise a significant fraction of the NFT market, make this marketparticularly vulnerable to fraudulent activities. Therefore it is essential toinvestigate and highlight the relevant risks involved in NFT trading. In thispaper, we attempted to uncover common fraudulent behaviors such as wash tradingthat could mislead other traders. Using market data, we designed quantitativefeatures from the network, monetary, and temporal perspectives that were fedinto K-means clustering unsupervised learning algorithm to sort traders intogroups. Lastly, we discussed the clustering results' significance and howregulations can reduce undesired behaviors. Our work can potentially helpregulators narrow down their search space for bad actors in the market as wellas provide insights for amateur traders to protect themselves from unforeseenfrauds.", "output": "Abnormal Trading Detection in the NFT Market."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image generative models have enabled high-resolution image synthesisacross different domains, but require users to specify the content they wish togenerate. In this paper, we consider the inverse problem -- given a collectionof different images, can we discover the generative concepts that representeach image? We present an unsupervised approach to discover generative conceptsfrom a collection of images, disentangling different art styles in paintings,objects, and lighting from kitchen scenes, and discovering image classes givenImageNet images. We show how such generative concepts can accurately representthe content of images, be recombined and composed to generate new artistic andhybrid images, and be further used as a representation for downstreamclassification tasks.", "output": "Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The impressive recent performance of large language models has led many towonder to what extent they can serve as models of general intelligence or aresimilar to human cognition. We address this issue by applying GPT-3.5 and GPT-4to a classic problem in human inductive reasoning known as property induction.Over two experiments, we elicit human judgments on a range of propertyinduction tasks spanning multiple domains. Although GPT-3.5 struggles tocapture many aspects of human behaviour, GPT-4 is much more successful: for themost part, its performance qualitatively matches that of humans, and the onlynotable exception is its failure to capture the phenomenon of premisenon-monotonicity. Our work demonstrates that property induction allows forinteresting comparisons between human and machine intelligence and provides twolarge datasets that can serve as benchmarks for future work in this vein.", "output": "Inductive reasoning in humans and large language models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have emerged as the emph{de-facto} technique for imagegeneration, yet they entail significant computational overhead, hindering thetechnique's broader application in the research community. We propose aprior-based denoising training framework, the first to incorporate thepre-train and fine-tune paradigm into the diffusion model training process,which substantially improves training efficiency and shows potential infacilitating various downstream tasks. Our approach centers on masking a highproportion (e.g., up to 90%) of the input image and employing masked denoisingscore matching to denoise the visible areas, thereby guiding the diffusionmodel to learn more salient features from training data as prior knowledge. Byutilizing masked learning in a pre-training stage, we efficiently train theViT-based diffusion model on CelebA-HQ $256 times 256$ in the pixel space,achieving a 4x acceleration and enhancing the quality of generated imagescompared to denoising diffusion probabilistic model (DDPM). Moreover, ourmasked pre-training technique can be universally applied to various diffusionmodels that directly generate images in the pixel space, aiding in the learningof pre-trained models with superior generalizability. For instance, a diffusionmodel pre-trained on VGGFace2 attains a 46% quality improvement throughfine-tuning with merely 10% data from a different distribution. Moreover, ourmethod shows the potential to serve as a training paradigm for enhancing theprivacy protection capabilities of diffusion models. Our code is available aturl{", "output": "Masked Diffusion Models Are Fast and Privacy-Aware Learners."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optical flow and disparity are two informative visual features for autonomousdriving perception. They have been used for a variety of applications, such asobstacle and lane detection. The concept of \"U-V-Disparity\" has been widelyexplored in the literature, while its counterpart in optical flow has receivedrelatively little attention. Traditional motion analysis algorithms estimateoptical flow by matching correspondences between two successive video frames,which limits the full utilization of environmental information and geometricconstraints. Therefore, we propose a novel strategy to model optical flow inthe collision-free space (also referred to as drivable area or simplyfreespace) for intelligent vehicles, with the full utilization of geometryinformation in a 3D driving environment. We provide explicit representations ofoptical flow and deduce the quadratic relationship between the optical flowcomponent and the vertical coordinate. Through extensive experiments on severalpublic datasets, we demonstrate the high accuracy and robustness of our model.Additionally, our proposed freespace optical flow model boasts a diverse arrayof applications within the realm of automated driving, providing a geometricconstraint in freespace detection, vehicle localization, and more. We have madeour source code publicly available at ", "output": "Freespace Optical Flow Modeling for Automated Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy theft detection (ETD) and energy consumption forecasting (ECF) are twointerconnected challenges in smart grid systems. Addressing these issuescollectively is crucial for ensuring system security. This paper addresses theinterconnected challenges of ETD and ECF in smart grid systems. The proposedsolution combines long short-term memory (LSTM) and a denoising diffusionprobabilistic model (DDPM) to generate input reconstruction and forecasting. Byleveraging the reconstruction and forecasting errors, the system identifiesinstances of energy theft, with the methods based on reconstruction error andforecasting error complementing each other in detecting different types ofattacks. Through extensive experiments on real-world and synthetic datasets,the proposed scheme outperforms baseline methods in ETD and ECF problems. Theensemble method significantly enhances ETD performance, accurately detectingenergy theft attacks that baseline methods fail to detect. The research offersa comprehensive and effective solution for addressing ETD and ECF challenges,demonstrating promising results and improved security in smart grid systems.", "output": "An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For the performance modeling of power converters, the mainstream approachesare essentially knowledge-based, suffering from heavy manpower burden and lowmodeling accuracy. Recent emerging data-driven techniques greatly relieve humanreliance by automatic modeling from simulation data. However, model discrepancymay occur due to unmodeled parasitics, deficient thermal and magnetic models,unpredictable ambient conditions, etc. These inaccurate data-driven modelsbased on pure simulation cannot represent the practical performance in physicalworld, hindering their applications in power converter modeling. To alleviatemodel discrepancy and improve accuracy in practice, this paper proposes a noveldata-driven modeling with experimental augmentation (D2EA), leveraging bothsimulation data and experimental data. In D2EA, simulation data aims toestablish basic functional landscape, and experimental data focuses on matchingactual performance in real world. The D2EA approach is instantiated for theefficiency optimization of a hybrid modulation for neutral-point-clampeddual-active-bridge (NPC-DAB) converter. The proposed D2EA approach realizes99.92% efficiency modeling accuracy, and its feasibility is comprehensivelyvalidated in 2-kW hardware experiments, where the peak efficiency of 98.45% isattained. Overall, D2EA is data-light and can achieve highly accurate andhighly practical data-driven models in one shot, and it is scalable to otherapplications, effortlessly.", "output": "Data-Driven Modeling with Experimental Augmentation for the Modulation Strategy of the Dual-Active-Bridge Converter."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In machine learning, we naturally apply an Observation-Oriented principle, inwhich observational variables preexist and set the stage for constructingrelationships. While sufficient for traditional models, the integration of AIwith big data exposes the misalignment between the observational models and ouractual comprehension. Contrarily, humans shape cognitive entities defined byrelationships, enabling us to formulate knowledge across temporal andhyper-dimensional spaces, rather than being confined to observationalconstructs. From an innovative Relation-Oriented perspective, this studyexamines the roots of this misalignment within our current modeling paradigm,illuminated by intuitive examples from computer vision and health informatics.We also introduce the relation-defined representation learning methodology as apractical implementation of Relation-Oriented modeling, supported by extensiveexperimental validation.", "output": "Relation-Oriented: Toward Knowledge-Aligned Causal AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models and large language models have emerged as leading-edgegenerative models and have sparked a revolutionary impact on various aspects ofhuman life. However, the practical implementation of these models has alsoexposed inherent risks, highlighting their dual nature and raising concernsregarding their trustworthiness. Despite the abundance of literature on thissubject, a comprehensive survey specifically delving into the intersection oflarge-scale generative models and their trustworthiness remains largely absent.To bridge this gap, This paper investigates both the long-standing and emergingthreats associated with these models across four fundamental dimensions:privacy, security, fairness, and responsibility. In this way, we construct anextensive map outlining the trustworthiness of these models, while alsoproviding practical recommendations and identifying future directions. Theseefforts are crucial for promoting the trustworthy deployment of these models,ultimately benefiting society as a whole.", "output": "On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the broad application of Machine Learning models as a Service(MLaaS), they are vulnerable to model stealing attacks. These attacks canreplicate the model functionality by using the black-box query process withoutany prior knowledge of the target victim model. Existing stealing defenses adddeceptive perturbations to the victim's posterior probabilities to mislead theattackers. However, these defenses are now suffering problems of high inferencecomputational overheads and unfavorable trade-offs between benign accuracy andstealing robustness, which challenges the feasibility of deployed models inpractice. To address the problems, this paper proposes Isolation and Induction(InI), a novel and effective training framework for model stealing defenses.Instead of deploying auxiliary defense modules that introduce redundantinference time, InI directly trains a defensive model by isolating theadversary's training gradient from the expected gradient, which can effectivelyreduce the inference computational cost. In contrast to adding perturbationsover model predictions that harm the benign accuracy, we train models toproduce uninformative outputs against stealing queries, which can induce theadversary to extract little useful knowledge from victim models with minimalimpact on the benign performance. Extensive experiments on several visualclassification datasets (e.g., MNIST and CIFAR10) demonstrate the superiorrobustness (up to 48% reduction on stealing accuracy) and speed (up to 25.4xfaster) of our InI over other state-of-the-art methods. Our codes can be foundin ", "output": "Isolation and Induction: Training Robust Deep Neural Networks against Model Stealing Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Building a multi-modality multi-task neural network toward accurate androbust performance is a de-facto standard in perception task of autonomousdriving. However, leveraging such data from multiple sensors to jointlyoptimize the prediction and planning tasks remains largely unexplored. In thispaper, we present FusionAD, to the best of our knowledge, the first unifiedframework that fuse the information from two most critical sensors, camera andLiDAR, goes beyond perception task. Concretely, we first build a transformerbased multi-modality fusion network to effectively produce fusion basedfeatures. In constrast to camera-based end-to-end method UniAD, we thenestablish a fusion aided modality-aware prediction and status-aware planningmodules, dubbed FMSPnP that take advantages of multi-modality features. Weconduct extensive experiments on commonly used benchmark nuScenes dataset, ourFusionAD achieves state-of-the-art performance and surpassing baselines onaverage 15% on perception tasks like detection and tracking, 10% on occupancyprediction accuracy, reducing prediction error from 0.708 to 0.389 in ADE scoreand reduces the collision rate from 0.31% to only 0.12%.", "output": "FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays, registration methods are typically evaluated based onsub-resolution tracking error differences. In an effort to reinfuse thisevaluation process with clinical relevance, we propose to reframe imageregistration as a landmark detection problem. Ideally, landmark-specificdetection thresholds are derived from an inter-rater analysis. To approximatethis costly process, we propose to compute hit rate curves based on thedistribution of errors of a sub-sample inter-rater analysis. Therefore, wesuggest deriving thresholds from the error distribution using the formula:median + delta * median absolute deviation. The method promises differentiationof previously indistinguishable registration algorithms and further enablesassessing the clinical significance in algorithm development.", "output": "Framing image registration as a landmark detection problem for better representation of clinical relevance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Determining lymphoma subtypes is a crucial step for better patients treatmenttargeting to potentially increase their survival chances. In this context, theexisting gold standard diagnosis method, which is based on gene expressiontechnology, is highly expensive and time-consuming making difficult itsaccessibility. Although alternative diagnosis methods based on IHC(immunohistochemistry) technologies exist (recommended by the WHO), they stillsuffer from similar limitations and are less accurate. WSI (Whole Slide Image)analysis by deep learning models showed promising new directions for cancerdiagnosis that would be cheaper and faster than existing alternative methods.In this work, we propose a vision transformer-based framework fordistinguishing DLBCL (Diffuse Large B-Cell Lymphoma) cancer subtypes fromhigh-resolution WSIs. To this end, we propose a multi-modal architecture totrain a classifier model from various WSI modalities. We then exploit thismodel through a knowledge distillation mechanism for efficiently driving thelearning of a mono-modal classifier. Our experimental study conducted on adataset of 157 patients shows the promising performance of our mono-modalclassification model, outperforming six recent methods from thestate-of-the-art dedicated for cancer classification. Moreover, the power-lawcurve, estimated on our experimental data, shows that our classification modelrequires a reasonable number of additional patients for its training topotentially reach identical diagnosis accuracy as IHC technologies.", "output": "A vision transformer-based framework for knowledge transfer from multi-modal to mono-modal lymphoma subtyping models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Long exposure photography produces stunning imagery, representing movingelements in a scene with motion-blur. It is generally employed in twomodalities, producing either a foreground or a background blur effect.Foreground blur images are traditionally captured on a tripod-mounted cameraand portray blurred moving foreground elements, such as silky water or lighttrails, over a perfectly sharp background landscape. Background blur images,also called panning photography, are captured while the camera is tracking amoving subject, to produce an image of a sharp subject over a backgroundblurred by relative motion. Both techniques are notoriously challenging andrequire additional equipment and advanced skills. In this paper, we describe acomputational burst photography system that operates in a hand-held smartphonecamera app, and achieves these effects fully automatically, at the tap of theshutter button. Our approach first detects and segments the salient subject. Wetrack the scene motion over multiple frames and align the images in order topreserve desired sharpness and to produce aesthetically pleasing motionstreaks. We capture an under-exposed burst and select the subset of inputframes that will produce blur trails of controlled length, regardless of sceneor camera motion velocity. We predict inter-frame motion and synthesizemotion-blur to fill the temporal gaps between the input frames. Finally, wecomposite the blurred image with the sharp regular exposure to protect thesharpness of faces or areas of the scene that are barely moving, and produce afinal high resolution and high dynamic range (HDR) photograph. Our systemdemocratizes a capability previously reserved to professionals, and makes thiscreative style accessible to most casual photographers.More information and supplementary material can be found on our projectwebpage: ", "output": "Computational Long Exposure Mobile Photography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays, autonomous cars are gaining traction due to their numerouspotential applications on battlefields and in resolving a variety of otherreal-world challenges. The main goal of our project is to build an autonomoussystem using DeepRacer which will follow a specific person (for our project, asoldier) when they will be moving in any direction. Two main components toaccomplish this project is an optimized Single-Shot Multibox Detection (SSD)object detection model and a Reinforcement Learning (RL) model. We accomplishedthe task using SSD Lite instead of SSD and at the end, compared the resultsamong SSD, SSD with Neural Computing Stick (NCS), and SSD Lite. Experimentalresults show that SSD Lite gives better performance among these threetechniques and exhibits a considerable boost in inference speed (~2-3 times)without compromising accuracy.", "output": "Follow the Soldiers with Optimized Single-Shot Multibox Detection and Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce OpenFlamingo, a family of autoregressive vision-language modelsranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to producean open-source replication of DeepMind's Flamingo models. On sevenvision-language datasets, OpenFlamingo models average between 80 - 89% ofcorresponding Flamingo performance. This technical report describes our models,training data, hyperparameters, and evaluation suite. We share our models andcode at ", "output": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Our method builds upon previous Medical Out-of-Distribution (MOOD) challengewinners that empirically show that synthetic local anomalies generated copying/ interpolating foreign patches are useful to train segmentation networks ableto generalize to unseen types of anomalies. In terms of the synthetic anomalygeneration process, our contributions makes synthetic anomalies moreheterogeneous and challenging by 1) using random shapes instead of squares and2) smoothing the interpolation edge of anomalies so networks cannot rely on thehigh gradient between image - foreign patch to identify anomalies. Ourexperiments using the validation set of 2020 MOOD winners show that bothcontributions improved substantially the method performance. We used a standard3D U-Net architecture as segmentation network, trained patch-wise in both brainand abdominal datasets. Our final challenge submission consisted of 10 U-Netstrained across 5 data folds with different configurations of the anomalygeneration process. Our method achieved first position in both sample-wise andpixel-wise tasks in the 2022 edition of the Medical Out-of-Distribution held atMICCAI.", "output": "Harder synthetic anomalies to improve OoD detection in Medical Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning models for self-driving cars require a diverse training datasetto safely manage critical driving scenarios on public roads. This includeshaving data from divergent trajectories such as the oncoming traffic lane orsidewalks. Such data would be too dangerous to collect in the real world. Dataaugmentation approaches have been proposed to tackle this issue using RGBimages. However, solutions based on LiDAR sensors are scarce. We thereforepropose an approach to synthesize additional LiDAR point clouds from novelviewpoints without having the need to physically drive at dangerous positions.The LiDAR view synthesis is done using mesh reconstruction and ray casting. Wetrain a deep learning model, which takes a LiDAR scan as input and predicts thefuture trajectory as output. A waypoint controller is then applied on thispredicted trajectory to determine the throttle and steering labels of theego-vehicle. Our method neither requires expert driving labels for the originalnor for the synthesized LiDAR sequence. Instead, we infer labels from LiDARodometry. We demonstrate the effectiveness of our approach in a comprehensiveonline evaluation and with a comparison to concurrent work. Our results showthe importance of synthesizing additional LiDAR point clouds, particularly interms of model robustness. Code and supplementary visualizations are availableat  .", "output": "LiDAR View Synthesis for Robust Vehicle Navigation Without Expert Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The COVID-19 pandemic presented numerous challenges to healthcare systemsworldwide. Given that lung infections are prevalent among COVID-19 patients,chest Computer Tomography (CT) scans have frequently been utilized as analternative method for identifying COVID-19 conditions and various other typesof pulmonary diseases. Deep learning architectures have emerged to automate theidentification of pulmonary disease types by leveraging CT scan slices asinputs for classification models. This paper introduces COVID-VR, a novelapproach for classifying pulmonary diseases based on volume rendering images ofthe lungs captured from multiple angles, thereby providing a comprehensive viewof the entire lung in each image. To assess the effectiveness of our proposal,we compared it against competing strategies utilizing both private dataobtained from partner hospitals and a publicly available dataset. The resultsdemonstrate that our approach effectively identifies pulmonary lesions andperforms competitively when compared to slice-based methods.", "output": "COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A self-driving vehicle (SDV) must be able to perceive its surroundings andpredict the future behavior of other traffic participants. Existing workseither perform object detection followed by trajectory forecasting of thedetected objects, or predict dense occupancy and flow grids for the wholescene. The former poses a safety concern as the number of detections needs tobe kept low for efficiency reasons, sacrificing object recall. The latter iscomputationally expensive due to the high-dimensionality of the output grid,and suffers from the limited receptive field inherent to fully convolutionalnetworks. Furthermore, both approaches employ many computational resourcespredicting areas or objects that might never be queried by the motion planner.This motivates our unified approach to perception and future prediction thatimplicitly represents occupancy and flow over time with a single neuralnetwork. Our method avoids unnecessary computation, as it can be directlyqueried by the motion planner at continuous spatio-temporal locations.Moreover, we design an architecture that overcomes the limited receptive fieldof previous explicit occupancy prediction methods by adding an efficient yeteffective global attention mechanism. Through extensive experiments in bothurban and highway settings, we demonstrate that our implicit model outperformsthe current state-of-the-art. For more information, visit the project website:", "output": "Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image diffusion models such as Stable Diffusion have recentlyattracted the interest of many researchers, and inverting the diffusion processcan play an important role in better understanding the generative process andhow to engineer prompts in order to obtain the desired images. To this end, weintroduce the new task of predicting the text prompt given an image generatedby a generative diffusion model. We combine a series of white-box and black-boxmodels (with and without access to the weights of the diffusion network) todeal with the proposed task. We propose a novel learning framework comprisingof a joint prompt regression and multi-label vocabulary classificationobjective that generates improved prompts. To further improve our method, weemploy a curriculum learning procedure that promotes the learning ofimage-prompt pairs with lower labeling noise (i.e. that are better aligned),and an unsupervised domain-adaptive kernel learning method that uses thesimilarities between samples in the source and target domains as extrafeatures. We conduct experiments on the DiffusionDB data set, predicting textprompts from images generated by Stable Diffusion. Our novel learning frameworkproduces excellent results on the aforementioned task, yielding the highestgains when applied on the white-box model. In addition, we make an interestingdiscovery: training a diffusion model on the prompt generation task can makethe model generate images that are much better aligned with the input prompts,when the model is directly reused for text-to-image generation.", "output": "Reverse Stable Diffusion: What prompt was used to generate this image?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the HANDAL dataset for category-level object pose estimation andaffordance prediction. Unlike previous datasets, ours is focused onrobotics-ready manipulable objects that are of the proper size and shape forfunctional grasping by robot manipulators, such as pliers, utensils, andscrewdrivers. Our annotation process is streamlined, requiring only a singleoff-the-shelf camera and semi-automated processing, allowing us to producehigh-quality 3D annotations without crowd-sourcing. The dataset consists of308k annotated image frames from 2.2k videos of 212 real-world objects in 17categories. We focus on hardware and kitchen tool objects to facilitateresearch in practical scenarios in which a robot manipulator needs to interactwith the environment beyond simple pushing or indiscriminate grasping. Weoutline the usefulness of our dataset for 6-DoF category-level pose+scaleestimation and related tasks. We also provide 3D reconstructed meshes of allobjects, and we outline some of the bottlenecks to be addressed fordemocratizing the collection of datasets like this one.", "output": "HANDAL: A Dataset of Real-World Manipulable Object Categories with Pose Annotations, Affordances, and Reconstructions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-time rendering for video games has become increasingly challenging dueto the need for higher resolutions, framerates and photorealism. Supersamplinghas emerged as an effective solution to address this challenge. Our workintroduces a novel neural algorithm for supersampling rendered content that is4 times more efficient than existing methods while maintaining the same levelof accuracy. Additionally, we introduce a new dataset which provides auxiliarymodalities such as motion vectors and depth generated using graphics renderingfeatures like viewport jittering and mipmap biasing at different resolutions.We believe that this dataset fills a gap in the current dataset landscape andcan serve as a valuable resource to help measure progress in the field andadvance the state-of-the-art in super-resolution techniques for gaming content.", "output": "Efficient neural supersampling on a novel gaming dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic colored meshes (DCM) are widely used in various applications;however, these meshes may undergo different processes, such as compression ortransmission, which can distort them and degrade their quality. To facilitatethe development of objective metrics for DCMs and study the influence oftypical distortions on their perception, we create the Tencent - dynamiccolored mesh database (TDMD) containing eight reference DCM objects with sixtypical distortions. Using processed video sequences (PVS) derived from theDCM, we have conducted a large-scale subjective experiment that resulted in 303distorted DCM samples with mean opinion scores, making the TDMD the largestavailable DCM database to our knowledge. This database enabled us to study theimpact of different types of distortion on human perception and offerrecommendations for DCM compression and related tasks. Additionally, we haveevaluated three types of state-of-the-art objective metrics on the TDMD,including image-based, point-based, and video-based metrics, on the TDMD. Ourexperimental results highlight the strengths and weaknesses of each metric, andwe provide suggestions about the selection of metrics in practical DCMapplications. The TDMD will be made publicly available at the followinglocation: ", "output": "TDMD: A Database for Dynamic Color Mesh Subjective and Objective Quality Explorations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image generative models can produce photo-realistic images for anextremely broad range of concepts, and their usage has proliferated widelyamong the general public. On the flip side, these models have numerousdrawbacks, including their potential to generate images featuring sexuallyexplicit content, mirror artistic styles without permission, or evenhallucinate (or deepfake) the likenesses of celebrities. Consequently, variousmethods have been proposed in order to \"erase\" sensitive concepts fromtext-to-image models. In this work, we examine five recently proposed concepterasure methods, and show that targeted concepts are not fully excised from anyof these methods. Specifically, we leverage the existence of special learnedword embeddings that can retrieve \"erased\" concepts from the sanitized modelswith no alterations to their weights. Our results highlight the brittleness ofpost hoc concept erasure methods, and call into question their use in thealgorithmic toolkit for AI safety.", "output": "Circumventing Concept Erasure Methods For Text-to-Image Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "DeepFakes have raised serious societal concerns, leading to a great surge indetection-based forensics methods in recent years. Face forgery recognition isthe conventional detection method that usually follows a two-phase pipeline: itextracts the face first and then determines its authenticity by classification.Since DeepFakes in the wild usually contain multiple faces, using face forgerydetection methods is merely practical as they have to process faces in asequel, i.e., only one face is processed at the same time. One straightforwardway to address this issue is to integrate face extraction and forgery detectionin an end-to-end fashion by adapting advanced object detection architectures.However, as these object detection architectures are designed to capture thesemantic information of different object categories rather than the subtleforgery traces among the faces, the direct adaptation is far from optimal. Inthis paper, we describe a new end-to-end framework, ContrastiveMulti-FaceForensics (COMICS), to enhance multi-face forgery detection. The coreof the proposed framework is a novel bi-grained contrastive learning approachthat explores effective face forgery traces at both the coarse- andfine-grained levels. Specifically, the coarse-grained level contrastivelearning captures the discriminative features among positive and negativeproposal pairs in multiple scales with the instruction of the proposalgenerator, and the fine-grained level contrastive learning captures thepixel-wise discrepancy between the forged and original areas of the same faceand the pixel-wise content inconsistency between different faces. Extensiveexperiments on the OpenForensics dataset demonstrate our method outperformsother counterparts by a large margin (~18.5%) and shows great potential forintegration into various architectures.", "output": "Contrastive Multi-FaceForensics: An End-to-end Bi-grained Contrastive Learning Approach for Multi-face Forgery Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In engineering applications, line, circle, arc, and point are collectivelyreferred to as primitives, and they play a crucial role in path planning,simulation analysis, and manufacturing. When designing CAD models, engineerstypically start by sketching the model's orthographic view on paper or awhiteboard and then translate the design intent into a CAD program. Althoughthis design method is powerful, it often involves challenging and repetitivetasks, requiring engineers to perform numerous similar operations in eachdesign. To address this conversion process, we propose an efficient andaccurate end-to-end method that avoids the inefficiency and error accumulationissues associated with using auto-regressive models to infer parametricprimitives from hand-drawn sketch images. Since our model samples match therepresentation format of standard CAD software, they can be imported into CADsoftware for solving, editing, and applied to downstream design tasks.", "output": "PPI-NET: End-to-End Parametric Primitive Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "AI alignment refers to models acting towards human-intended goals,preferences, or ethical principles. Given that most large-scale deep learningmodels act as black boxes and cannot be manually controlled, analyzing thesimilarity between models and humans can be a proxy measure for ensuring AIsafety. In this paper, we focus on the models' visual perception alignment withhumans, further referred to as AI-human visual alignment. Specifically, wepropose a new dataset for measuring AI-human visual alignment in terms of imageclassification, a fundamental task in machine perception. In order to evaluateAI-human visual alignment, a dataset should encompass samples with variousscenarios that may arise in the real world and have gold human perceptionlabels. Our dataset consists of three groups of samples, namely Must-Act (i.e.,Must-Classify), Must-Abstain, and Uncertain, based on the quantity and clarityof visual information in an image and further divided into eight categories.All samples have a gold human perception label; even Uncertain (severelyblurry) sample labels were obtained via crowd-sourcing. The validity of ourdataset is verified by sampling theory, statistical theories related to surveydesign, and experts in the related fields. Using our dataset, we analyze thevisual alignment and reliability of five popular visual perception models andseven abstention methods. Our code and data is available aturl{", "output": "VisAlign: Dataset for Measuring the Degree of Alignment between AI and Humans in Visual Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present the solution of our team HFUT-VUT for theMultiMediate Grand Challenge 2023 at ACM Multimedia 2023. The solution coversthree sub-challenges: bodily behavior recognition, eye contact detection, andnext speaker prediction. We select Swin Transformer as the baseline and exploitdata augmentation strategies to address the above three tasks. Specifically, wecrop the raw video to remove the noise from other parts. At the same time, weutilize data augmentation to improve the generalization of the model. As aresult, our solution achieves the best results of 0.6262 for bodily behaviorrecognition in terms of mean average precision and the accuracy of 0.7771 foreye contact detection on the corresponding test set. In addition, our approachalso achieves comparable results of 0.5281 for the next speaker prediction interms of unweighted average recall.", "output": "Data Augmentation for Human Behavior Analysis in Multi-Person Conversations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Applying large-scale pre-trained visual models like CLIP to few-shot actionrecognition tasks can benefit performance and efficiency. Utilizing the\"pre-training, fine-tuning\" paradigm makes it possible to avoid training anetwork from scratch, which can be time-consuming and resource-intensive.However, this method has two drawbacks. First, limited labeled samples forfew-shot action recognition necessitate minimizing the number of tunableparameters to mitigate over-fitting, also leading to inadequate fine-tuningthat increases resource consumption and may disrupt the generalizedrepresentation of models. Second, the video's extra-temporal dimensionchallenges few-shot recognition's effective temporal modeling, whilepre-trained visual models are usually image models. This paper proposes a novelmethod called Multimodal Adaptation of CLIP (MA-CLIP) to address these issues.It adapts CLIP for few-shot action recognition by adding lightweight adapters,which can minimize the number of learnable parameters and enable the model totransfer across different tasks quickly. The adapters we design can combineinformation from video-text multimodal sources for task-oriented spatiotemporalmodeling, which is fast, efficient, and has low training costs. Additionally,based on the attention mechanism, we design a text-guided prototypeconstruction module that can fully utilize video-text information to enhancethe representation of video prototypes. Our MA-CLIP is plug-and-play, which canbe used in any different few-shot action recognition temporal alignment metric.", "output": "Multimodal Adaptation of CLIP for Few-Shot Action Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face swapping is a task that changes a facial identity of a given image tothat of another person. In this work, we propose a novel face-swappingframework called Megapixel Facial Identity Manipulation (MFIM). Theface-swapping model should achieve two goals. First, it should be able togenerate a high-quality image. We argue that a model which is proficient ingenerating a megapixel image can achieve this goal. However, generating amegapixel image is generally difficult without careful model design. Therefore,our model exploits pretrained StyleGAN in the manner of GAN-inversion toeffectively generate a megapixel image. Second, it should be able toeffectively transform the identity of a given image. Specifically, it should beable to actively transform ID attributes (e.g., face shape and eyes) of a givenimage into those of another person, while preserving ID-irrelevant attributes(e.g., pose and expression). To achieve this goal, we exploit 3DMM that cancapture various facial attributes. Specifically, we explicitly supervise ourmodel to generate a face-swapped image with the desirable attributes using3DMM. We show that our model achieves state-of-the-art performance throughextensive experiments. Furthermore, we propose a new operation called IDmixing, which creates a new identity by semantically mixing the identities ofseveral people. It allows the user to customize the new identity.", "output": "MFIM: Megapixel Facial Identity Manipulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning methods are developing rapidly in coded aperture snapshotspectral imaging (CASSI). The number of parameters and FLOPs of existingstate-of-the-art methods (SOTA) continues to increase, but the reconstructionaccuracy improves slowly. Current methods still face two problems: 1) Theperformance of the spatial light modulator (SLM) is not fully developed due tothe limitation of fixed Mask coding. 2) The single input limits the networkperformance. In this paper we present a dynamic-mask-based dual camera system,which consists of an RGB camera and a CASSI system running in parallel. First,the system learns the spatial feature distribution of the scene based on theRGB images, then instructs the SLM to encode each scene, and finally sends bothRGB and CASSI images to the network for reconstruction. We further designed theDMDC-net, which consists of two separate networks, a small-scale CNN-baseddynamic mask network for dynamic adjustment of the mask and a multimodalreconstruction network for reconstruction using RGB and CASSI measurements.Extensive experiments on multiple datasets show that our method achieves morethan 9 dB improvement in PSNR over the SOTA.(", "output": "DMDC: Dynamic-mask-based dual camera design for snapshot Hyperspectral Imaging."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language models demonstrate remarkable capacity to generalize representationslearned in one modality to downstream tasks in other modalities. Can we tracethis ability to individual neurons? We study the case where a frozen texttransformer is augmented with vision using a self-supervised visual encoder anda single linear projection learned on an image-to-text task. Outputs of theprojection layer are not immediately decodable into language describing imagecontent; instead, we find that translation between modalities occurs deeperwithin the transformer. We introduce a procedure for identifying \"multimodalneurons\" that convert visual representations into corresponding text, anddecoding the concepts they inject into the model's residual stream. In a seriesof experiments, we show that multimodal neurons operate on specific visualconcepts across inputs, and have a systematic causal effect on imagecaptioning.", "output": "Multimodal Neurons in Pretrained Text-Only Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We generalize the class vectors found in neural networks to linear subspaces(i.e.~points in the Grassmann manifold) and show that the Grassmann ClassRepresentation (GCR) enables the simultaneous improvement in accuracy andfeature transferability. In GCR, each class is a subspace and the logit isdefined as the norm of the projection of a feature onto the class subspace. Weintegrate Riemannian SGD into deep learning frameworks such that classsubspaces in a Grassmannian are jointly optimized with the rest modelparameters. Compared to the vector form, the representative capability ofsubspaces is more powerful. We show that on ImageNet-1K, the top-1 error ofResNet50-D, ResNeXt50, Swin-T and Deit3-S are reduced by 5.6%, 4.5%, 3.0% and3.5%, respectively. Subspaces also provide freedom for features to vary and weobserved that the intra-class feature variability grows when the subspacedimension increases. Consequently, we found the quality of GCR features isbetter for downstream tasks. For ResNet50-D, the average linear transferaccuracy across 6 datasets improves from 77.98% to 79.70% compared to thestrong baseline of vanilla softmax. For Swin-T, it improves from 81.5% to 83.4%and for Deit3, it improves from 73.8% to 81.4%. With these encouraging results,we believe that more applications could benefit from the Grassmann classrepresentation. Code is released at ", "output": "Get the Best of Both Worlds: Improving Accuracy and Transferability by Grassmann Class Representation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, many deep learning-based methods have been proposed totackle the problem of optical flow estimation and achieved promising results.However, they hardly consider that most videos are compressed and thus ignorethe pre-computed information in compressed video streams. Motion vectors, oneof the compression information, record the motion of the video frames. They canbe directly extracted from the compression code stream without computationalcost and serve as a solid prior for optical flow estimation. Therefore, wepropose an optical flow model, MVFlow, which uses motion vectors to improve thespeed and accuracy of optical flow estimation for compressed videos. In detail,MVFlow includes a key Motion-Vector Converting Module, which ensures that themotion vectors can be transformed into the same domain of optical flow and thenbe utilized fully by the flow estimation module. Meanwhile, we construct fouroptical flow datasets for compressed videos containing frames and motionvectors in pairs. The experimental results demonstrate the superiority of ourproposed MVFlow, which can reduce the AEPE by 1.09 compared to existing modelsor save 52% time to achieve similar accuracy to existing models.", "output": "MVFlow: Deep Optical Flow Estimation of Compressed Videos with Motion Vector Prior."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Source-free domain adaptation (SFDA) aims to adapt a well-trained sourcemodel to an unlabelled target domain without accessing the source dataset,making it applicable in a variety of real-world scenarios. Existing SFDAmethods ONLY assess their adapted models on the target training set, neglectingthe data from unseen but identically distributed testing sets. This oversightleads to overfitting issues and constrains the model's generalization ability.In this paper, we propose a consistency regularization framework to develop amore generalizable SFDA method, which simultaneously boosts model performanceon both target training and testing datasets. Our method leverages softpseudo-labels generated from weakly augmented images to supervise stronglyaugmented images, facilitating the model training process and enhancing thegeneralization ability of the adapted model. To leverage more potentiallyuseful supervision, we present a sampling-based pseudo-label selectionstrategy, taking samples with severer domain shift into consideration.Moreover, global-oriented calibration methods are introduced to exploit globalclass distribution and feature cluster information, further improving theadaptation process. Extensive experiments demonstrate our method achievesstate-of-the-art performance on several SFDA benchmarks, and exhibitsrobustness on unseen testing datasets.", "output": "Consistency Regularization for Generalizable Source-free Domain Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Electron microscopy (EM) images exhibit anisotropic axial resolution due tothe characteristics inherent to the imaging modality, presenting challenges inanalysis and downstream tasks.In this paper, we propose a diffusion-model-basedframework that overcomes the limitations of requiring reference data or priorknowledge about the degradation process. Our approach utilizes 2D diffusionmodels to consistently reconstruct 3D volumes and is well-suited for highlydownsampled data. Extensive experiments conducted on two public datasetsdemonstrate the robustness and superiority of leveraging the generative priorcompared to supervised learning methods. Additionally, we demonstrate ourmethod's feasibility for self-supervised reconstruction, which can restore asingle anisotropic volume without any training data.", "output": "Reference-Free Isotropic 3D EM Reconstruction using Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Herbal plants are nutritious plants that can be used as an alternative totraditional disease healing. In Indonesia there are various types of herbalplants. But with the development of the times, the existence of herbal plantsas traditional medicines began to be forgotten so that not everyone couldrecognize them. Having the ability to identify herbal plants can have manypositive impacts. However, there is a problem where identifying plants can takea long time because it requires in-depth knowledge and careful examination ofplant criteria. So that the application of computer vision can help identifyherbal plants. Previously, research had been conducted on the introduction ofherbal plants from Vietnam using several algorithms, but from these researchthe accuracy was not high enough. Therefore, this study intends to implementtransfer learning from the Convolutional Neural Network (CNN) algorithm toclassify types of herbal plants from Indonesia. This research was conducted bycollecting image data of herbal plants from Indonesia independently through theGoogle Images search engine. After that, it will go through the datapreprocessing, classification using the transfer learning method from CNN, andanalysis will be carried out. The CNN transfer learning models used areResNet34, DenseNet121, and VGG11_bn. Based on the test results of the threemodels, it was found that DenseNet121 was the model with the highest accuracy,which was 87.4%. In addition, testing was also carried out using the scratchmodel and obtained an accuracy of 43.53%. The Hyperparameter configuration usedin this test is the ExponentialLR scheduler with a gamma value of 0.9; learningrate 0.001; Cross Entropy Loss function; Adam optimizer; and the number ofepochs is 50. Indonesia Medicinal Plant Dataset can be accessed at thefollowing link ", "output": "IndoHerb: Indonesia Medicinal Plants Recognition using Transfer Learning and Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a pipeline for realistic embedding of virtual objects into footageof indoor scenes with focus on real-time AR applications. Our pipeline consistsof two main components: A light estimator and a neural soft shadow texturegenerator. Our light estimation is based on deep neural nets and determines themain light direction, light color, ambient color and an opacity parameter forthe shadow texture. Our neural soft shadow method encodes object-basedrealistic soft shadows as light direction dependent textures in a small MLP. Weshow that our pipeline can be used to integrate objects into AR scenes in a newlevel of realism in real-time. Our models are small enough to run on currentmobile devices. We achieve runtimes of 9ms for light estimation and 5ms forneural shadows on an iPhone 11 Pro.", "output": "Real-time Light Estimation and Neural Soft Shadows for AR Indoor Scenarios."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the advancement of DNNs into safety-critical applications, testingapproaches for such models have gained more attention. A current direction isthe search for and identification of systematic weaknesses that put safetyassumptions based on average performance values at risk. Such weaknesses cantake on the form of (semantically coherent) subsets or areas in the input spacewhere a DNN performs systematically worse than its expected average. However,it is non-trivial to attribute the reason for such observed low performances tothe specific semantic features that describe the subset. For instance,inhomogeneities within the data w.r.t. other (non-considered) attributes mightdistort results. However, taking into account all (available) attributes andtheir interaction is often computationally highly expensive. Inspired bycounterfactual explanations, we propose an effective and computationally cheapalgorithm to validate the semantic attribution of existing subsets, i.e., tocheck whether the identified attribute is likely to have caused the degradedperformance. We demonstrate this approach on an example from the autonomousdriving domain using highly annotated simulated data, where we show for asemantic segmentation model that (i) performance differences among thedifferent pedestrian assets exist, but (ii) only in some cases is the assettype itself the reason for this reduction in the performance.", "output": "Assessing Systematic Weaknesses of DNNs using Counterfactuals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Spatio-temporal action detection (STAD) aims to classify the actions presentin a video and localize them in space and time. It has become a particularlyactive area of research in computer vision because of its explosively emergingreal-world applications, such as autonomous driving, visual surveillance,entertainment, etc. Many efforts have been devoted in recent years to buildinga robust and effective framework for STAD. This paper provides a comprehensivereview of the state-of-the-art deep learning-based methods for STAD. Firstly, ataxonomy is developed to organize these methods. Next, the linking algorithms,which aim to associate the frame- or clip-level detection results together toform action tubes, are reviewed. Then, the commonly used benchmark datasets andevaluation metrics are introduced, and the performance of state-of-the-artmodels is compared. At last, this paper is concluded, and a set of potentialresearch directions of STAD are discussed.", "output": "A Survey on Deep Learning-based Spatio-temporal Action Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces a new Convolutional Neural Network (ConvNet)architecture inspired by a class of partial differential equations (PDEs)called quasi-linear hyperbolic systems. With comparable performance on imageclassification task, it allows for the modification of the weights via acontinuous group of symmetry. This is a significant shift from traditionalmodels where the architecture and weights are essentially fixed. We wish topromote the (internal) symmetry as a new desirable property for a neuralnetwork, and to draw attention to the PDE perspective in analyzing andinterpreting ConvNets in the broader Deep Learning community.", "output": "A Novel Convolutional Neural Network Architecture with a Continuous Symmetry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, dominant Multi-object tracking (MOT) and segmentation (MOTS)methods mainly follow the tracking-by-detection paradigm. Transformer-basedend-to-end (E2E) solutions bring some ideas to MOT and MOTS, but they cannotachieve a new state-of-the-art (SOTA) performance in major MOT and MOTSbenchmarks. Detection and association are two main modules of thetracking-by-detection paradigm. Association techniques mainly depend on thecombination of motion and appearance information. As deep learning has beenrecently developed, the performance of the detection and appearance model israpidly improved. These trends made us consider whether we can achieve SOTAbased on only high-performance detection and appearance model. Our paper mainlyfocuses on exploring this direction based on CBNetV2 with Swin-B as a detectionmodel and MoCo-v2 as a self-supervised appearance model. Motion information andIoU mapping were removed during the association. Our method wins 1st place onthe MOTS track and wins 2nd on the MOT track in the CVPR2023 WAD workshop. Wehope our simple and effective method can give some insights to the MOT and MOTSresearch community. Source code will be released under this git repository", "output": "ReIDTrack: Multi-Object Track and Segmentation Without Motion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An attractive book cover is important for the success of a book. In thispaper, we apply Generative Adversarial Networks (GANs) to the book coversdomain, using different methods for training in order to obtain bettergenerated images. We interleave GANs with knowledge graphs to alter the inputtitle to obtain multiple possible options for any given title, which are thenused as an augmented input to the generator. Finally, we use the discriminatorobtained during the training phase to select the best images generated with newtitles. Our method performed better at generating book covers than previousattempts, and the knowledge graph gives better options to the book author oreditor compared to using GANs alone.", "output": "Interleaving GANs with knowledge graphs to support design creativity for book covers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, many breakthroughs are made in the field of Video Object Detection(VOD), but the performance is still limited due to the imaging limitations ofRGB sensors in adverse illumination conditions. To alleviate this issue, thiswork introduces a new computer vision task called RGB-thermal (RGBT) VOD byintroducing the thermal modality that is insensitive to adverse illuminationconditions. To promote the research and development of RGBT VOD, we design anovel Erasure-based Interaction Network (EINet) and establish a comprehensivebenchmark dataset (VT-VOD50) for this task. Traditional VOD methods oftenleverage temporal information by using many auxiliary frames, and thus havelarge computational burden. Considering that thermal images exhibit less noisethan RGB ones, we develop a negative activation function that is used to erasethe noise of RGB features with the help of thermal image features. Furthermore,with the benefits from thermal images, we rely only on a small temporal windowto model the spatio-temporal information to greatly improve efficiency whilemaintaining detection accuracy.VT-VOD50 dataset consists of 50 pairs of challenging RGBT video sequenceswith complex backgrounds, various objects and different illuminations, whichare collected in real traffic scenarios. Extensive experiments on VT-VOD50dataset demonstrate the effectiveness and efficiency of our proposed methodagainst existing mainstream VOD methods. The code of EINet and the dataset willbe released to the public for free academic usage.", "output": "Erasure-based Interaction Network for RGBT Video Object Detection and A Unified Benchmark."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-view (or -modality) representation learning aims to understand therelationships between different view representations. Existing methodsdisentangle multi-view representations into consistent and view-specificrepresentations by introducing strong inductive biases, which can limit theirgeneralization ability. In this paper, we propose a novel multi-viewrepresentation disentangling method that aims to go beyond inductive biases,ensuring both interpretability and generalizability of the resultingrepresentations. Our method is based on the observation that discoveringmulti-view consistency in advance can determine the disentangling informationboundary, leading to a decoupled learning objective. We also found that theconsistency can be easily extracted by maximizing the transformation invarianceand clustering consistency between views. These observations drive us topropose a two-stage framework. In the first stage, we obtain multi-viewconsistency by training a consistent encoder to produce semantically-consistentrepresentations across views as well as their corresponding pseudo-labels. Inthe second stage, we disentangle specificity from comprehensive representationsby minimizing the upper bound of mutual information between consistent andcomprehensive representations. Finally, we reconstruct the original data byconcatenating pseudo-labels and view-specific representations. Our experimentson four multi-view datasets demonstrate that our proposed method outperforms 12comparison methods in terms of clustering and classification performance. Thevisualization results also show that the extracted consistency and specificityare compact and interpretable. Our code can be found aturl{", "output": "Disentangling Multi-view Representations Beyond Inductive Bias."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Electrocardiogram (ECG) is a widely used diagnostic tool for detecting heartconditions. Rare cardiac diseases may be underdiagnosed using traditional ECGanalysis, considering that no training dataset can exhaust all possible cardiacdisorders. This paper proposes using anomaly detection to identify anyunhealthy status, with normal ECGs solely for training. However, detectinganomalies in ECG can be challenging due to significant inter-individualdifferences and anomalies present in both global rhythm and local morphology.To address this challenge, this paper introduces a novel multi-scalecross-restoration framework for ECG anomaly detection and localization thatconsiders both local and global ECG characteristics. The proposed frameworkemploys a two-branch autoencoder to facilitate multi-scale feature learningthrough a masking and restoration process, with one branch focusing on globalfeatures from the entire ECG and the other on local features fromheartbeat-level details, mimicking the diagnostic process of cardiologists.Anomalies are identified by their high restoration errors. To evaluate theperformance on a large number of individuals, this paper introduces a newchallenging benchmark with signal point-level ground truths annotated byexperienced cardiologists. The proposed method demonstrates state-of-the-artperformance on this benchmark and two other well-known ECG datasets. Thebenchmark dataset and source code are available at:url{", "output": "Multi-scale Cross-restoration Framework for Electrocardiogram Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent data-driven image colorization methods have enabled automatic orreference-based colorization, while still suffering from unsatisfactory andinaccurate object-level color control. To address these issues, we propose anew method called DiffColor that leverages the power of pre-trained diffusionmodels to recover vivid colors conditioned on a prompt text, without anyadditional inputs. DiffColor mainly contains two stages: colorization withgenerative color prior and in-context controllable colorization. Specifically,we first fine-tune a pre-trained text-to-image model to generate colorizedimages using a CLIP-based contrastive loss. Then we try to obtain an optimizedtext embedding aligning the colorized image and the text prompt, and afine-tuned diffusion model enabling high-quality image reconstruction. Ourmethod can produce vivid and diverse colors with a few iterations, and keep thestructure and background intact while having colors well-aligned with thetarget language guidance. Moreover, our method allows for in-contextcolorization, i.e., producing different colorization results by modifyingprompt texts without any fine-tuning, and can achieve object-level controllablecolorization results. Extensive experiments and user studies demonstrate thatDiffColor outperforms previous works in terms of visual quality, colorfidelity, and diversity of colorization options.", "output": "DiffColor: Toward High Fidelity Text-Guided Image Colorization with Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Using synthesized images to boost the performance of perception models is along-standing research challenge in computer vision. It becomes more eminent invisual-centric autonomous driving systems with multi-view cameras as somelong-tail scenarios can never be collected. Guided by the BEV segmentationlayouts, the existing generative networks seem to synthesize photo-realisticstreet-view images when evaluated solely on scene-level metrics. However, oncezoom-in, they usually fail to produce accurate foreground and backgrounddetails such as heading. To this end, we propose a two-stage generative method,dubbed BEVControl, that can generate accurate foreground and backgroundcontents. In contrast to segmentation-like input, it also supports sketch styleinput, which is more flexible for humans to edit. In addition, we propose acomprehensive multi-level evaluation protocol to fairly compare the quality ofthe generated scene, foreground object, and background geometry. Our extensiveexperiments show that our BEVControl surpasses the state-of-the-art method,BEVGen, by a significant margin, from 5.89 to 26.80 on foreground segmentationmIoU. In addition, we show that using images generated by BEVControl to trainthe downstream perception model, it achieves on average 1.29 improvement in NDSscore.", "output": "BEVControl: Accurately Controlling Street-view Elements with Multi-perspective Consistency via BEV Sketch Layout."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D panoptic segmentation is a challenging perception task that requires bothsemantic segmentation and instance segmentation. In this task, we notice thatimages could provide rich texture, color, and discriminative information, whichcan complement LiDAR data for evident performance improvement, but their fusionremains a challenging problem. To this end, we propose LCPS, the firstLiDAR-Camera Panoptic Segmentation network. In our approach, we conductLiDAR-Camera fusion in three stages: 1) an Asynchronous Compensation PixelAlignment (ACPA) module that calibrates the coordinate misalignment caused byasynchronous problems between sensors; 2) a Semantic-Aware Region Alignment(SARA) module that extends the one-to-one point-pixel mapping to one-to-manysemantic relations; 3) a Point-to-Voxel feature Propagation (PVP) module thatintegrates both geometric and semantic fusion information for the entire pointcloud. Our fusion strategy improves about 6.9% PQ performance over theLiDAR-only baseline on NuScenes dataset. Extensive quantitative and qualitativeexperiments further demonstrate the effectiveness of our novel framework. Thecode will be released at ", "output": "LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Class incremental learning (CIL) aims to incrementally update a trained modelwith the new classes of samples (plasticity) while retaining previously learnedability (stability). To address the most challenging issue in this goal, i.e.,catastrophic forgetting, the mainstream paradigm is memory-replay CIL, whichconsolidates old knowledge by replaying a small number of old classes ofsamples saved in the memory. Despite effectiveness, the inherentdestruction-reconstruction dynamics in memory-replay CIL are an intrinsiclimitation: if the old knowledge is severely destructed, it will be quite hardto reconstruct the lossless counterpart. Our theoretical analysis shows thatthe destruction of old knowledge can be effectively alleviated by balancing thecontribution of samples from the current phase and those saved in the memory.Motivated by this theoretical finding, we propose a novel BalancedDestruction-Reconstruction module (BDR) for memory-replay CIL, which canachieve better knowledge reconstruction by reducing the degree of maximaldestruction of old knowledge. Specifically, to achieve a better balance betweenold knowledge and new classes, the proposed BDR module takes into account twofactors: the variance in training status across different classes and thequantity imbalance of samples from the current phase and memory. By dynamicallymanipulating the gradient during training based on these factors, BDR caneffectively alleviate knowledge destruction and improve knowledgereconstruction. Extensive experiments on a range of CIL benchmarks have shownthat as a lightweight plug-and-play module, BDR can significantly improve theperformance of existing state-of-the-art methods with good generalization.", "output": "Balanced Destruction-Reconstruction Dynamics for Memory-replay Class Incremental Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Feature selection could be defined as an optimization problem and solved bybio-inspired algorithms. Bees Algorithm (BA) shows decent performance infeature selection optimization tasks. On the other hand, Local PhaseQuantization (LPQ) is a frequency domain feature which has excellentperformance on Depth images. Here, after extracting LPQ features out of RGB(colour) and Depth images from the Iranian Kinect Face Database (IKFDB), theBees feature selection algorithm applies to select the desired number offeatures for final classification tasks. IKFDB is recorded with Kinect sensorV.2 and contains colour and depth images for facial and facialmicro-expressions recognition purposes. Here five facial expressions of Anger,Joy, Surprise, Disgust and Fear are used for final validation. The proposedBees LPQ method is compared with Particle Swarm Optimization (PSO) LPQ, PCALPQ, Lasso LPQ, and just LPQ features for classification tasks with SupportVector Machines (SVM), K-Nearest Neighbourhood (KNN), Shallow Neural Networkand Ensemble Subspace KNN. Returned results, show a decent performance of theproposed algorithm (99 % accuracy) in comparison with others.", "output": "Bees Local Phase Quantization Feature Selection for RGB-D Facial Expressions Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D semantic scene understanding tasks have achieved great success with theemergence of deep learning, but often require a huge amount of manuallyannotated training data. To alleviate the annotation cost, we propose the firstweakly-supervised 3D instance segmentation method that only requirescategorical semantic labels as supervision, and we do not need instance-levellabels. The required semantic annotations can be either dense or extreme sparse(e.g. 0.02% of total points). Even without having any instance-relatedground-truth, we design an approach to break point clouds into raw fragmentsand find the most confident samples for learning instance centroids.Furthermore, we construct a recomposed dataset using pseudo instances, which isused to learn our defined multilevel shape-aware objectness signal. Anasymmetrical object inference algorithm is followed to process core points andboundary points with different strategies, and generate high-quality pseudoinstance labels to guide iterative training. Experiments demonstrate that ourmethod can achieve comparable results with recent fully supervised methods. Bygenerating pseudo instance labels from categorical semantic labels, ourdesigned approach can also assist existing methods for learning 3D instancesegmentation at reduced annotation cost.", "output": "Weakly Supervised 3D Instance Segmentation without Instance-level Annotations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predictive variability due to data ambiguities has typically been addressedvia construction of dedicated models with built-in probabilistic capabilitiesthat are trained to predict uncertainty estimates as variables of interest.These approaches require distinct architectural components and trainingmechanisms, may include restrictive assumptions and exhibit overconfidence,i.e., high confidence in imprecise predictions. In this work, we propose apost-hoc sampling strategy for estimating predictive uncertainty accounting fordata ambiguity. The method can generate different plausible outputs for a giveninput and does not assume parametric forms of predictive distributions. It isarchitecture agnostic and can be applied to any feed-forward deterministicnetwork without changes to the architecture or training procedure. Experimentson regression tasks on imaging and non-imaging input data show the method'sability to generate diverse and multi-modal predictive distributions, and adesirable correlation of the estimated uncertainty with the prediction error.", "output": "Quantification of Predictive Uncertainty via Inference-Time Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visibility in hazy nighttime scenes is frequently reduced by multiplefactors, including low light, intense glow, light scattering, and the presenceof multicolored light sources. Existing nighttime dehazing methods oftenstruggle with handling glow or low-light conditions, resulting in eitherexcessively dark visuals or unsuppressed glow outputs. In this paper, weenhance the visibility from a single nighttime haze image by suppressing glowand enhancing low-light regions. To handle glow effects, our framework learnsfrom the rendered glow pairs. Specifically, a light source aware network isproposed to detect light sources of night images, followed by the APSF (AngularPoint Spread Function)-guided glow rendering. Our framework is then trained onthe rendered images, resulting in glow suppression. Moreover, we utilizegradient-adaptive convolution, to capture edges and textures in hazy scenes. Byleveraging extracted edges and textures, we enhance the contrast of the scenewithout losing important structural details. To boost low-light intensity, ournetwork learns an attention map, then adjusted by gamma correction. Thisattention has high values on low-light regions and low values on haze and glowregions. Extensive evaluation on real nighttime haze images, demonstrates theeffectiveness of our method. Our experiments demonstrate that our methodachieves a PSNR of 30.72dB, outperforming state-of-the-art methods by 14$%$ onGTA5 nighttime haze dataset. Our data and code is available at:url{", "output": "Enhancing Visibility in Nighttime Haze Images Using Guided APSF and Gradient Adaptive Convolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How to enable learnability for new classes while keeping the capability wellon old classes has been a crucial challenge for class incremental learning.Beyond the normal case, long-tail class incremental learning and few-shot classincremental learning are also proposed to consider the data imbalance and datascarcity, respectively, which are common in real-world implementations andfurther exacerbate the well-known problem of catastrophic forgetting. Existingmethods are specifically proposed for one of the three tasks. In this paper, weoffer a unified solution to the misalignment dilemma in the three tasks.Concretely, we propose neural collapse terminus that is a fixed structure withthe maximal equiangular inter-class separation for the whole label space. Itserves as a consistent target throughout the incremental training to avoiddividing the feature space incrementally. For CIL and LTCIL, we further proposea prototype evolving scheme to drive the backbone features into our neuralcollapse terminus smoothly. Our method also works for FSCIL with only minoradaptations. Theoretical analysis indicates that our method holds the neuralcollapse optimality in an incremental fashion regardless of data imbalance ordata scarcity. We also design a generalized case where we do not know the totalnumber of classes and whether the data distribution is normal, long-tail, orfew-shot for each coming session, to test the generalizability of our method.Extensive experiments with multiple datasets are conducted to demonstrate theeffectiveness of our unified solution to all the three tasks and thegeneralized case.", "output": "Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In computational pathology, automatic nuclei instance segmentation plays anessential role in whole slide image analysis. While many computerizedapproaches have been proposed for this task, supervised deep learning (DL)methods have shown superior segmentation performances compared to classicalmachine learning and image processing techniques. However, these models needfully annotated datasets for training which is challenging to acquire,especially in the medical domain. In this work, we release one of the biggestfully manually annotated datasets of nuclei in Hematoxylin and Eosin(H&amp;E)-stained histological images, called NuInsSeg. This dataset contains 665image patches with more than 30,000 manually segmented nuclei from 31 human andmouse organs. Moreover, for the first time, we provide additional ambiguousarea masks for the entire dataset. These vague areas represent the parts of theimages where precise and deterministic manual annotations are impossible, evenfor human experts. The dataset and detailed step-by-step instructions togenerate related segmentation masks are publicly available at and respectively.", "output": "NuInsSeg: A Fully Annotated Dataset for Nuclei Instance Segmentation in H&E-Stained Histological Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce PoissonNet, an architecture for shape reconstruction thataddresses the challenge of recovering 3D shapes from points. Traditional deepneural networks face challenges with common 3D shape discretization techniquesdue to their computational complexity at higher resolutions. To overcome this,we leverage Fourier Neural Operators (FNOs) to solve the Poisson equation andreconstruct a mesh from oriented point cloud measurements. PoissonNet exhibitstwo main advantages. First, it enables efficient training on low-resolutiondata while achieving comparable performance at high-resolution evaluation,thanks to the resolution-agnostic nature of FNOs. This feature allows forone-shot super-resolution. Second, our method surpasses existing approaches inreconstruction quality while being differentiable. Overall, our proposed methodnot only improves upon the limitations of classical deep neural networks inshape reconstruction but also achieves superior results in terms ofreconstruction quality, running time, and resolution flexibility. Furthermore,we demonstrate that the Poisson surface reconstruction problem is well-posed inthe limit case by showing a universal approximation theorem for the solutionoperator of the Poisson equation with distributional data utilizing the FourierNeuronal Operator, which provides a theoretical foundation for our numericalresults. The code to reproduce the experiments is available on:url{", "output": "PoissonNet: Resolution-Agnostic 3D Shape Reconstruction using Fourier Neural Operators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tensor decompositions are powerful tools for analyzing multi-dimensional datain their original format. Besides tensor decompositions like Tucker and CP,Tensor SVD (t-SVD) which is based on the t-product of tensors is anotherextension of SVD to tensors that recently developed and has found numerousapplications in analyzing high dimensional data. This paper offers a newinsight into the t-Product and shows that this product is a block convolutionof two tensors with periodic boundary conditions. Based on this viewpoint, wepropose a new tensor-tensor product called the $star_c{}text{-Product}$ basedon Block convolution with reflective boundary conditions. Using a tensorframework, this product can be easily extended to tensors of arbitrary order.Additionally, we introduce a tensor decomposition based on our$star_c{}text{-Product}$ for arbitrary order tensors. Compared to t-SVD, ournew decomposition has lower complexity, and experiments show that it yieldshigher-quality results in applications such as classification and compression.", "output": "A Novel Tensor Decomposition of arbitrary order based on Block Convolution with Reflective Boundary Conditions for Multi-Dimensional Data Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Annotating nuclei in microscopy images for the training of neural networks isa laborious task that requires expert knowledge and suffers from inter- andintra-rater variability, especially in fluorescence microscopy. Generativenetworks such as CycleGAN can inverse the process and generate syntheticmicroscopy images for a given mask, thereby building a synthetic dataset.However, past works report content inconsistencies between the mask andgenerated image, partially due to CycleGAN minimizing its loss by hidingshortcut information for the image reconstruction in high frequencies ratherthan encoding the desired image content and learning the target task. In thiswork, we propose to remove the hidden shortcut information, calledsteganography, from generated images by employing a low pass filtering based onthe DCT. We show that this increases coherence between generated images andcycled masks and evaluate synthetic datasets on a downstream nucleisegmentation task. Here we achieve an improvement of 5.4 percentage points inthe F1-score compared to a vanilla CycleGAN. Integrating advancedregularization techniques into the CycleGAN architecture may help mitigatesteganography-related issues and produce more accurate synthetic datasets fornuclei segmentation.", "output": "Focus on Content not Noise: Improving Image Generation for Nuclei Segmentation by Suppressing Steganography in CycleGAN."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study investigated the potential of end-to-end deep learning tools as amore effective substitute for FEM in predicting stress-strain fields within 2Dcross sections of arterial wall. We first proposed a U-Net based fullyconvolutional neural network (CNN) to predict the von Mises stress and straindistribution based on the spatial arrangement of calcification within arterialwall cross-sections. Further, we developed a conditional generative adversarialnetwork (cGAN) to enhance, particularly from the perceptual perspective, theprediction accuracy of stress and strain field maps for arterial walls withvarious calcification quantities and spatial configurations. On top of U-Netand cGAN, we also proposed their ensemble approaches, respectively, to furtherimprove the prediction accuracy of field maps. Our dataset, consisting of inputand output images, was generated by implementing boundary conditions andextracting stress-strain field maps. The trained U-Net models can accuratelypredict von Mises stress and strain fields, with structural similarity indexscores (SSIM) of 0.854 and 0.830 and mean squared errors of 0.017 and 0.018 forstress and strain, respectively, on a reserved test set. Meanwhile, the cGANmodels in a combination of ensemble and transfer learning techniquesdemonstrate high accuracy in predicting von Mises stress and strain fields, asevidenced by SSIM scores of 0.890 for stress and 0.803 for strain.Additionally, mean squared errors of 0.008 for stress and 0.017 for strainfurther support the model's performance on a designated test set. Overall, thisstudy developed a surrogate model for finite element analysis, which canaccurately and efficiently predict stress-strain fields of arterial wallsregardless of complex geometries and boundary conditions.", "output": "Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weakly-supervised image segmentation has recently attracted increasingresearch attentions, aiming to avoid the expensive pixel-wise labeling. In thispaper, we present an effective method, namely Point2Mask, to achievehigh-quality panoptic prediction using only a single random point annotationper target for training. Specifically, we formulate the panoptic pseudo-maskgeneration as an Optimal Transport (OT) problem, where each ground-truth (gt)point label and pixel sample are defined as the label supplier and consumer,respectively. The transportation cost is calculated by the introducedtask-oriented maps, which focus on the category-wise and instance-wisedifferences among the various thing and stuff targets. Furthermore, acentroid-based scheme is proposed to set the accurate unit number for each gtpoint supplier. Hence, the pseudo-mask generation is converted into finding theoptimal transport plan at a globally minimal transportation cost, which can besolved via the Sinkhorn-Knopp Iteration. Experimental results on Pascal VOC andCOCO demonstrate the promising performance of our proposed Point2Mask approachto point-supervised panoptic segmentation. Source code is available at:", "output": "Point2Mask: Point-supervised Panoptic Segmentation via Optimal Transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cooperative perception can effectively enhance individual perceptionperformance by providing additional viewpoint and expanding the sensing field.Existing cooperation paradigms are either interpretable (result cooperation) orflexible (feature cooperation). In this paper, we propose the concept of querycooperation to enable interpretable instance-level flexible featureinteraction. To specifically explain the concept, we propose a cooperativeperception framework, termed QUEST, which let query stream flow among agents.The cross-agent queries are interacted via fusion for co-aware instances andcomplementation for individual unaware instances. Taking camera-basedvehicle-infrastructure perception as a typical practical application scene, theexperimental results on the real-world dataset, DAIR-V2X-Seq, demonstrate theeffectiveness of QUEST and further reveal the advantage of the querycooperation paradigm on transmission flexibility and robustness to packetdropout. We hope our work can further facilitate the cross-agent representationinteraction for better cooperative perception in practice.", "output": "QUEST: Query Stream for Vehicle-Infrastructure Cooperative Perception."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dietary assessment is a key contributor to monitoring health status. Existingself-report methods are tedious and time-consuming with substantial biases anderrors. Image-based food portion estimation aims to estimate food energy valuesdirectly from food images, showing great potential for automated dietaryassessment solutions. Existing image-based methods either use a single-viewimage or incorporate multi-view images and depth information to estimate thefood energy, which either has limited performance or creates user burdens. Inthis paper, we propose an end-to-end deep learning framework for food energyestimation from a monocular image through 3D shape reconstruction. We leveragea generative model to reconstruct the voxel representation of the food objectfrom the input image to recover the missing 3D information. Our method isevaluated on a publicly available food image dataset Nutrition5k, resulting aMean Absolute Error (MAE) of 40.05 kCal and Mean Absolute Percentage Error(MAPE) of 11.47% for food energy estimation. Our method uses RGB image as theonly input at the inference stage and achieves competitive results compared tothe existing method requiring both RGB and depth information.", "output": "An End-to-end Food Portion Estimation Framework Based on Shape Reconstruction from Monocular Image."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fine-grained image classification (FGIC) is a challenging task in computervision for due to small visual differences among inter-subcategories, but,large intra-class variations. Deep learning methods have achieved remarkablesuccess in solving FGIC. In this paper, we propose a fusion approach to addressFGIC by combining global texture with local patch-based information. The firstpipeline extracts deep features from various fixed-size non-overlapping patchesand encodes features by sequential modelling using the long short-term memory(LSTM). Another path computes image-level textures at multiple scales using thelocal binary patterns (LBP). The advantages of both streams are integrated torepresent an efficient feature vector for image classification. The method istested on eight datasets representing the human faces, skin lesions, fooddishes, marine lives, etc. using four standard backbone CNNs. Our method hasattained better classification accuracy over existing methods with notablemargins.", "output": "Deep Neural Networks Fused with Textures for Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Single-cell data integration can provide a comprehensive molecular view ofcells, and many algorithms have been developed to remove unwanted technical orbiological variations and integrate heterogeneous single-cell datasets. Despitetheir wide usage, existing methods suffer from several fundamental limitations.In particular, we lack a rigorous statistical test for whether twohigh-dimensional single-cell datasets are alignable (and therefore should evenbe aligned). Moreover, popular methods can substantially distort the dataduring alignment, making the aligned data and downstream analysis difficult tointerpret. To overcome these limitations, we present a spectral manifoldalignment and inference (SMAI) framework, which enables principled andinterpretable alignability testing and structure-preserving integration ofsingle-cell data. SMAI provides a statistical test to robustly determine thealignability between datasets to avoid misleading inference, and is justifiedby high-dimensional statistical theory. On a diverse range of real andsimulated benchmark datasets, it outperforms commonly used alignment methods.Moreover, we show that SMAI improves various downstream analyses such asidentification of differentially expressed genes and imputation of single-cellspatial transcriptomics, providing further biological insights. SMAI'sinterpretability also enables quantification and a deeper understanding of thesources of technical confounders in single-cell data.", "output": "Is your data alignable? Principled and interpretable alignability testing and integration of single-cell data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-motion generation has gained increasing attention, but most existingmethods are limited to generating short-term motions that correspond to asingle sentence describing a single action. However, when a text streamdescribes a sequence of continuous motions, the generated motions correspondingto each sentence may not be coherently linked. Existing long-term motiongeneration methods face two main issues. Firstly, they cannot directly generatecoherent motions and require additional operations such as interpolation toprocess the generated actions. Secondly, they generate subsequent actions in anautoregressive manner without considering the influence of future actions onprevious ones. To address these issues, we propose a novel approach thatutilizes a past-conditioned diffusion model with two optional coherent samplingmethods: Past Inpainting Sampling and Compositional Transition Sampling. PastInpainting Sampling completes subsequent motions by treating previous motionsas conditions, while Compositional Transition Sampling models the distributionof the transition as the composition of two adjacent motions guided bydifferent text prompts. Our experimental results demonstrate that our proposedmethod is capable of generating compositional and coherent long-term 3D humanmotions controlled by a user-instructed long text stream. The code is availableathref{", "output": "Synthesizing Long-Term Human Motions with Diffusion Models via Coherent Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding 3d human interactions is fundamental for fine-grained sceneanalysis and behavioural modeling. However, most of the existing models predictincorrect, lifeless 3d estimates, that miss the subtle human contactaspects--the essence of the event--and are of little use for detailedbehavioral understanding. This paper addresses such issues with severalcontributions: (1) we introduce models for interaction signature estimation(ISP) encompassing contact detection, segmentation, and 3d contact signatureprediction; (2) we show how such components can be leveraged to ensure contactconsistency during 3d reconstruction; (3) we construct several large datasetsfor learning and evaluating 3d contact prediction and reconstruction methods;specifically, we introduce CHI3D, a lab-based accurate 3d motion capturedataset with 631 sequences containing $2,525$ contact events, $728,664$ groundtruth 3d poses, as well as FlickrCI3D, a dataset of $11,216$ images, with$14,081$ processed pairs of people, and $81,233$ facet-level surfacecorrespondences. Finally, (4) we propose methodology for recovering theground-truth pose and shape of interacting people in a controlled setup and (5)annotate all 3d interaction motions in CHI3D with textual descriptions. Motiondata in multiple formats (GHUM and SMPLX parameters, Human3.6m 3d joints) ismade available for research purposes at url{ togetherwith an evaluation server and a public benchmark.", "output": "Reconstructing Three-Dimensional Models of Interacting Humans."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU,DPU), deploying deep learning models on edge devices with fixed-point hardwareis still challenging due to complex model quantization and conversion. Existingmodel quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], andQualcomm AIMET [3] supports only a limited set of quantization schemes (e.g.,only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deeplearning models cannot be easily quantized for diverse fixed-point hardwares,mainly due to slightly different quantization requirements. In this paper, weenvision a new type of model quantization approach called MRQ (modelre-quantization), which takes existing quantized models and quickly transformsthe models to meet different quantization requirements (e.g., asymmetric -&gt;symmetric, non-power-of-2 scale -&gt; power-of-2 scale). Re-quantization is muchsimpler than quantizing from scratch because it avoids costly re-training andprovides support for multiple quantization schemes simultaneously. To minimizere-quantization error, we developed a new set of re-quantization algorithmsincluding weight correction and rounding error folding. We have demonstratedthat MobileNetV2 QAT model [7] can be quickly re-quantized into two differentquantization schemes (i.e., symmetric and symmetric+power-of-2 scale) with lessthan 0.64 units of accuracy loss. We believe our work is the first to leveragethis concept of re-quantization for model quantization and models obtained fromthe re-quantization process have been successfully deployed on NNA in the EchoShow devices.", "output": "MRQ:Support Multiple Quantization Schemes through Model Re-Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object detection is a vital task in computer vision and has become anintegral component of numerous critical systems. However, state-of-the-artobject detectors, similar to their classification counterparts, are susceptibleto small adversarial perturbations that can significantly alter their normalbehavior. Unlike classification, the robustness of object detectors has notbeen thoroughly explored. In this work, we take the initial step towardsbridging the gap between the robustness of classification and object detectionby leveraging adversarially trained classification models. Merely utilizingadversarially trained models as backbones for object detection does not resultin robustness. We propose effective modifications to the classification-basedbackbone to instill robustness in object detection without incurring anycomputational overhead. To further enhance the robustness achieved by theproposed modified backbone, we introduce two lightweight components: imitationloss and delayed adversarial training. Extensive experiments on the MS-COCO andPascal VOC datasets are conducted to demonstrate the effectiveness of ourproposed approach.", "output": "FROD: Robust Object Detection for Free."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-label image recognition in the low-label regime is a task of greatchallenge and practical significance. Previous works have focused on learningthe alignment between textual and visual spaces to compensate for limited imagelabels, yet may suffer from reduced accuracy due to the scarcity ofhigh-quality multi-label annotations. In this research, we leverage thepowerful alignment between textual and visual features pretrained with millionsof auxiliary image-text pairs. We introduce an efficient and effectiveframework called Evidence-guided Dual Context Optimization (DualCoOp++), whichserves as a unified approach for addressing partial-label and zero-shotmulti-label recognition. In DualCoOp++ we separately encode evidential,positive, and negative contexts for target classes as parametric components ofthe linguistic input (i.e., prompts). The evidential context aims to discoverall the related visual content for the target class, and serves as guidance toaggregate positive and negative contexts from the spatial domain of the image,enabling better distinguishment between similar categories. Additionally, weintroduce a Winner-Take-All module that promotes inter-class interaction duringtraining, while avoiding the need for extra parameters and costs. As DualCoOp++imposes minimal additional learnable overhead on the pretrained vision-languageframework, it enables rapid adaptation to multi-label recognition tasks withlimited annotations and even unseen classes. Experiments on standardmulti-label recognition benchmarks across two challenging low-label settingsdemonstrate the superior performance of our approach compared tostate-of-the-art methods.", "output": "DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rigorously testing autonomy systems is essential for making safe self-drivingvehicles (SDV) a reality. It requires one to generate safety critical scenariosbeyond what can be collected safely in the world, as many scenarios happenrarely on public roads. To accurately evaluate performance, we need to test theSDV on these scenarios in closed-loop, where the SDV and other actors interactwith each other at each timestep. Previously recorded driving logs provide arich resource to build these new scenarios from, but for closed loopevaluation, we need to modify the sensor data based on the new sceneconfiguration and the SDV's decisions, as actors might be added or removed andthe trajectories of existing actors and the SDV will differ from the originallog. In this paper, we present UniSim, a neural sensor simulator that takes asingle recorded log captured by a sensor-equipped vehicle and converts it intoa realistic closed-loop multi-sensor simulation. UniSim builds neural featuregrids to reconstruct both the static background and dynamic actors in thescene, and composites them together to simulate LiDAR and camera data at newviewpoints, with actors added or removed and at new placements. To betterhandle extrapolated views, we incorporate learnable priors for dynamic objects,and leverage a convolutional network to complete unseen regions. Ourexperiments show UniSim can simulate realistic sensor data with small domaingap on downstream tasks. With UniSim, we demonstrate closed-loop evaluation ofan autonomy system on safety-critical scenarios as if it were in the realworld.", "output": "UniSim: A Neural Closed-Loop Sensor Simulator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents an improved DETR detector that maintains a \"plain\"nature: using a single-scale feature map and global cross-attentioncalculations without specific locality constraints, in contrast to previousleading DETR-based detectors that reintroduce architectural inductive biases ofmulti-scale and locality into the decoder. We show that two simple technologiesare surprisingly effective within a plain design to compensate for the lack ofmulti-scale feature maps and locality constraints. The first is a box-to-pixelrelative position bias (BoxRPB) term added to the cross-attention formulation,which well guides each query to attend to the corresponding object region whilealso providing encoding flexibility. The second is masked image modeling(MIM)-based backbone pre-training which helps learn representation withfine-grained localization ability and proves crucial for remedying dependencieson the multi-scale feature maps. By incorporating these technologies and recentadvancements in training and problem formation, the improved \"plain\" DETRshowed exceptional improvements over the original DETR detector. By leveragingthe Object365 dataset for pre-training, it achieved 63.9 mAP accuracy using aSwin-L backbone, which is highly competitive with state-of-the-art detectorswhich all heavily rely on multi-scale feature maps and region-based featureextraction. Code is available at  .", "output": "DETR Doesn't Need Multi-Scale or Locality Design."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Depth completion, which aims to generate high-quality dense depth maps fromsparse depth maps, has attracted increasing attention in recent years. Previouswork usually employs RGB images as guidance, and introduces iterative spatialpropagation to refine estimated coarse depth maps. However, most of thepropagation refinement methods require several iterations and suffer from afixed receptive field, which may contain irrelevant and useless informationwith very sparse input. In this paper, we address these two challengessimultaneously by revisiting the idea of deformable convolution. We propose aneffective architecture that leverages deformable kernel convolution as asingle-pass refinement module, and empirically demonstrate its superiority. Tobetter understand the function of deformable convolution and exploit it fordepth completion, we further systematically investigate a variety ofrepresentative strategies. Our study reveals that, different from prior work,deformable convolution needs to be applied on an estimated depth map with arelatively high density for better performance. We evaluate our model on thelarge-scale KITTI dataset and achieve state-of-the-art level performance inboth accuracy and inference speed. Our code is available at", "output": "Revisiting Deformable Convolution for Depth Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the All-Seeing (AS) project: a large-scale data and model forrecognizing and understanding everything in the open world. Using a scalabledata engine that incorporates human feedback and efficient models in the loop,we create a new dataset (AS-1B) with over 1 billion regions annotated withsemantic tags, question-answering pairs, and detailed captions. It covers awide range of 3.5 million common and rare concepts in the real world, and has132.2 billion tokens that describe the concepts and their attributes.Leveraging this new dataset, we develop the All-Seeing model (ASM), a unifiedframework for panoptic visual recognition and understanding. The model istrained with open-ended language prompts and locations, which allows it togeneralize to various vision and language tasks with remarkable zero-shotperformance, including region-text retrieval, region recognition, captioning,and question-answering. We hope that this project can serve as a foundation forvision-language artificial general intelligence research. Models and thedataset shall be released at  and democan be seen at ", "output": "The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Albeit being a prevalent architecture searching approach, differentiablearchitecture search (DARTS) is largely hindered by its substantial memory costsince the entire supernet resides in the memory. This is where the single-pathDARTS comes in, which only chooses a single-path submodel at each step. Whilebeing memory-friendly, it also comes with low computational costs. Nonetheless,we discover a critical issue of single-path DARTS that has not been primarilynoticed. Namely, it also suffers from severe performance collapse since toomany parameter-free operations like skip connections are derived, just likeDARTS does. In this paper, we propose a new algorithm called RObustifyingMemory-Efficient NAS (ROME) to give a cure. First, we disentangle the topologysearch from the operation search to make searching and evaluation consistent.We then adopt Gumbel-Top2 reparameterization and gradient accumulation torobustify the unwieldy bi-level optimization. We verify ROME extensively across15 benchmarks to demonstrate its effectiveness and robustness.", "output": "ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present an efficient alternative to the convolutional layer using cheapspatial transformations. This construction exploits an inherent spatialredundancy of the learned convolutional filters to enable a much greaterparameter efficiency, while maintaining the top-end accuracy of their densecounter-parts. Training these networks is modelled as a generalised pruningproblem, whereby the pruned filters are replaced with cheap transformationsfrom the set of non-pruned filters. We provide an efficient implementation ofthe proposed layer, followed by two natural extensions to avoid excessivefeature compression and to improve the expressivity of the transformedfeatures. We show that these networks can achieve comparable or improvedperformance to state-of-the-art pruning models across both the CIFAR-10 andImageNet-1K datasets.", "output": "Reconstructing Pruned Filters using Cheap Spatial Transformations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning is a promising machine learning paradigm to learn newtasks while retaining previously learned knowledge over streaming trainingdata. Till now, rehearsal-based methods, keeping a small part of data from oldtasks as a memory buffer, have shown good performance in mitigatingcatastrophic forgetting for previously learned knowledge. However, most ofthese methods typically treat each new task equally, which may not adequatelyconsider the relationship or similarity between old and new tasks. Furthermore,these methods commonly neglect sample importance in the continual trainingprocess and result in sub-optimal performance on certain tasks. To address thischallenging problem, we propose Relational Experience Replay (RER), a bi-levellearning framework, to adaptively tune task-wise relationships and sampleimportance within each task to achieve a better `stability' and `plasticity'trade-off. As such, the proposed method is capable of accumulating newknowledge while consolidating previously learned old knowledge during continuallearning. Extensive experiments conducted on three publicly available datasets(i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet) show that the proposed methodcan consistently improve the performance of all baselines and surpass currentstate-of-the-art methods.", "output": "Relational Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The multi-level aggregation (MLA) module has emerged as a critical componentfor advancing new-era vision back-bones in semantic segmentation. In thispaper, we propose Lawin (large window) Transformer, a novel MLA architecturethat creatively utilizes multi-scale feature maps from the vision backbone. Atthe core of Lawin Transformer is the Lawin attention, a newly designed windowattention mechanism capable of querying much larger context windows than localwindows. We focus on studying the efficient and simplistic application of thelarge-window paradigm, allowing for flexible regulation of the ratio of largecontext to query and capturing multi-scale representations. We validate theeffectiveness of Lawin Transformer on Cityscapes and ADE20K, consistentlydemonstrating great superiority to widely-used MLA modules when combined withnew-era vision backbones. The code is available at", "output": "Lawin Transformer: Improving New-Era Vision Backbones with Multi-Scale Representations for Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cross-modal representation learning learns a shared embedding between two ormore modalities to improve performance in a given task compared to using onlyone of the modalities. Cross-modal representation learning from different datatypes -- such as images and time-series data (e.g., audio or text data) --requires a deep metric learning loss that minimizes the distance between themodality embeddings. In this paper, we propose to use the contrastive ortriplet loss, which uses positive and negative identities to create samplepairs with different labels, for cross-modal representation learning betweenimage and time-series modalities (CMR-IS). By adapting the triplet loss forcross-modal representation learning, higher accuracy in the main (time-seriesclassification) task can be achieved by exploiting additional information ofthe auxiliary (image classification) task. We present a triplet loss with adynamic margin for single label and sequence-to-sequence classification tasks.We perform extensive evaluations on synthetic image and time-series data, andon data for offline handwriting recognition (HWR) and on online HWR fromsensor-enhanced pens for classifying written words. Our experiments show animproved classification accuracy, faster convergence, and bettergeneralizability due to an improved cross-modal representation. Furthermore,the more suitable generalizability leads to a better adaptability betweenwriters for online HWR.", "output": "Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this study, we present a method for synthesizing novel views from a single360-degree RGB-D image based on the neural radiance field (NeRF) . Priorstudies relied on the neighborhood interpolation capability of multi-layerperceptrons to complete missing regions caused by occlusion and zooming, whichleads to artifacts. In the method proposed in this study, the input image isreprojected to 360-degree RGB images at other camera positions, the missingregions of the reprojected images are completed by a 2D image generative model,and the completed images are utilized to train the NeRF. Because multiplecompleted images contain inconsistencies in 3D, we introduce a method to learnthe NeRF model using a subset of completed images that cover the target scenewith less overlap of completed regions. The selection of such a subset ofimages can be attributed to the maximum weight independent set problem, whichis solved through simulated annealing. Experiments demonstrated that theproposed method can synthesize plausible novel views while preserving thefeatures of the scene for both artificial and real-world data.", "output": "Enhancement of Novel View Synthesis Using Omnidirectional Image Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the recent efforts in accurate 3D annotations in hand and objectdatasets, there still exist gaps in 3D hand and object reconstructions.Existing works leverage contact maps to refine inaccurate hand-object poseestimations and generate grasps given object models. However, they requireexplicit 3D supervision which is seldom available and therefore, are limited toconstrained settings, e.g., where thermal cameras observe residual heat left onmanipulated objects. In this paper, we propose a novel semi-supervisedframework that allows us to learn contact from monocular images. Specifically,we leverage visual and geometric consistency constraints in large-scaledatasets for generating pseudo-labels in semi-supervised learning and proposean efficient graph-based network to infer contact. Our semi-supervised learningframework achieves a favourable improvement over the existing supervisedlearning methods trained on data with `limited' annotations. Notably, ourproposed model is able to achieve superior results with less than half thenetwork parameters and memory access cost when compared with the commonly-usedPointNet-based approach. We show benefits from using a contact map that ruleshand-object interactions to produce more accurate reconstructions. We furtherdemonstrate that training with pseudo-labels can extend contact map estimationsto out-of-domain objects and generalise better across multiple datasets.", "output": "S$^2$Contact: Graph-based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual Question Answering (VQA) based on multi-modal data facilitatesreal-life applications such as home robots and medical diagnoses. Onesignificant challenge is to devise a robust decentralized learning frameworkfor various client models where centralized data collection is refrained due toconfidentiality concerns. This work aims to tackle privacy-preserving VQA bydecoupling a multi-modal model into representation modules and a contrastivemodule and leveraging inter-module gradients sharing and inter-client weightsharing. To this end, we propose Bidirectional Contrastive Split Learning(BiCSL) to train a global multi-modal model on the entire data distribution ofdecentralized clients. We employ the contrastive loss that enables a moreefficient self-supervised learning of decentralized modules. Comprehensiveexperiments are conducted on the VQA-v2 dataset based on five SOTA VQA models,demonstrating the effectiveness of the proposed method. Furthermore, we inspectBiCSL's robustness against a dual-key backdoor attack on VQA. Consequently,BiCSL shows much better robustness to the multi-modal adversarial attackcompared to the centralized learning method, which provides a promisingapproach to decentralized multi-modal learning.", "output": "Bidirectional Contrastive Split Learning for Visual Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel image dataset focused on tiny faces wearing face masks formask classification purposes, dubbed Small Face MASK (SF-MASK), composed of acollection made from 20k low-resolution images exported from diverse andheterogeneous datasets, ranging from 7 x 7 to 64 x 64 pixel resolution. Anaccurate visualization of this collection, through counting grids, made itpossible to highlight gaps in the variety of poses assumed by the heads of thepedestrians. In particular, faces filmed by very high cameras, in which thefacial features appear strongly skewed, are absent. To address this structuraldeficiency, we produced a set of synthetic images which resulted in asatisfactory covering of the intra-class variance. Furthermore, a smallsubsample of 1701 images contains badly worn face masks, opening to multi-classclassification challenges. Experiments on SF-MASK focus on face maskclassification using several classifiers. Results show that the richness ofSF-MASK (real + synthetic images) leads all of the tested classifiers toperform better than exploiting comparative face mask datasets, on a fixed 1077images testing set. Dataset and evaluation code are publicly available here:", "output": "A Masked Face Classification Benchmark on Low-Resolution Surveillance Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present TemporalStereo, a coarse-to-fine stereo matching network that ishighly efficient, and able to effectively exploit the past geometry and contextinformation to boost matching accuracy. Our network leverages sparse costvolume and proves to be effective when a single stereo pair is given. However,its peculiar ability to use spatio-temporal information across stereo sequencesallows TemporalStereo to alleviate problems such as occlusions and reflectiveregions while enjoying high efficiency also in this latter case. Notably, ourmodel -- trained once with stereo videos -- can run in both single-pair andtemporal modes seamlessly. Experiments show that our network relying on cameramotion is robust even to dynamic objects when running on videos. We validateTemporalStereo through extensive experiments on synthetic (SceneFlow,TartanAir) and real (KITTI 2012, KITTI 2015) datasets. Our model achievesstate-of-the-art performance on any of these datasets. Code is available aturl{", "output": "TemporalStereo: Efficient Spatial-Temporal Stereo Matching Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation models classify pixels into a set of known(``in-distribution'') visual classes. When deployed in an open world, thereliability of these models depends on their ability not only to classifyin-distribution pixels but also to detect out-of-distribution (OoD) pixels.Historically, the poor OoD detection performance of these models has motivatedthe design of methods based on model re-training using synthetic trainingimages that include OoD visual objects. Although successful, these re-trainedmethods have two issues: 1) their in-distribution segmentation accuracy maydrop during re-training, and 2) their OoD detection accuracy does notgeneralise well to new contexts (e.g., country surroundings) outside thetraining set (e.g., city surroundings). In this paper, we mitigate these issueswith: (i) a new residual pattern learning (RPL) module that assists thesegmentation model to detect OoD pixels without affecting the inliersegmentation performance; and (ii) a novel context-robust contrastive learning(CoroCL) that enforces RPL to robustly detect OoD pixels among variouscontexts. Our approach improves by around 10% FPR and 7% AuPRC the previousstate-of-the-art in Fishyscapes, Segment-Me-If-You-Can, and RoadAnomalydatasets. Our code is available at: ", "output": "Residual Pattern Learning for Pixel-wise Out-of-Distribution Detection in Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Thanks to the development of 2D keypoint detectors, monocular 3D human poseestimation (HPE) via 2D-to-3D uplifting approaches have achieved remarkableimprovements. Still, monocular 3D HPE is a challenging problem due to theinherent depth ambiguities and occlusions. To handle this problem, manyprevious works exploit temporal information to mitigate such difficulties.However, there are many real-world applications where frame sequences are notaccessible. This paper focuses on reconstructing a 3D pose from a single 2Dkeypoint detection. Rather than exploiting temporal information, we alleviatethe depth ambiguity by generating multiple 3D pose candidates which can bemapped to an identical 2D keypoint. We build a novel diffusion-based frameworkto effectively sample diverse 3D poses from an off-the-shelf 2D detector. Byconsidering the correlation between human joints by replacing the conventionaldenoising U-Net with graph convolutional network, our approach accomplishesfurther performance improvements. We evaluate our method on the widely adoptedHuman3.6M and HumanEva-I datasets. Comprehensive experiments are conducted toprove the efficacy of the proposed method, and they confirm that our modeloutperforms state-of-the-art multi-hypothesis 3D HPE methods.", "output": "DiffuPose: Monocular 3D Human Pose Estimation via Denoising Diffusion Probabilistic Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large-scale pre-training has shown promising results on thevision-and-language navigation (VLN) task. However, most existing pre-trainingmethods employ discrete panoramas to learn visual-textual associations. Thisrequires the model to implicitly correlate incomplete, duplicate observationswithin the panoramas, which may impair an agent's spatial understanding. Thus,we propose a new map-based pre-training paradigm that is spatial-aware for usein VLN. Concretely, we build a local metric map to explicitly aggregateincomplete observations and remove duplicates, while modeling navigationdependency in a global topological map. This hybrid design can balance thedemand of VLN for both short-term reasoning and long-term planning. Then, basedon the hybrid map, we devise a pre-training framework to learn a multimodal maprepresentation, which enhances spatial-aware cross-modal reasoning therebyfacilitating the language-guided navigation goal. Extensive experimentsdemonstrate the effectiveness of the map-based pre-training route for VLN, andthe proposed method achieves state-of-the-art on four VLN benchmarks.", "output": "BEVBert: Multimodal Map Pre-training for Language-guided Navigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image-based head swapping task aims to stitch a source head to another sourcebody flawlessly. This seldom-studied task faces two major challenges: 1)Preserving the head and body from various sources while generating a seamlesstransition region. 2) No paired head swapping dataset and benchmark so far. Inthis paper, we propose a semantic-mixing diffusion model for head swapping(HS-Diffusion) which consists of a latent diffusion model (LDM) and a semanticlayout generator. We blend the semantic layouts of source head and source body,and then inpaint the transition region by the semantic layout generator,achieving a coarse-grained head swapping. Semantic-mixing LDM can furtherimplement a fine-grained head swapping with the inpainted layout as conditionby a progressive fusion process, while preserving head and body withhigh-quality reconstruction. To this end, we propose a semantic calibrationstrategy for natural inpainting and a neck alignment for geometric realism.Importantly, we construct a new image-based head swapping benchmark and designtwo tailor-designed metrics (Mask-FID and Focal-FID). Extensive experimentsdemonstrate the superiority of our framework. The code will be available:", "output": "HS-Diffusion: Semantic-Mixing Diffusion for Head Swapping."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automatic defect detection for 3D printing processes, which shares manycharacteristics with change detection problems, is a vital step for qualitycontrol of 3D printed products. However, there are some critical challenges inthe current state of practice. First, existing methods for computervision-based process monitoring typically work well only under specific cameraviewpoints and lighting situations, requiring expensive pre-processing,alignment, and camera setups. Second, many defect detection techniques arespecific to pre-defined defect patterns and/or print schematics. In this work,we approach the defect detection problem using a novel Semi-Siamese deeplearning model that directly compares a reference schematic of the desiredprint and a camera image of the achieved print. The model then solves an imagesegmentation problem, precisely identifying the locations of defects ofdifferent types with respect to the reference schematic. Our model is designedto enable comparison of heterogeneous images from different domains while beingrobust against perturbations in the imaging setup such as different cameraangles and illumination. Crucially, we show that our simple architecture, whichis easy to pre-train for enhanced performance on new datasets, outperforms morecomplex state-of-the-art approaches based on generative adversarial networksand transformers. Using our model, defect localization predictions can be madein less than half a second per layer using a standard MacBook Pro whileachieving an F1-score of more than 0.9, demonstrating the efficacy of using ourmethod for in-situ defect detection in 3D printing.", "output": "Semi-Siamese Network for Robust Change Detection Across Different Domains with Applications to 3D Printing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The past two decades have seen increasingly rapid advances in the field ofmulti-view representation learning due to it extracting useful information fromdiverse domains to facilitate the development of multi-view applications.However, the community faces two challenges: i) how to learn robustrepresentations from a large amount of unlabeled data to against noise orincomplete views setting, and ii) how to balance view consistency andcomplementary for various downstream tasks. To this end, we utilize a deepfusion network to fuse view-specific representations into the view-commonrepresentation, extracting high-level semantics for obtaining robustrepresentation. In addition, we employ a clustering task to guide the fusionnetwork to prevent it from leading to trivial solutions. For balancingconsistency and complementary, then, we design an asymmetrical contrastivestrategy that aligns the view-common representation and each view-specificrepresentation. These modules are incorporated into a unified method known asCLustering-guided cOntrastiVE fusioN (CLOVEN). We quantitatively andqualitatively evaluate the proposed method on five datasets, demonstrating thatCLOVEN outperforms 11 competitive multi-view learning methods in clustering andclassification. In the incomplete view scenario, our proposed method resistsnoise interference better than those of our competitors. Furthermore, thevisualization analysis shows that CLOVEN can preserve the intrinsic structureof view-specific representation while also improving the compactness ofview-commom representation. Our source code will be available soon at", "output": "A Clustering-guided Contrastive Fusion for Multi-view Representation Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to quickly learn a new task with minimal instruction - known asfew-shot learning - is a central aspect of intelligent agents. Classicalfew-shot benchmarks make use of few-shot samples from a single modality, butsuch samples may not be sufficient to characterize an entire concept class. Incontrast, humans use cross-modal information to learn new concepts efficiently.In this work, we demonstrate that one can indeed build a better ${bf visual}$dog classifier by ${bf read}$ing about dogs and ${bf listen}$ing to thembark. To do so, we exploit the fact that recent multimodal foundation modelssuch as CLIP are inherently cross-modal, mapping different modalities to thesame representation space. Specifically, we propose a simple cross-modaladaptation approach that learns from few-shot examples spanning differentmodalities. By repurposing class names as additional one-shot training samples,we achieve SOTA results with an embarrassingly simple linear classifier forvision-language adaptation. Furthermore, we show that our approach can benefitexisting methods such as prefix tuning, adapters, and classifier ensembling.Finally, to explore other modalities beyond vision and language, we constructthe first (to our knowledge) audiovisual few-shot benchmark and use cross-modaltraining to improve the performance of both image and audio classification.", "output": "Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces the Efficient Decoupled Masked Autoencoder (EDMAE), anovel self-supervised method for recognizing standard views in pediatricechocardiography. EDMAE introduces a new proxy task based on theencoder-decoder structure. The EDMAE encoder is composed of a teacher and astudent encoder. The teacher encoder extracts the potential representation ofthe masked image blocks, while the student encoder extracts the potentialrepresentation of the visible image blocks. The loss is calculated between thefeature maps output by the two encoders to ensure consistency in the latentrepresentations they extract. EDMAE uses pure convolution operations instead ofthe ViT structure in the MAE encoder. This improves training efficiency andconvergence speed. EDMAE is pre-trained on a large-scale private dataset ofpediatric echocardiography using self-supervised learning, and then fine-tunedfor standard view recognition. The proposed method achieves high classificationaccuracy in 27 standard views of pediatric echocardiography. To further verifythe effectiveness of the proposed method, the authors perform anotherdownstream task of cardiac ultrasound segmentation on the public dataset CAMUS.The experimental results demonstrate that the proposed method outperforms somepopular supervised and recent self-supervised methods, and is more competitiveon different downstream tasks.", "output": "EDMAE: An Efficient Decoupled Masked Autoencoder for Standard View Identification in Pediatric Echocardiography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The goal of continual learning is to improve the performance of recognitionmodels in learning sequentially arrived data. Although most existing works areestablished on the premise of learning from scratch, growing efforts have beendevoted to incorporating the benefits of pre-training. However, how toadaptively exploit the pre-trained knowledge for each incremental task whilemaintaining its generalizability remains an open question. In this work, wepresent an extensive analysis for continual learning on a pre-trained model(CLPM), and attribute the key challenge to a progressive overfitting problem.Observing that selectively reducing the learning rate can almost resolve thisissue in the representation layer, we propose a simple but extremely effectiveapproach named Slow Learner with Classifier Alignment (SLCA), which furtherimproves the classification layer by modeling the class-wise distributions andaligning the classification layers in a post-hoc fashion. Across a variety ofscenarios, our proposal provides substantial improvements for CLPM (e.g., up to49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, SplitCUB-200 and Split Cars-196, respectively), and thus outperformsstate-of-the-art approaches by a large margin. Based on such a strong baseline,critical factors and promising directions are analyzed in-depth to facilitatesubsequent research. Code has been made available at:", "output": "SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrastive learning has recently demonstrated great potential forunsupervised pre-training in 3D scene understanding tasks. However, mostexisting work randomly selects point features as anchors while buildingcontrast, leading to a clear bias toward background points that often dominatein 3D scenes. Also, object awareness and foreground-to-backgrounddiscrimination are neglected, making contrastive learning less effective. Totackle these issues, we propose a general foreground-aware feature contrast(FAC) framework to learn more effective point cloud representations inpre-training. FAC consists of two novel contrast designs to construct moreeffective and informative contrast pairs. The first is building positive pairswithin the same foreground segment where points tend to have the samesemantics. The second is that we prevent over-discrimination between 3Dsegments/objects and encourage foreground-to-background distinctions at thesegment level with adaptive feature learning in a Siamese correspondencenetwork, which adaptively learns feature correlations within and across pointcloud views effectively. Visualization with point activation maps shows thatour contrast pairs capture clear correspondences among foreground regionsduring pre-training. Quantitative experiments also show that FAC achievessuperior knowledge transfer and data efficiency in various downstream 3Dsemantic segmentation and object detection tasks.", "output": "FAC: 3D Representation Learning via Foreground Aware Feature Contrast."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Precise and fast prediction methods for ischemic areas comprised of deadtissue, core, and salvageable tissue, penumbra, in acute ischemic stroke (AIS)patients are of significant clinical interest. They play an essential role inimproving diagnosis and treatment planning. Computed Tomography (CT) scan isone of the primary modalities for early assessment in patients with suspectedAIS. CT Perfusion (CTP) is often used as a primary assessment to determinestroke location, severity, and volume of ischemic lesions. Current automaticsegmentation methods for CTP mostly use already processed 3D parametric mapsconventionally used for clinical interpretation by radiologists as input.Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+timeinput, where the spatial information over the volume is ignored. In addition,these methods are only interested in segmenting core regions, while predictingpenumbra can be essential for treatment planning. This paper investigatesdifferent methods to utilize the entire 4D CTP as input to fully exploit thespatio-temporal information, leading us to propose a novel 4D convolutionlayer. Our comprehensive experiments on a local dataset of 152 patients dividedinto three groups show that our proposed models generate more precise resultsthan other methods explored. Adopting the proposed 4D mJ-Net, a DiceCoefficient of 0.53 and 0.23 is achieved for segmenting penumbra and coreareas, respectively. The code is available on", "output": "CT Perfusion is All We Need: 4D CNN Segmentation of Penumbra and Core in Patient With Suspected Ischemic Stroke."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Emotions play an essential role in human communication. Developing computervision models for automatic recognition of emotion expression can aid in avariety of domains, including robotics, digital behavioral healthcare, andmedia analytics. There are three types of emotional representations which aretraditionally modeled in affective computing research: Action Units, ValenceArousal (VA), and Categorical Emotions. As part of an effort to move beyondthese representations towards more fine-grained labels, we describe oursubmission to the newly introduced Emotional Reaction Intensity (ERI)Estimation challenge in the 5th competition for Affective Behavior Analysisin-the-Wild (ABAW). We developed four deep neural networks trained in thevisual domain and a multimodal model trained with both visual and audiofeatures to predict emotion reaction intensity. Our best performing model onthe Hume-Reaction dataset achieved an average Pearson correlation coefficientof 0.4080 on the test set using a pre-trained ResNet50 model. This workprovides a first step towards the development of production-grade models whichpredict emotion reaction intensities rather than discrete emotion categories.", "output": "Computer Vision Estimation of Emotion Reaction Intensity in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we revisit the efficacy of knowledge distillation as a functionmatching and metric learning problem. In doing so we verify three importantdesign decisions, namely the normalisation, soft maximum function, andprojection layers as key ingredients. We theoretically show that the projectorimplicitly encodes information on past examples, enabling relational gradientsfor the student. We then show that the normalisation of representations istightly coupled with the training dynamics of this projector, which can have alarge impact on the students performance. Finally, we show that a simple softmaximum function can be used to address any significant capacity gap problems.Experimental results on various benchmark datasets demonstrate that using theseinsights can lead to superior or comparable performance to state-of-the-artknowledge distillation techniques, despite being much more computationallyefficient. In particular, we obtain these results across image classification(CIFAR100 and ImageNet), object detection (COCO2017), and on more difficultdistillation objectives, such as training data efficient transformers, wherebywe attain a 77.2% top-1 accuracy with DeiT-Ti on ImageNet.", "output": "A closer look at the training dynamics of knowledge distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many material properties are manifested in the morphological appearance andcharacterized with microscopic image, such as scanning electron microscopy(SEM). Polymer miscibility is a key physical quantity of polymer material andcommonly and intuitively judged by SEM images. However, human observation andjudgement for the images is time-consuming, labor-intensive and hard to bequantified. Computer image recognition with machine learning method can make upthe defects of artificial judging, giving accurate and quantitative judgement.We achieve automatic miscibility recognition utilizing convolution neuralnetwork and transfer learning method, and the model obtains up to 94% accuracy.We also put forward a quantitative criterion for polymer miscibility with thismodel. The proposed method can be widely applied to the quantitativecharacterization of the microstructure and properties of various materials.", "output": "Automatically Predict Material Properties with Microscopic Image Example Polymer Compatibility."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Uni-Fusion, a universal continuous mapping framework for surfaces,surface properties (color, infrared, etc.) and more (latent features in CLIPembedding space, etc.). We propose the first universal implicit encoding modelthat supports encoding of both geometry and different types of properties (RGB,infrared, features, etc.) without requiring any training. Based on this, ourframework divides the point cloud into regular grid voxels and generates alatent feature in each voxel to form a Latent Implicit Map (LIM) for geometriesand arbitrary properties. Then, by fusing a local LIM frame-wisely into aglobal LIM, an incremental reconstruction is achieved. Encoded withcorresponding types of data, our Latent Implicit Map is capable of generatingcontinuous surfaces, surface property fields, surface feature fields, and allother possible options. To demonstrate the capabilities of our model, weimplement three applications: (1) incremental reconstruction for surfaces andcolor (2) 2D-to-3D transfer of fabricated properties (3) open-vocabulary sceneunderstanding by creating a text CLIP feature field on surfaces. We evaluateUni-Fusion by comparing it in corresponding applications, from which Uni-Fusionshows high-flexibility in various applications while performing best or beingcompetitive. The project page of Uni-Fusion is available at .", "output": "Uni-Fusion: Universal Continuous Mapping."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Actively planning sensor views during object reconstruction is crucial forautonomous mobile robots. An effective method should be able to strike abalance between accuracy and efficiency. In this paper, we propose a seamlessintegration of the emerging implicit representation with the activereconstruction task. We build an implicit occupancy field as our geometryproxy. While training, the prior object bounding box is utilized as auxiliaryinformation to generate clean and detailed reconstructions. To evaluate viewuncertainty, we employ a sampling-based approach that directly extracts entropyfrom the reconstructed occupancy probability field as our measure of viewinformation gain. This eliminates the need for additional uncertainty maps orlearning. Unlike previous methods that compare view uncertainty within a finiteset of candidates, we aim to find the next-best-view (NBV) on a continuousmanifold. Leveraging the differentiability of the implicit representation, theNBV can be optimized directly by maximizing the view uncertainty using gradientdescent. It significantly enhances the method's adaptability to differentscenarios. Simulation and real-world experiments demonstrate that our approacheffectively improves reconstruction accuracy and efficiency of view planning inactive reconstruction tasks. The proposed system will open source at", "output": "Active Implicit Object Reconstruction using Uncertainty-guided Next-Best-View Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Obtaining sufficient labeled data for training deep models is oftenchallenging in real-life applications. To address this issue, we propose anovel solution for single-source domain generalized semantic segmentation.Recent approaches have explored data diversity enhancement using hallucinationtechniques. However, excessive hallucination can degrade performance,particularly for imbalanced datasets. As shown in our experiments, minorityclasses are more susceptible to performance reduction due to hallucinationcompared to majority classes. To tackle this challenge, we introduce adual-stage Feature Transform (dFT) layer within the Adversarial SemanticHallucination+ (ASH+) framework. The ASH+ framework performs a dual-stagemanipulation of hallucination strength. By leveraging semantic information foreach pixel, our approach adaptively adjusts the pixel-wise hallucinationstrength, thus providing fine-grained control over hallucination. We validatethe effectiveness of our proposed method through comprehensive experiments onpublicly available semantic segmentation benchmark datasets (Cityscapes andSYNTHIA). Quantitative and qualitative comparisons demonstrate that ourapproach is competitive with state-of-the-art methods for the Cityscapesdataset and surpasses existing solutions for the SYNTHIA dataset. Code for ourframework will be made readily available to the research community.", "output": "Dual Stage Stylization Modulation for Domain Generalized Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Very high-resolution (VHR) remote sensing (RS) scene classification is achallenging task due to the higher inter-class similarity and intra-classvariability problems. Recently, the existing deep learning (DL)-based methodshave shown great promise in VHR RS scene classification. However, they stillprovide an unstable classification performance. To address such a problem, we,in this letter, propose a novel DL-based approach. For this, we devise anenhanced VHR attention module (EAM), followed by the atrous spatial pyramidpooling (ASPP) and global average pooling (GAP). This procedure imparts theenhanced features from the corresponding level. Then, the multi-level featurefusion is performed. Experimental results on two widely-used VHR RS datasetsshow that the proposed approach yields a competitive and stable/robustclassification performance with the least standard deviation of 0.001. Further,the highest overall accuracies on the AID and the NWPU datasets are 95.39% and93.04%, respectively.", "output": "Enhanced Multi-level Features for Very High Resolution Remote Sensing Scene Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a latent variable generalisation of neural network softmaxclassification trained with cross-entropy loss, referred to as variationalclassification (VC). Our approach offers a novel probabilistic perspective onthe highly familiar softmax classification model, to which it relates similarlyto how variational and traditional autoencoders relate. We derive a trainingobjective based on the evidence lower bound (ELBO) that is non-trivial tooptimize, and therefore propose an adversarial approach to maximise it. We showthat VC addresses an inherent inconsistency within softmax classification,whilst also allowing more flexible choices of prior distributions in the latentspace in place of implicit assumptions revealed within off-the-shelf softmaxclassifiers. Empirical evaluation on image and text classification datasetsdemonstrates that variational classification maintains prediction accuracywhile improving other desirable properties such as calibration and adversarialrobustness, particularly under distribution shift and low data settings.", "output": "Variational Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The proliferation of in-the-wild videos has greatly expanded the VideoQuality Assessment (VQA) problem. Unlike early definitions that usually focuson limited distortion types, VQA on in-the-wild videos is especiallychallenging as it could be affected by complicated factors, including variousdistortions and diverse contents. Though subjective studies have collectedoverall quality scores for these videos, how the abstract quality scores relatewith specific factors is still obscure, hindering VQA methods from moreconcrete quality evaluations (e.g. sharpness of a video). To solve thisproblem, we collect over two million opinions on 4,543 in-the-wild videos on 13dimensions of quality-related factors, including in-capture authenticdistortions (e.g. motion blur, noise, flicker), errors introduced bycompression and transmission, and higher-level experiences on semantic contentsand aesthetic issues (e.g. composition, camera trajectory), to establish themulti-dimensional Maxwell database. Specifically, we ask the subjects to labelamong a positive, a negative, and a neutral choice for each dimension. Theseexplanation-level opinions allow us to measure the relationships betweenspecific quality factors and abstract subjective quality ratings, and tobenchmark different categories of VQA algorithms on each dimension, so as tomore comprehensively analyze their strengths and weaknesses. Furthermore, wepropose the MaxVQA, a language-prompted VQA approach that modifiesvision-language foundation model CLIP to better capture important qualityissues as observed in our analyses. The MaxVQA can jointly evaluate variousspecific quality factors and final quality scores with state-of-the-artaccuracy on all dimensions, and superb generalization ability on existingdatasets. Code and data available at ", "output": "Towards Explainable In-the-Wild Video Quality Assessment: A Database and a Language-Prompted Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The rapidly evolving fields of e-commerce and metaverse continue to seekinnovative approaches to enhance the consumer experience. At the same time,recent advancements in the development of diffusion models have enabledgenerative networks to create remarkably realistic images. In this context,image-based virtual try-on, which consists in generating a novel image of atarget model wearing a given in-shop garment, has yet to capitalize on thepotential of these powerful generative solutions. This work introducesLaDI-VTON, the first Latent Diffusion textual Inversion-enhanced model for theVirtual Try-ON task. The proposed architecture relies on a latent diffusionmodel extended with a novel additional autoencoder module that exploitslearnable skip connections to enhance the generation process preserving themodel's characteristics. To effectively maintain the texture and details of thein-shop garment, we propose a textual inversion component that can map thevisual features of the garment to the CLIP token embedding space and thusgenerate a set of pseudo-word token embeddings capable of conditioning thegeneration process. Experimental results on Dress Code and VITON-HD datasetsdemonstrate that our approach outperforms the competitors by a consistentmargin, achieving a significant milestone for the task. Source code and trainedmodels are publicly available at: ", "output": "LaDI-VTON: Latent Diffusion Textual-Inversion Enhanced Virtual Try-On."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Combustion vehicle emissions contribute to poor air quality and releasegreenhouse gases into the atmosphere, and vehicle pollution has been associatedwith numerous adverse health effects. Roadways with extensive waiting and/orpassenger drop off, such as schools and hospital drop-off zones, can result inhigh incidence and density of idling vehicles. This can produce micro-climatesof increased vehicle pollution. Thus, the detection of idling vehicles can behelpful in monitoring and responding to unnecessary idling and be integratedinto real-time or off-line systems to address the resulting pollution. In thispaper we present a real-time, dynamic vehicle idling detection algorithm. Theproposed idle detection algorithm and notification rely on an algorithm todetect these idling vehicles. The proposed method relies on a multi-sensor,audio-visual, machine-learning workflow to detect idling vehicles visuallyunder three conditions: moving, static with the engine on, and static with theengine off. The visual vehicle motion detector is built in the first stage, andthen a contrastive-learning-based latent space is trained for classifyingstatic vehicle engine sound. We test our system in real-time at a hospitaldrop-off point in Salt Lake City. This in-situ dataset was collected andannotated, and it includes vehicles of varying models and types. Theexperiments show that the method can detect engine switching on or offinstantly and achieves 71.02 average precision (AP) for idle detections and91.06 for engine off detections.", "output": "Real-Time Idling Vehicles Detection using Combined Audio-Visual Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Accurately identifying white matter tracts in medical images is essential forvarious applications, including surgery planning and tract-specific analysis.Supervised machine learning models have reached state-of-the-art solving thistask automatically. However, these models are primarily trained on healthysubjects and struggle with strong anatomical aberrations, e.g. caused by braintumors. This limitation makes them unsuitable for tasks such as preoperativeplanning, wherefore time-consuming and challenging manual delineation of thetarget tract is typically employed. We propose semi-automatic entropy-basedactive learning for quick and intuitive segmentation of white matter tractsfrom whole-brain tractography consisting of millions of streamlines. The methodis evaluated on 21 openly available healthy subjects from the Human ConnectomeProject and an internal dataset of ten neurosurgical cases. With only a fewannotations, the proposed approach enables segmenting tracts on tumor casescomparable to healthy subjects (dice=0.71), while the performance of automaticmethods, like TractSeg dropped substantially (dice=0.34) in comparison tohealthy subjects. The method is implemented as a prototype named atTRACTive inthe freely available software MITK Diffusion. Manual experiments on tumor datashowed higher efficiency due to lower segmentation times compared totraditional ROI-based segmentation.", "output": "atTRACTive: Semi-automatic white matter tract segmentation using active learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image generative models have enabled high-resolution image synthesisacross different domains, but require users to specify the content they wish togenerate. In this paper, we consider the inverse problem -- given a collectionof different images, can we discover the generative concepts that representeach image? We present an unsupervised approach to discover generative conceptsfrom a collection of images, disentangling different art styles in paintings,objects, and lighting from kitchen scenes, and discovering image classes givenImageNet images. We show how such generative concepts can accurately representthe content of images, be recombined and composed to generate new artistic andhybrid images, and be further used as a representation for downstreamclassification tasks.", "output": "Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Chest X-ray (CXR) images are commonly compressed to a lower resolution andbit depth to reduce their size, potentially altering subtle diagnosticfeatures.Radiologists use windowing operations to enhance image contrast, but theimpact of such operations on CXR classification performance is unclear.In this study, we show that windowing can improve CXR classificationperformance, and propose WindowNet, a model that learns optimal windowsettings.We first investigate the impact of bit-depth on classification performanceand find that a higher bit-depth (12-bit) leads to improved performance.We then evaluate different windowing settings and show that training with adistinct window generally improves pathology-wise classification performance.Finally, we propose and evaluate WindowNet, a model that learns optimalwindow settings, and show that it significantly improves performance comparedto the baseline model without windowing.", "output": "WindowNet: Learnable Windows for Chest X-ray Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning models for image classification are often trained at aresolution of 224 x 224 pixels for historical and efficiency reasons. However,chest X-rays are acquired at a much higher resolution to display subtlepathologies. This study investigates the effect of training resolution on chestX-ray classification performance, using the chest X-ray 14 dataset. The resultsshow that training with a higher image resolution, specifically 1024 x 1024pixels, results in the best overall classification performance with a mean AUCof 84.2 % compared to 82.7 % when trained with 256 x 256 pixel images.Additionally, comparison of bounding boxes and GradCAM saliency maps suggestthat low resolutions, such as 256 x 256 pixels, are insufficient foridentifying small pathologies and force the model to use spuriousdiscriminating features. Our code is publicly available at", "output": "Higher Chest X-ray Resolution Improves Classification Performance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have emerged as the emph{de-facto} technique for imagegeneration, yet they entail significant computational overhead, hindering thetechnique's broader application in the research community. We propose aprior-based denoising training framework, the first to incorporate thepre-train and fine-tune paradigm into the diffusion model training process,which substantially improves training efficiency and shows potential infacilitating various downstream tasks. Our approach centers on masking a highproportion (e.g., up to 90%) of the input image and employing masked denoisingscore matching to denoise the visible areas, thereby guiding the diffusionmodel to learn more salient features from training data as prior knowledge. Byutilizing masked learning in a pre-training stage, we efficiently train theViT-based diffusion model on CelebA-HQ $256 times 256$ in the pixel space,achieving a 4x acceleration and enhancing the quality of generated imagescompared to denoising diffusion probabilistic model (DDPM). Moreover, ourmasked pre-training technique can be universally applied to various diffusionmodels that directly generate images in the pixel space, aiding in the learningof pre-trained models with superior generalizability. For instance, a diffusionmodel pre-trained on VGGFace2 attains a 46% quality improvement throughfine-tuning with merely 10% data from a different distribution. Moreover, ourmethod shows the potential to serve as a training paradigm for enhancing theprivacy protection capabilities of diffusion models. Our code is available aturl{", "output": "Masked Diffusion Models Are Fast and Privacy-Aware Learners."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-based teleoperation offers the possibility to endow robots withhuman-level intelligence to physically interact with the environment, whileonly requiring low-cost camera sensors. However, current vision-basedteleoperation systems are designed and engineered towards a particular robotmodel and deploy environment, which scales poorly as the pool of the robotmodels expands and the variety of the operating environment increases. In thispaper, we propose AnyTeleop, a unified and general teleoperation system tosupport multiple different arms, hands, realities, and camera configurationswithin a single system. Although being designed to provide great flexibility tothe choice of simulators and real hardware, our system can still achieve greatperformance. For real-world experiments, AnyTeleop can outperform a previoussystem that was designed for a specific robot hardware with a higher successrate, using the same robot. For teleoperation in simulation, AnyTeleop leads tobetter imitation learning performance, compared with a previous system that isparticularly designed for that simulator. Project page: <a href=\" http URL</a>", "output": "AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The SOTA face swap models still suffer the problem of either target identity(i.e., shape) being leaked or the target non-identity attributes (i.e.,background, hair) failing to be fully preserved in the final results. We showthat this insufficient disentanglement is caused by two flawed designs thatwere commonly adopted in prior models: (1) counting on only one compressedencoder to represent both the semantic-level non-identity facialattributes(i.e., pose) and the pixel-level non-facial region details, which iscontradictory to satisfy at the same time; (2) highly relying on longskip-connections between the encoder and the final generator, leaking a certainamount of target face identity into the result. To fix them, we introduce a newface swap framework called 'WSC-swap' that gets rid of skip connections anduses two target encoders to respectively capture the pixel-level non-facialregion attributes and the semantic non-identity attributes in the face region.To further reinforce the disentanglement learning for the target encoder, weemploy both identity removal loss via adversarial training (i.e., GAN) and thenon-identity preservation loss via prior 3DMM models like [11]. Extensiveexperiments on both FaceForensics++ and CelebA-HQ show that our resultssignificantly outperform previous works on a rich set of metrics, including onenovel metric for measuring identity consistency that was completely neglectedbefore.", "output": "Reinforced Disentanglement for Face Swapping without Skip Connection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Though performed almost effortlessly by humans, segmenting 2D gray-scale orcolor images in terms of regions of interest (e.g.~background, objects, orportions of objects) constitutes one of the greatest challenges in science andtechnology as a consequence of the involved dimensionality reduction(3D to 2D),noise, reflections, shades, and occlusions, among many other possible effects.While a large number of interesting related approaches have been suggestedalong the last decades, it was mainly thanks to the recent development of deeplearning that more effective and general solutions have been obtained,currently constituting the basic comparison reference for this type ofoperation. Also developed recently, a multiset-based methodology has beendescribed that is capable of encouraging image segmentation performance whilecombining spatial accuracy, stability, and robustness while requiring littlecomputational resources (hardware and/or training and recognition time). Theinteresting features of the multiset neurons methodology mostly follow from theenhanced selectivity and sensitivity, as well as good robustness to dataperturbations and outliers, allowed by the coincidence similarity index onwhich the multiset approach to supervised image segmentation is based. Afterdescribing the deep learning and multiset neurons approaches, the present workdevelops two comparison experiments between them which are primarily aimed atillustrating their respective main interesting features when applied to theadopted specific type of data and parameter configurations. While the deeplearning approach confirmed its potential for performing image segmentation,the alternative multiset methodology allowed for enhanced accuracy whilerequiring little computational resources.", "output": "Two Approaches to Supervised Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a simple three-stage approach to segment unseen objects in RGBimages using their CAD models. Leveraging recent powerful foundation models,DINOv2 and Segment Anything, we create descriptors and generate proposals,including binary masks for a given input RGB image. By matching proposals withreference descriptors created from CAD models, we achieve precise object IDassignment along with modal masks. We experimentally demonstrate that ourmethod achieves state-of-the-art results in CAD-based novel objectsegmentation, surpassing existing approaches on the seven core datasets of theBOP challenge by 19.8% AP using the same BOP evaluation protocol. Our sourcecode is available at ", "output": "CNOS: A Strong Baseline for CAD-based Novel Object Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For safety-related applications, it is crucial to produce trustworthy deepneural networks whose prediction is associated with confidence that canrepresent the likelihood of correctness for subsequent decision-making.Existing dense binary classification models are prone to being over-confident.To improve model calibration, we propose Adaptive Stochastic Label Perturbation(ASLP) which learns a unique label perturbation level for each training image.ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss,which unifies label perturbation processes including stochastic approaches(like DisturbLabel), and label smoothing, to correct calibration whilemaintaining classification rates. ASLP follows Maximum Entropy Inference ofclassic statistical mechanics to maximise prediction entropy with respect tomissing information. It performs this while: (1) preserving classificationaccuracy on known data as a conservative solution, or (2) specifically improvesmodel calibration degree by minimising the gap between the prediction accuracyand expected confidence of the target training label. Extensive resultsdemonstrate that ASLP can significantly improve calibration degrees of densebinary classification models on both in-distribution and out-of-distributiondata. The code is available on ", "output": "Model Calibration in Dense Classification with Adaptive Label Perturbation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, diffusion models like StableDiffusion have achieved impressiveimage generation results. However, the generation process of such diffusionmodels is uncontrollable, which makes it hard to generate videos withcontinuous and consistent content. In this work, by using the diffusion modelwith ControlNet, we proposed a new motion-guided video-to-video translationframework called VideoControlNet to generate various videos based on the givenprompts and the condition from the input video. Inspired by the video codecsthat use motion information for reducing temporal redundancy, our frameworkuses motion information to prevent the regeneration of the redundant areas forcontent consistency. Specifically, we generate the first frame (i.e., theI-frame) by using the diffusion model with ControlNet. Then we generate otherkey frames (i.e., the P-frame) based on the previous I/P-frame by using ournewly proposed motion-guided P-frame generation (MgPG) method, in which theP-frames are generated based on the motion information and the occlusion areasare inpainted by using the diffusion model. Finally, the rest frames (i.e., theB-frame) are generated by using our motion-guided B-frame interpolation (MgBI)module. Our experiments demonstrate that our proposed VideoControlNet inheritsthe generation capability of the pre-trained large diffusion model and extendsthe image diffusion model to the video diffusion model by using motioninformation. More results are provided at our project page.", "output": "VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised Out-of-Distribution (OOD) detection consists in identifyinganomalous regions in images leveraging only models trained on images of healthyanatomy. An established approach is to tokenize images and model thedistribution of tokens with Auto-Regressive (AR) models. AR models are used to1) identify anomalous tokens and 2) in-paint anomalous representations within-distribution tokens. However, AR models are slow at inference time and proneto error accumulation issues which negatively affect OOD detection performance.Our novel method, MIM-OOD, overcomes both speed and error accumulation issuesby replacing the AR model with two task-specific networks: 1) a transformeroptimized to identify anomalous tokens and 2) a transformer optimized toin-paint anomalous tokens using masked image modelling (MIM). Our experimentswith brain MRI anomalies show that MIM-OOD substantially outperforms AR models(DICE 0.458 vs 0.301) while achieving a nearly 25x speedup (9.5s vs 244s).", "output": "MIM-OOD: Generative Masked Image Modelling for Out-of-Distribution Detection in Medical Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optical flow and disparity are two informative visual features for autonomousdriving perception. They have been used for a variety of applications, such asobstacle and lane detection. The concept of \"U-V-Disparity\" has been widelyexplored in the literature, while its counterpart in optical flow has receivedrelatively little attention. Traditional motion analysis algorithms estimateoptical flow by matching correspondences between two successive video frames,which limits the full utilization of environmental information and geometricconstraints. Therefore, we propose a novel strategy to model optical flow inthe collision-free space (also referred to as drivable area or simplyfreespace) for intelligent vehicles, with the full utilization of geometryinformation in a 3D driving environment. We provide explicit representations ofoptical flow and deduce the quadratic relationship between the optical flowcomponent and the vertical coordinate. Through extensive experiments on severalpublic datasets, we demonstrate the high accuracy and robustness of our model.Additionally, our proposed freespace optical flow model boasts a diverse arrayof applications within the realm of automated driving, providing a geometricconstraint in freespace detection, vehicle localization, and more. We have madeour source code publicly available at ", "output": "Freespace Optical Flow Modeling for Automated Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models and large language models have emerged as leading-edgegenerative models and have sparked a revolutionary impact on various aspects ofhuman life. However, the practical implementation of these models has alsoexposed inherent risks, highlighting their dual nature and raising concernsregarding their trustworthiness. Despite the abundance of literature on thissubject, a comprehensive survey specifically delving into the intersection oflarge-scale generative models and their trustworthiness remains largely absent.To bridge this gap, This paper investigates both the long-standing and emergingthreats associated with these models across four fundamental dimensions:privacy, security, fairness, and responsibility. In this way, we construct anextensive map outlining the trustworthiness of these models, while alsoproviding practical recommendations and identifying future directions. Theseefforts are crucial for promoting the trustworthy deployment of these models,ultimately benefiting society as a whole.", "output": "On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Contrast Enhanced Spectral Mammography (CESM) is a dual-energy mammographicimaging technique that first needs intravenously administration of an iodinatedcontrast medium; then, it collects both a low-energy image, comparable tostandard mammography, and a high-energy image. The two scans are then combinedto get a recombined image showing contrast enhancement. Despite CESM diagnosticadvantages for breast cancer diagnosis, the use of contrast medium can causeside effects, and CESM also beams patients with a higher radiation dosecompared to standard mammography. To address these limitations this workproposes to use deep generative models for virtual contrast enhancement onCESM, aiming to make the CESM contrast-free as well as to reduce the radiationdose. Our deep networks, consisting of an autoencoder and two GenerativeAdversarial Networks, the Pix2Pix, and the CycleGAN, generate syntheticrecombined images solely from low-energy images. We perform an extensivequantitative and qualitative analysis of the model's performance, alsoexploiting radiologists' assessments, on a novel CESM dataset that includes1138 images that, as a further contribution of this work, we make publiclyavailable. The results show that CycleGAN is the most promising deep network togenerate synthetic recombined images, highlighting the potential of artificialintelligence techniques for virtual contrast enhancement in this field.", "output": "A Deep Learning Approach for Virtual Contrast Enhancement in Contrast Enhanced Spectral Mammography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although perception systems have made remarkable advancements in recentyears, they still rely on explicit human instruction to identify the targetobjects or categories before executing visual recognition tasks. Such systemslack the ability to actively reason and comprehend implicit user intentions. Inthis work, we propose a new segmentation task -- reasoning segmentation. Thetask is designed to output a segmentation mask given a complex and implicitquery text. Furthermore, we establish a benchmark comprising over one thousandimage-instruction pairs, incorporating intricate reasoning and world knowledgefor evaluation purposes. Finally, we present LISA: large Language InstructedSegmentation Assistant, which inherits the language generation capabilities ofthe multi-modal Large Language Model (LLM) while also possessing the ability toproduce segmentation masks. We expand the original vocabulary with a &lt;SEG&gt;token and propose the embedding-as-mask paradigm to unlock the segmentationcapability. Remarkably, LISA can handle cases involving: 1) complex reasoning;2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also,it demonstrates robust zero-shot capability when trained exclusively onreasoning-free datasets. In addition, fine-tuning the model with merely 239reasoning segmentation image-instruction pairs results in further performanceenhancement. Experiments show our method not only unlocks new reasoningsegmentation capabilities but also proves effective in both complex reasoningsegmentation and standard referring segmentation tasks. Code, models, and demoare at ", "output": "LISA: Reasoning Segmentation via Large Language Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Building a multi-modality multi-task neural network toward accurate androbust performance is a de-facto standard in perception task of autonomousdriving. However, leveraging such data from multiple sensors to jointlyoptimize the prediction and planning tasks remains largely unexplored. In thispaper, we present FusionAD, to the best of our knowledge, the first unifiedframework that fuse the information from two most critical sensors, camera andLiDAR, goes beyond perception task. Concretely, we first build a transformerbased multi-modality fusion network to effectively produce fusion basedfeatures. In constrast to camera-based end-to-end method UniAD, we thenestablish a fusion aided modality-aware prediction and status-aware planningmodules, dubbed FMSPnP that take advantages of multi-modality features. Weconduct extensive experiments on commonly used benchmark nuScenes dataset, ourFusionAD achieves state-of-the-art performance and surpassing baselines onaverage 15% on perception tasks like detection and tracking, 10% on occupancyprediction accuracy, reducing prediction error from 0.708 to 0.389 in ADE scoreand reduces the collision rate from 0.31% to only 0.12%.", "output": "FusionAD: Multi-modality Fusion for Prediction and Planning Tasks of Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The U-shaped architecture has emerged as a crucial paradigm in the design ofmedical image segmentation networks. However, due to the inherent locallimitations of convolution, a fully convolutional segmentation network withU-shaped architecture struggles to effectively extract global contextinformation, which is vital for the precise localization of lesions. Whilehybrid architectures combining CNNs and Transformers can address these issues,their application in real medical scenarios is limited due to the computationalresource constraints imposed by the environment and edge devices. In addition,the convolutional inductive bias in lightweight networks adeptly fits thescarce medical data, which is lacking in the Transformer based network. Inorder to extract global context information while taking advantage of theinductive bias, we propose CMUNeXt, an efficient fully convolutionallightweight medical image segmentation network, which enables fast and accurateauxiliary diagnosis in real scene scenarios. CMUNeXt leverages large kernel andinverted bottleneck design to thoroughly mix distant spatial and locationinformation, efficiently extracting global context information. We alsointroduce the Skip-Fusion block, designed to enable smooth skip-connections andensure ample feature fusion. Experimental results on multiple medical imagedatasets demonstrate that CMUNeXt outperforms existing heavyweight andlightweight medical image segmentation networks in terms of segmentationperformance, while offering a faster inference speed, lighter weights, and areduced computational cost. The code is available at", "output": "CMUNeXt: An Efficient Medical Image Segmentation Network based on Large Kernel and Skip Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Computer-aided diagnosis (CAD), a vibrant medical imaging research field, isexpanding quickly. Because errors in medical diagnostic systems might lead toseriously misleading medical treatments, major efforts have been made in recentyears to improve computer-aided diagnostics applications. The use of machinelearning in computer-aided diagnosis is crucial. A simple equation may resultin a false indication of items like organs. Therefore, learning from examplesis a vital component of pattern recognition. Pattern recognition and machinelearning in the biomedical area promise to increase the precision of diseasedetection and diagnosis. They also support the decision-making process'sobjectivity. Machine learning provides a practical method for creating elegantand autonomous algorithms to analyze high-dimensional and multimodalbio-medical data. This review article examines machine-learning algorithms fordetecting diseases, including hepatitis, diabetes, liver disease, dengue fever,and heart disease. It draws attention to the collection of machine learningtechniques and algorithms employed in studying conditions and the ensuingdecision-making process.", "output": "Recent advancement in Disease Diagnostic using machine learning: Systematic survey of decades, comparisons, and challenges."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatGPT-like models have revolutionized various applications in artificialintelligence, from summarization and coding to translation, matching or evensurpassing human performance. However, the current landscape lacks anaccessible, efficient, and cost-effective end-to-end RLHF (ReinforcementLearning with Human Feedback) training pipeline for these powerful models,particularly when training at the scale of billions of parameters. This paperintroduces DeepSpeed-Chat, a novel system that democratizes RLHF training,making it accessible to the AI community. DeepSpeed-Chat offers three keycapabilities: an easy-to-use training and inference experience for ChatGPT-likemodels, a DeepSpeed-RLHF pipeline that replicates the training pipeline fromInstructGPT, and a robust DeepSpeed-RLHF system that combines variousoptimizations for training and inference in a unified way. The system deliversunparalleled efficiency and scalability, enabling training of models withhundreds of billions of parameters in record time and at a fraction of thecost. With this development, DeepSpeed-Chat paves the way for broader access toadvanced RLHF training, even for data scientists with limited resources,thereby fostering innovation and further development in the field of AI.", "output": "DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Gene network information is believed to be beneficial for disease module andpathway identification, but has not been explicitly utilized in the standardrandom forest (RF) algorithm for gene expression data analysis. We investigatethe performance of a network-guided RF where the network information issummarized into a sampling probability of predictor variables which is furtherused in the construction of the RF. Our results suggest that network-guided RFdoes not provide better disease prediction than the standard RF. In terms ofdisease gene discovery, if disease genes form module(s), network-guided RFidentifies them more accurately. In addition, when disease status isindependent from genes in the given network, spurious gene selection resultscan occur when using network information, especially on hub genes. Ourempirical analysis on two balanced microarray and RNA-Seq breast cancerdatasets from The Cancer Genome Atlas (TCGA) for classification of progesteronereceptor (PR) status also demonstrates that network-guided RF can identifygenes from PGR-related pathways, which leads to a better connected module ofidentified genes.", "output": "Evaluation of network-guided random forest for disease gene discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a fully automated approach for identifying speechanomalies from voice recordings to aid in the assessment of speech impairments.By combining Connectionist Temporal Classification (CTC) andencoder-decoder-based automatic speech recognition models, we generate richacoustic and clean transcripts. We then apply several natural languageprocessing methods to extract features from these transcripts to produceprototypes of healthy speech. Basic distance measures from these prototypesserve as input features for standard machine learning classifiers, yieldinghuman-level accuracy for the distinction between recordings of people withaphasia and a healthy control group. Furthermore, the most frequently occurringaphasia types can be distinguished with 90% accuracy. The pipeline is directlyapplicable to other diseases and languages, showing promise for robustlyextracting diagnostic speech biomarkers.", "output": "Careful Whisper -- leveraging advances in automatic speech recognition for robust and interpretable aphasia subtype classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Embedding learning transforms discrete data entities into continuousnumerical representations, encoding features/properties of the entities.Despite the outstanding performance reported from different embedding learningalgorithms, few efforts were devoted to structurally interpreting how featuresare encoded in the learned embedding space. This work proposes EmbeddingTree, ahierarchical embedding exploration algorithm that relates the semantics ofentity features with the less-interpretable embedding vectors. An interactivevisualization tool is also developed based on EmbeddingTree to explorehigh-dimensional embeddings. The tool helps users discover nuance features ofdata entities, perform feature denoising/injecting in embedding training, andgenerate embeddings for unseen entities. We demonstrate the efficacy ofEmbeddingTree and our visualization tool through embeddings generated forindustry-scale merchant data and the public 30Music listening/playlistsdataset.", "output": "EmbeddingTree: Hierarchical Exploration of Entity Features in Embedding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we investigate the impact of compression on stochasticgradient algorithms for machine learning, a technique widely used indistributed and federated learning. We underline differences in terms ofconvergence rates between several unbiased compression operators, that allsatisfy the same condition on their variance, thus going beyond the classicalworst-case analysis. To do so, we focus on the case of least-squares regression(LSR) and analyze a general stochastic approximation algorithm for minimizingquadratic functions relying on a random field. We consider weak assumptions onthe random field, tailored to the analysis (specifically, expected H\"olderregularity), and on the noise covariance, enabling the analysis of variousrandomizing mechanisms, including compression. We then extend our results tothe case of federated learning.More formally, we highlight the impact on the convergence of the covariance$mathfrak{C}_{mathrm{ania}}$ of the additive noise induced by the algorithm.We demonstrate despite the non-regularity of the stochastic field, that thelimit variance term scales with $mathrm{Tr}(mathfrak{C}_{mathrm{ania}}H^{-1})/K$ (where $H$ is the Hessian of the optimization problem and $K$ thenumber of iterations) generalizing the rate for the vanilla LSR case where itis $sigma^2 mathrm{Tr}(H H^{-1}) / K = sigma^2 d / K$ (Bach and Moulines,2013). Then, we analyze the dependency of $mathfrak{C}_{mathrm{ania}}$ on thecompression strategy and ultimately its impact on convergence, first in thecentralized case, then in two heterogeneous FL frameworks.", "output": "Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While tumor dynamic modeling has been widely applied to support thedevelopment of oncology drugs, there remains a need to increase predictivity,enable personalized therapy, and improve decision-making. We propose the use ofTumor Dynamic Neural-ODE (TDNODE) as a pharmacology-informed neural network toenable model discovery from longitudinal tumor size data. We show that TDNODEovercomes a key limitation of existing models in its ability to make unbiasedpredictions from truncated data. The encoder-decoder architecture is designedto express an underlying dynamical law which possesses the fundamental propertyof generalized homogeneity with respect to time. Thus, the modeling formalismenables the encoder output to be interpreted as kinetic rate metrics, withinverse time as the physical unit. We show that the generated metrics can beused to predict patients' overall survival (OS) with high accuracy. Theproposed modeling formalism provides a principled way to integrate multimodaldynamical datasets in oncology disease modeling.", "output": "Explainable Deep Learning for Tumor Dynamic Modeling and Overall Survival Prediction using Neural-ODE."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Long exposure photography produces stunning imagery, representing movingelements in a scene with motion-blur. It is generally employed in twomodalities, producing either a foreground or a background blur effect.Foreground blur images are traditionally captured on a tripod-mounted cameraand portray blurred moving foreground elements, such as silky water or lighttrails, over a perfectly sharp background landscape. Background blur images,also called panning photography, are captured while the camera is tracking amoving subject, to produce an image of a sharp subject over a backgroundblurred by relative motion. Both techniques are notoriously challenging andrequire additional equipment and advanced skills. In this paper, we describe acomputational burst photography system that operates in a hand-held smartphonecamera app, and achieves these effects fully automatically, at the tap of theshutter button. Our approach first detects and segments the salient subject. Wetrack the scene motion over multiple frames and align the images in order topreserve desired sharpness and to produce aesthetically pleasing motionstreaks. We capture an under-exposed burst and select the subset of inputframes that will produce blur trails of controlled length, regardless of sceneor camera motion velocity. We predict inter-frame motion and synthesizemotion-blur to fill the temporal gaps between the input frames. Finally, wecomposite the blurred image with the sharp regular exposure to protect thesharpness of faces or areas of the scene that are barely moving, and produce afinal high resolution and high dynamic range (HDR) photograph. Our systemdemocratizes a capability previously reserved to professionals, and makes thiscreative style accessible to most casual photographers.More information and supplementary material can be found on our projectwebpage: ", "output": "Computational Long Exposure Mobile Photography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nowadays, autonomous cars are gaining traction due to their numerouspotential applications on battlefields and in resolving a variety of otherreal-world challenges. The main goal of our project is to build an autonomoussystem using DeepRacer which will follow a specific person (for our project, asoldier) when they will be moving in any direction. Two main components toaccomplish this project is an optimized Single-Shot Multibox Detection (SSD)object detection model and a Reinforcement Learning (RL) model. We accomplishedthe task using SSD Lite instead of SSD and at the end, compared the resultsamong SSD, SSD with Neural Computing Stick (NCS), and SSD Lite. Experimentalresults show that SSD Lite gives better performance among these threetechniques and exhibits a considerable boost in inference speed (~2-3 times)without compromising accuracy.", "output": "Follow the Soldiers with Optimized Single-Shot Multibox Detection and Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce OpenFlamingo, a family of autoregressive vision-language modelsranging from 3B to 9B parameters. OpenFlamingo is an ongoing effort to producean open-source replication of DeepMind's Flamingo models. On sevenvision-language datasets, OpenFlamingo models average between 80 - 89% ofcorresponding Flamingo performance. This technical report describes our models,training data, hyperparameters, and evaluation suite. We share our models andcode at ", "output": "OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To interact with humans in the world, agents need to understand the diversetypes of language that people use, relate them to the visual world, and actbased on them. While current agents learn to execute simple languageinstructions from task rewards, we aim to build agents that leverage diverselanguage that conveys general knowledge, describes the state of the world,provides interactive feedback, and more. Our key idea is that language helpsagents predict the future: what will be observed, how the world will behave,and which situations will be rewarded. This perspective unifies languageunderstanding with future prediction as a powerful self-supervised learningobjective. We present Dynalang, an agent that learns a multimodal world modelthat predicts future text and image representations and learns to act fromimagined model rollouts. Unlike traditional agents that use language only topredict actions, Dynalang acquires rich language understanding by using pastlanguage also to predict future language, video, and rewards. In addition tolearning from online interaction in an environment, Dynalang can be pretrainedon datasets of text, video, or both without actions or rewards. From usinglanguage hints in grid worlds to navigating photorealistic scans of homes,Dynalang utilizes diverse types of language to improve task performance,including environment descriptions, game rules, and instructions.", "output": "Learning to Model the World with Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Are current language models capable of deception and lie detection? We studythis question by introducing a text-based game called $textit{Hoodwinked}$,inspired by $textit{Mafia}$ and $textit{Among Us}$. Players are locked in ahouse and must find a key to escape, but one player is tasked with killing theothers. Each time a murder is committed, the surviving players have a naturallanguage discussion then vote to banish one player from the game. We conductexperiments with agents controlled by GPT-3, GPT-3.5, and GPT-4 and findevidence of deception and lie detection capabilities. The killer often deniestheir crime and accuses others, leading to measurable effects on votingoutcomes. More advanced models are more effective killers, outperformingsmaller models in 18 of 24 pairwise comparisons. Secondary metrics provideevidence that this improvement is not mediated by different actions, but ratherby stronger deception capabilities during discussions. Overall, we findsubstantial evidence that current language models are capable of deception. Tobetter evaluate the ability of AI agents to deceive humans, we make this gamepublicly available at  .", "output": "Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "At the beginning era of large language model, it is quite critical togenerate a high-quality financial dataset to fine-tune a large language modelfor financial related tasks. Thus, this paper presents a carefully designeddata creation pipeline for this purpose. Particularly, we initiate a dialoguebetween an AI investor and financial expert using ChatGPT and incorporate thefeedback of human financial experts, leading to the refinement of the dataset.This pipeline yielded a robust instruction tuning dataset comprised of 103kmulti-turn chats. Extensive experiments have been conducted on this dataset toevaluate the model's performance by adopting an external GPT-4 as the judge.The promising experimental results verify that our approach led to significantadvancements in generating accurate, relevant, and financial-style responsesfrom AI models, and thus providing a powerful tool for applications within thefinancial sector.", "output": "An Effective Data Creation Pipeline to Generate High-quality Financial Instruction Data for Large Language Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel methodology for modeling and forecasting multivariaterealized volatilities using customized graph neural networks to incorporatespillover effects across stocks. The proposed model offers the benefits ofincorporating spillover effects from multi-hop neighbors, capturing nonlinearrelationships, and flexible training with different loss functions. Ourempirical findings provide compelling evidence that incorporating spillovereffects from multi-hop neighbors alone does not yield a clear advantage interms of predictive accuracy. However, modeling nonlinear spillover effectsenhances the forecasting accuracy of realized volatilities, particularly forshort-term horizons of up to one week. Moreover, our results consistentlyindicate that training with the Quasi-likelihood loss leads to substantialimprovements in model performance compared to the commonly-used mean squarederror. A comprehensive series of empirical evaluations in alternative settingsconfirm the robustness of our results.", "output": "Graph Neural Networks for Forecasting Multivariate Realized Volatility with Spillover Effects."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A common way to explore text corpora is through low-dimensional projectionsof the documents, where one hopes that thematically similar documents will beclustered together in the projected space. However, popular algorithms fordimensionality reduction of text corpora, like Latent Dirichlet Allocation(LDA), often produce projections that do not capture human notions of documentsimilarity. We propose a semi-supervised human-in-the-loop LDA-based method forlearning topics that preserve semantically meaningful relationships betweendocuments in low-dimensional projections. On synthetic corpora, our methodyields more interpretable projections than baseline methods with only afraction of labels provided. On a real corpus, we obtain qualitatively similarresults.", "output": "SAP-sLDA: An Interpretable Interface for Exploring Unstructured Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work we approach attractor neural networks from a machine learningperspective: we look for optimal network parameters by applying a gradientdescent over a regularized loss function. Within this framework, the optimalneuron-interaction matrices turn out to be a class of matrices which correspondto Hebbian kernels revised by iteratively applying some unlearning protocols.Remarkably, the number of unlearning steps is proved to be related to theregularization hyperparameters of the loss function and to the training time.Thus, we can design strategies to avoid overfitting that are formulated interms of the algebraic properties of the interaction matrix, or, equivalently,in terms of regularization tuning and early-stopping strategies. Thegeneralization capabilities of these attractor networks are also investigated:analytical results are obtained for random synthetic datasets, next, theemerging picture is corroborated by numerical experiments that highlight theexistence of several regimes (i.e., overfitting, failure and success) as thedataset parameters are varied.", "output": "Regularization, early-stopping and dreaming: a Hopfield-like setup to address generalization and overfitting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "ChatMOF is an autonomous Artificial Intelligence (AI) system that is built topredict and generate of metal-organic frameworks (MOFs). By leveraging alarge-scale language model (gpt-3.5-turbo), ChatMOF extracts key details fromtextual inputs and delivers appropriate responses, thus eliminating thenecessity for rigid structured queries. The system is comprised of three corecomponents (i.e. an agent, a toolkit, and an evaluator) and it forms a robustpipeline that manages a variety of tasks, including data retrieval, propertyprediction, and structure generation. The study further explores the merits andconstraints of using large language models (LLMs) AI system in materialsciences using and showcases its transformative potential for futureadvancements.", "output": "ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The COVID-19 pandemic presented numerous challenges to healthcare systemsworldwide. Given that lung infections are prevalent among COVID-19 patients,chest Computer Tomography (CT) scans have frequently been utilized as analternative method for identifying COVID-19 conditions and various other typesof pulmonary diseases. Deep learning architectures have emerged to automate theidentification of pulmonary disease types by leveraging CT scan slices asinputs for classification models. This paper introduces COVID-VR, a novelapproach for classifying pulmonary diseases based on volume rendering images ofthe lungs captured from multiple angles, thereby providing a comprehensive viewof the entire lung in each image. To assess the effectiveness of our proposal,we compared it against competing strategies utilizing both private dataobtained from partner hospitals and a publicly available dataset. The resultsdemonstrate that our approach effectively identifies pulmonary lesions andperforms competitively when compared to slice-based methods.", "output": "COVID-VR: A Deep Learning COVID-19 Classification Model Using Volume-Rendered Computer Tomography."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While deep learning gradually penetrates operational planning, its inherentprediction errors may significantly affect electricity prices. This letterexamines how prediction errors propagate into electricity prices, revealingnotable pricing errors and their spatial disparity in congested power systems.To improve fairness, we propose to embed electricity market-clearingoptimization as a deep learning layer. Differentiating through this layerallows for balancing between prediction and pricing errors, as oppose tominimizing prediction errors alone. This layer implicitly optimizes fairnessand controls the spatial distribution of price errors across the system. Weshowcase the price-aware deep learning in the nexus of wind power forecastingand short-term electricity market clearing.", "output": "Price-Aware Deep Learning for Electricity Markets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cost-effective sensors are capable of real-time capturing a variety of airquality-related modalities from different pollutant concentrations toindoor/outdoor humidity and temperature. Machine learning (ML) models arecapable of performing air-quality \"ahead-of-time\" approximations. Undoubtedly,accurate indoor air quality approximation significantly helps provide a healthyindoor environment, optimize associated energy consumption, and offer humancomfort. However, it is crucial to design an ML architecture to capture thedomain knowledge, so-called problem physics. In this study, we propose sixnovel physics-based ML models for accurate indoor pollutant concentrationapproximations. The proposed models include an adroit combination ofstate-space concepts in physics, Gated Recurrent Units, and Decompositiontechniques. The proposed models were illustrated using data collected from fiveoffices in a commercial building in California. The proposed models are shownto be less complex, computationally more efficient, and more accurate thansimilar state-of-the-art transformer-based models. The superiority of theproposed models is due to their relatively light architecture (computationalefficiency) and, more importantly, their ability to capture the underlyinghighly nonlinear patterns embedded in the often contaminated sensor-collectedindoor air quality temporal data.", "output": "Novel Physics-Based Machine-Learning Models for Indoor Air Quality Approximations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The digital twin concept represents an appealing opportunity to advancecondition-based and predictive maintenance paradigms for civil engineeringsystems, thus allowing reduced lifecycle costs, increased system safety, andincreased system availability. This work proposes a predictive digital twinapproach to the health monitoring, maintenance, and management planning ofcivil engineering structures. The asset-twin coupled dynamical system isencoded employing a probabilistic graphical model, which allows all relevantsources of uncertainty to be taken into account. In particular, thetime-repeating observations-to-decisions flow is modeled using a dynamicBayesian network. Real-time structural health diagnostics are provided byassimilating sensed data with deep learning models. The digital twin state iscontinually updated in a sequential Bayesian inference fashion. This is thenexploited to inform the optimal planning of maintenance and management actionswithin a dynamic decision-making framework. A preliminary offline phaseinvolves the population of training datasets through a reduced-order numericalmodel and the computation of a health-dependent control policy. The strategy isassessed on two synthetic case studies, involving a cantilever beam and arailway bridge, demonstrating the dynamic decision-making capabilities ofhealth-aware digital twins.", "output": "A digital twin framework for civil engineering structures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural networks (GNNs) have brought superb performance to variousapplications utilizing graph structural data, such as social analysis and frauddetection. The graph links, e.g., social relationships and transaction history,are sensitive and valuable information, which raises privacy concerns whenusing GNNs. To exploit these vulnerabilities, we propose VertexSerum, a novelgraph poisoning attack that increases the effectiveness of graph link stealingby amplifying the link connectivity leakage. To infer node adjacency moreaccurately, we propose an attention mechanism that can be embedded into thelink detection network. Our experiments demonstrate that VertexSerumsignificantly outperforms the SOTA link inference attack, improving the AUCscores by an average of $9.8%$ across four real-world datasets and threedifferent GNN structures. Furthermore, our experiments reveal the effectivenessof VertexSerum in both black-box and online learning settings, furthervalidating its applicability in real-world scenarios.", "output": "VertexSerum: Poisoning Graph Neural Networks for Link Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A self-driving vehicle (SDV) must be able to perceive its surroundings andpredict the future behavior of other traffic participants. Existing workseither perform object detection followed by trajectory forecasting of thedetected objects, or predict dense occupancy and flow grids for the wholescene. The former poses a safety concern as the number of detections needs tobe kept low for efficiency reasons, sacrificing object recall. The latter iscomputationally expensive due to the high-dimensionality of the output grid,and suffers from the limited receptive field inherent to fully convolutionalnetworks. Furthermore, both approaches employ many computational resourcespredicting areas or objects that might never be queried by the motion planner.This motivates our unified approach to perception and future prediction thatimplicitly represents occupancy and flow over time with a single neuralnetwork. Our method avoids unnecessary computation, as it can be directlyqueried by the motion planner at continuous spatio-temporal locations.Moreover, we design an architecture that overcomes the limited receptive fieldof previous explicit occupancy prediction methods by adding an efficient yeteffective global attention mechanism. Through extensive experiments in bothurban and highway settings, we demonstrate that our implicit model outperformsthe current state-of-the-art. For more information, visit the project website:", "output": "Implicit Occupancy Flow Fields for Perception and Prediction in Self-Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image diffusion models such as Stable Diffusion have recentlyattracted the interest of many researchers, and inverting the diffusion processcan play an important role in better understanding the generative process andhow to engineer prompts in order to obtain the desired images. To this end, weintroduce the new task of predicting the text prompt given an image generatedby a generative diffusion model. We combine a series of white-box and black-boxmodels (with and without access to the weights of the diffusion network) todeal with the proposed task. We propose a novel learning framework comprisingof a joint prompt regression and multi-label vocabulary classificationobjective that generates improved prompts. To further improve our method, weemploy a curriculum learning procedure that promotes the learning ofimage-prompt pairs with lower labeling noise (i.e. that are better aligned),and an unsupervised domain-adaptive kernel learning method that uses thesimilarities between samples in the source and target domains as extrafeatures. We conduct experiments on the DiffusionDB data set, predicting textprompts from images generated by Stable Diffusion. Our novel learning frameworkproduces excellent results on the aforementioned task, yielding the highestgains when applied on the white-box model. In addition, we make an interestingdiscovery: training a diffusion model on the prompt generation task can makethe model generate images that are much better aligned with the input prompts,when the model is directly reused for text-to-image generation.", "output": "Reverse Stable Diffusion: What prompt was used to generate this image?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "New technologies have led to vast troves of large and complex datasets acrossmany scientific domains and industries. People routinely use machine learningtechniques to not only process, visualize, and make predictions from this bigdata, but also to make data-driven discoveries. These discoveries are oftenmade using Interpretable Machine Learning, or machine learning models andtechniques that yield human understandable insights. In this paper, we discussand review the field of interpretable machine learning, focusing especially onthe techniques as they are often employed to generate new knowledge or makediscoveries from large data sets. We outline the types of discoveries that canbe made using Interpretable Machine Learning in both supervised andunsupervised settings. Additionally, we focus on the grand challenge of how tovalidate these discoveries in a data-driven manner, which promotes trust inmachine learning systems and reproducibility in science. We discuss validationfrom both a practical perspective, reviewing approaches based on data-splittingand stability, as well as from a theoretical perspective, reviewing statisticalresults on model selection consistency and uncertainty quantification viastatistical inference. Finally, we conclude by highlighting open challenges inusing interpretable machine learning techniques to make discoveries, includinggaps between theory and practice for validating data-driven-discoveries.", "output": "Interpretable Machine Learning for Discovery: Statistical Challenges \\& Opportunities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study the online overlapping batch-means covariance estimator forStochastic Gradient Descent (SGD) under Markovian sampling. We show that theconvergence rates of the covariance estimator are$Obig(sqrt{d},n^{-1/8}(log n)^{1/4}big)$ and$Obig(sqrt{d},n^{-1/8}big)$ under state-dependent and state-independentMarkovian sampling, respectively, with $d$ representing dimensionality and $n$denoting the number of observations or SGD iterations. Remarkably, these ratesmatch the best-known convergence rate previously established for theindependent and identically distributed ($iid$) case by cite{zhu2021online},up to logarithmic factors. Our analysis overcomes significant challenges thatarise due to Markovian sampling, leading to the introduction of additionalerror terms and complex dependencies between the blocks of the batch-meanscovariance estimator. Moreover, we establish the convergence rate for the firstfour moments of the $ell_2$ norm of the error of SGD dynamics understate-dependent Markovian data, which holds potential interest as anindependent result. To validate our theoretical findings, we provide numericalillustrations to derive confidence intervals for SGD when training linear andlogistic regression models under Markovian sampling. Additionally, we apply ourapproach to tackle the intriguing problem of strategic classification withlogistic regression, where adversaries can adaptively modify features duringthe training process to increase their chances of being classified in aspecific target class.", "output": "Online covariance estimation for stochastic gradient descent under Markovian sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Real-time rendering for video games has become increasingly challenging dueto the need for higher resolutions, framerates and photorealism. Supersamplinghas emerged as an effective solution to address this challenge. Our workintroduces a novel neural algorithm for supersampling rendered content that is4 times more efficient than existing methods while maintaining the same levelof accuracy. Additionally, we introduce a new dataset which provides auxiliarymodalities such as motion vectors and depth generated using graphics renderingfeatures like viewport jittering and mipmap biasing at different resolutions.We believe that this dataset fills a gap in the current dataset landscape andcan serve as a valuable resource to help measure progress in the field andadvance the state-of-the-art in super-resolution techniques for gaming content.", "output": "Efficient neural supersampling on a novel gaming dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "$Q$ learning is a popular model free reinforcement learning method. Most ofexisting works focus on analyzing $Q$ learning for finite state and actionspaces. If the state space is continuous, then the original $Q$ learning methodcan not be directly used. A modification of the original $Q$ learning methodwas proposed in (Shah and Xie, 2018), which estimates $Q$ values with nearestneighbors. Such modification makes $Q$ learning suitable for continuous statespace. (Shah and Xie, 2018) shows that the convergence rate of estimated $Q$function is $tilde{O}(T^{-1/(d+3)})$, which is slower than the minimax lowerbound $tilde{Omega}(T^{-1/(d+2)})$, indicating that this method is notefficient. This paper proposes two new $Q$ learning methods to bridge the gapof convergence rates in (Shah and Xie, 2018), with one of them being offline,while the other is online. Despite that we still use nearest neighbor approachto estimate $Q$ function, the algorithms are crucially different from (Shah andXie, 2018). In particular, we replace the kernel nearest neighbor indiscretized region with a direct nearest neighbor approach. Consequently, ourapproach significantly improves the convergence rate. Moreover, the timecomplexity is also significantly improved in high dimensional state spaces. Ouranalysis shows that both offline and online methods are minimax rate optimal.", "output": "Minimax Optimal $Q$ Learning with Nearest Neighbors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image generative models can produce photo-realistic images for anextremely broad range of concepts, and their usage has proliferated widelyamong the general public. On the flip side, these models have numerousdrawbacks, including their potential to generate images featuring sexuallyexplicit content, mirror artistic styles without permission, or evenhallucinate (or deepfake) the likenesses of celebrities. Consequently, variousmethods have been proposed in order to \"erase\" sensitive concepts fromtext-to-image models. In this work, we examine five recently proposed concepterasure methods, and show that targeted concepts are not fully excised from anyof these methods. Specifically, we leverage the existence of special learnedword embeddings that can retrieve \"erased\" concepts from the sanitized modelswith no alterations to their weights. Our results highlight the brittleness ofpost hoc concept erasure methods, and call into question their use in thealgorithmic toolkit for AI safety.", "output": "Circumventing Concept Erasure Methods For Text-to-Image Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Face swapping is a task that changes a facial identity of a given image tothat of another person. In this work, we propose a novel face-swappingframework called Megapixel Facial Identity Manipulation (MFIM). Theface-swapping model should achieve two goals. First, it should be able togenerate a high-quality image. We argue that a model which is proficient ingenerating a megapixel image can achieve this goal. However, generating amegapixel image is generally difficult without careful model design. Therefore,our model exploits pretrained StyleGAN in the manner of GAN-inversion toeffectively generate a megapixel image. Second, it should be able toeffectively transform the identity of a given image. Specifically, it should beable to actively transform ID attributes (e.g., face shape and eyes) of a givenimage into those of another person, while preserving ID-irrelevant attributes(e.g., pose and expression). To achieve this goal, we exploit 3DMM that cancapture various facial attributes. Specifically, we explicitly supervise ourmodel to generate a face-swapped image with the desirable attributes using3DMM. We show that our model achieves state-of-the-art performance throughextensive experiments. Furthermore, we propose a new operation called IDmixing, which creates a new identity by semantically mixing the identities ofseveral people. It allows the user to customize the new identity.", "output": "MFIM: Megapixel Facial Identity Manipulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We explore AI-powered upscaling as a design assistance tool in the context ofcreating 2D game levels. Deep neural networks are used to upscale artificiallydownscaled patches of levels from the puzzle platformer game Lode Runner. Thetrained networks are incorporated into a web-based editor, where the user cancreate and edit levels at three different levels of resolution: 4x4, 8x8, and16x16. An edit at any resolution instantly transfers to the other resolutions.As upscaling requires inventing features that might not be present at lowerresolutions, we train neural networks to reproduce these features. We introducea neural network architecture that is capable of not only learning upscalingbut also giving higher priority to less frequent tiles. To investigate thepotential of this tool and guide further development, we conduct a qualitativestudy with 3 designers to understand how they use it. Designers enjoyedco-designing with the tool, liked its underlying concept, and provided feedbackfor further improvement.", "output": "Lode Enhancer: Level Co-creation Through Scaling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have shown promising results in cross-modal generationtasks, including text-to-image and text-to-audio generation. However,generating music, as a special type of audio, presents unique challenges due tolimited availability of music data and sensitive issues related to copyrightand plagiarism. In this paper, to tackle these challenges, we first construct astate-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusionand AudioLDM architectures to the music domain. We achieve this by retrainingthe contrastive language-audio pretraining model (CLAP) and the Hifi-GANvocoder, as components of MusicLDM, on a collection of music data samples.Then, to address the limitations of training data and to avoid plagiarism, weleverage a beat tracking model and propose two different mixup strategies fordata augmentation: beat-synchronous audio mixup and beat-synchronous latentmixup, which recombine training audio directly or via a latent embeddingsspace, respectively. Such mixup strategies encourage the model to interpolatebetween musical training samples and generate new music within the convex hullof the training data, making the generated music more diverse while stillstaying faithful to the corresponding style. In addition to popular evaluationmetrics, we design several new evaluation metrics based on CLAP score todemonstrate that our proposed MusicLDM and beat-synchronous mixup strategiesimprove both the quality and novelty of generated music, as well as thecorrespondence between input text and generated music.", "output": "MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This research paper delves into the integration of OpenAI's ChatGPT intoembodied agent systems, evaluating its influence on interactive decision-makingbenchmark. Drawing a parallel to the concept of people assuming roles accordingto their unique strengths, we introduce InterAct. In this approach, we feedChatGPT with varied prompts, assigning it a numerous roles like a checker and asorter, then integrating them with the original language model. Our researchshows a remarkable success rate of 98% in AlfWorld, which consists of 6different tasks in a simulated household environment, emphasizing thesignificance of proficient prompt engineering. The results highlight ChatGPT'scompetence in comprehending and performing intricate tasks effectively inreal-world settings, thus paving the way for further advancements in taskplanning.", "output": "InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning priors on trajectory distributions can help accelerate robot motionplanning optimization. Given previously successful plans, learning trajectorygenerative models as priors for a new planning problem is highly desirable.Prior works propose several ways on utilizing this prior to bootstrapping themotion planning problem. Either sampling the prior for initializations or usingthe prior distribution in a maximum-a-posterior formulation for trajectoryoptimization. In this work, we propose learning diffusion models as priors. Wethen can sample directly from the posterior trajectory distribution conditionedon task goals, by leveraging the inverse denoising process of diffusion models.Furthermore, diffusion has been recently shown to effectively encode datamultimodality in high-dimensional settings, which is particularly well-suitedfor large trajectory dataset. To demonstrate our method efficacy, we compareour proposed method - Motion Planning Diffusion - against several baselines insimulated planar robot and 7-dof robot arm manipulator environments. To assessthe generalization capabilities of our method, we test it in environments withpreviously unseen obstacles. Our experiments show that diffusion models arestrong priors to encode high-dimensional trajectory distributions of robotmotions.", "output": "Motion Planning Diffusion: Learning and Planning of Robot Motions with Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While a practical wireless network has many tiers where end users do notdirectly communicate with the central server, the users' devices have limitedcomputation and battery powers, and the serving base station (BS) has a fixedbandwidth. Owing to these practical constraints and system models, this paperleverages model pruning and proposes a pruning-enabled hierarchical federatedlearning (PHFL) in heterogeneous networks (HetNets). We first derive an upperbound of the convergence rate that clearly demonstrates the impact of the modelpruning and wireless communications between the clients and the associated BS.Then we jointly optimize the model pruning ratio, central processing unit (CPU)frequency and transmission power of the clients in order to minimize thecontrollable terms of the convergence bound under strict delay and energyconstraints. However, since the original problem is not convex, we performsuccessive convex approximation (SCA) and jointly optimize the parameters forthe relaxed convex problem. Through extensive simulation, we validate theeffectiveness of our proposed PHFL algorithm in terms of test accuracy, wallclock time, energy consumption and bandwidth requirement.", "output": "Hierarchical Federated Learning in Wireless Networks: Pruning Tackles Bandwidth Scarcity and System Heterogeneity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An increasingly important building block of large scale machine learningsystems is based on returning slates; an ordered lists of items given a query.Applications of this technology include: search, information retrieval andrecommender systems. When the action space is large, decision systems arerestricted to a particular structure to complete online queries quickly. Thispaper addresses the optimization of these large scale decision systems given anarbitrary reward function. We cast this learning problem in a policyoptimization framework and propose a new class of policies, born from a novelrelaxation of decision functions. This results in a simple, yet efficientlearning algorithm that scales to massive action spaces. We compare our methodto the commonly adopted Plackett-Luce policy class and demonstrate theeffectiveness of our approach on problems with action space sizes in the orderof millions.", "output": "Fast Slate Policy Optimization: Going Beyond Plackett-Luce."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The diffusion model is capable of generating high-quality data through aprobabilistic approach. However, it suffers from the drawback of slowgeneration speed due to the requirement of a large number of time steps. Toaddress this limitation, recent models such as denoising diffusion implicitmodels (DDIM) focus on generating samples without directly modeling theprobability distribution, while models like denoising diffusion generativeadversarial networks (GAN) combine diffusion processes with GANs. In the fieldof speech synthesis, a recent diffusion speech synthesis model calledDiffGAN-TTS, utilizing the structure of GANs, has been introduced anddemonstrates superior performance in both speech quality and generation speed.In this paper, to further enhance the performance of DiffGAN-TTS, we propose aspeech synthesis model with two discriminators: a diffusion discriminator forlearning the distribution of the reverse process and a spectrogramdiscriminator for learning the distribution of the generated data. Objectivemetrics such as structural similarity index measure (SSIM), mel-cepstraldistortion (MCD), F0 root mean squared error (F0 RMSE), short-time objectiveintelligibility (STOI), perceptual evaluation of speech quality (PESQ), as wellas subjective metrics like mean opinion score (MOS), are used to evaluate theperformance of the proposed model. The evaluation results show that theproposed model outperforms recent state-of-the-art models such as FastSpeech2and DiffGAN-TTS in various metrics. Our implementation and audio samples arelocated on GitHub.", "output": "Adversarial Training of Denoising Diffusion Model Using Dual Discriminators for High-Fidelity Multi-Speaker TTS."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised representation learning approaches aim to learn discriminativefeature representations from unlabeled data, without the requirement ofannotating every sample. Enabling unsupervised representation learning isextremely crucial for time series data, due to its unique annotation bottleneckcaused by its complex characteristics and lack of visual cues compared withother data modalities. In recent years, unsupervised representation learningtechniques have advanced rapidly in various domains. However, there is a lackof systematic analysis of unsupervised representation learning approaches fortime series. To fill the gap, we conduct a comprehensive literature review ofexisting rapidly evolving unsupervised representation learning approaches fortime series. Moreover, we also develop a unified and standardized library,named ULTS (i.e., Unsupervised Learning for Time Series), to facilitate fastimplementations and unified evaluations on various models. With ULTS, weempirically evaluate state-of-the-art approaches, especially the rapidlyevolving contrastive learning methods, on 9 diverse real-world datasets. Wefurther discuss practical considerations as well as open research challenges onunsupervised representation learning for time series to facilitate futureresearch in this field.", "output": "Unsupervised Representation Learning for Time Series: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mesh-based simulations play a key role when modeling complex physical systemsthat, in many disciplines across science and engineering, require the solutionof parametrized time-dependent nonlinear partial differential equations (PDEs).In this context, full order models (FOMs), such as those relying on the finiteelement method, can reach high levels of accuracy, however often yieldingintensive simulations to run. For this reason, surrogate models are developedto replace computationally expensive solvers with more efficient ones, whichcan strike favorable trade-offs between accuracy and efficiency. This workexplores the potential usage of graph neural networks (GNNs) for the simulationof time-dependent PDEs in the presence of geometrical variability. Inparticular, we propose a systematic strategy to build surrogate models based ona data-driven time-stepping scheme where a GNN architecture is used toefficiently evolve the system. With respect to the majority of surrogatemodels, the proposed approach stands out for its ability of tackling problemswith parameter dependent spatial domains, while simultaneously generalizing todifferent geometries and mesh resolutions. We assess the effectiveness of theproposed approach through a series of numerical experiments, involving bothtwo- and three-dimensional problems, showing that GNNs can provide a validalternative to traditional surrogate models in terms of computationalefficiency and generalization to new scenarios. We also assess, from anumerical standpoint, the importance of using GNNs, rather than classical densedeep neural networks, for the proposed framework.", "output": "Deep Learning-based surrogate models for parametrized PDEs: handling geometric variability through graph neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised multiplex graph learning (UMGL) has been shown to achievesignificant effectiveness for different downstream tasks by exploring bothcomplementary information and consistent information among multiple graphs.However, previous methods usually overlook the issues in practicalapplications, i.e., the out-of-sample issue and the noise issue. To address theabove issues, in this paper, we propose an effective and efficient UMGL methodto explore both complementary and consistent information. To do this, ourmethod employs multiple MLP encoders rather than graph convolutional network(GCN) to conduct representation learning with two constraints, i.e., preservingthe local graph structure among nodes to handle the out-of-sample issue, andmaximizing the correlation of multiple node representations to handle the noiseissue. Comprehensive experiments demonstrate that our proposed method achievessuperior effectiveness and efficiency over the comparison methods andeffectively tackles those two issues. Code is available at", "output": "Unsupervised Multiplex Graph Learning with Complementary and Consistent Information."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The presence of label noise in the training data has a profound impact on thegeneralization of deep neural networks (DNNs). In this study, we introduce andtheoretically demonstrate a simple feature noise method, which directly addsnoise to the features of training data, can enhance the generalization of DNNsunder label noise. Specifically, we conduct theoretical analyses to reveal thatlabel noise leads to weakened DNN generalization by loosening the PAC-Bayesgeneralization bound, and feature noise results in better DNN generalization byimposing an upper bound on the mutual information between the model weights andthe features, which constrains the PAC-Bayes generalization bound. Furthermore,to ensure effective generalization of DNNs in the presence of label noise, weconduct application analyses to identify the optimal types and levels offeature noise to add for obtaining desirable label noise generalization.Finally, extensive experimental results on several popular datasets demonstratethe feature noise method can significantly enhance the label noisegeneralization of the state-of-the-art label noise method.", "output": "Feature Noise Boosts DNN Generalization under Label Noise."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the advancement of DNNs into safety-critical applications, testingapproaches for such models have gained more attention. A current direction isthe search for and identification of systematic weaknesses that put safetyassumptions based on average performance values at risk. Such weaknesses cantake on the form of (semantically coherent) subsets or areas in the input spacewhere a DNN performs systematically worse than its expected average. However,it is non-trivial to attribute the reason for such observed low performances tothe specific semantic features that describe the subset. For instance,inhomogeneities within the data w.r.t. other (non-considered) attributes mightdistort results. However, taking into account all (available) attributes andtheir interaction is often computationally highly expensive. Inspired bycounterfactual explanations, we propose an effective and computationally cheapalgorithm to validate the semantic attribution of existing subsets, i.e., tocheck whether the identified attribute is likely to have caused the degradedperformance. We demonstrate this approach on an example from the autonomousdriving domain using highly annotated simulated data, where we show for asemantic segmentation model that (i) performance differences among thedifferent pedestrian assets exist, but (ii) only in some cases is the assettype itself the reason for this reduction in the performance.", "output": "Assessing Systematic Weaknesses of DNNs using Counterfactuals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper introduces a new Convolutional Neural Network (ConvNet)architecture inspired by a class of partial differential equations (PDEs)called quasi-linear hyperbolic systems. With comparable performance on imageclassification task, it allows for the modification of the weights via acontinuous group of symmetry. This is a significant shift from traditionalmodels where the architecture and weights are essentially fixed. We wish topromote the (internal) symmetry as a new desirable property for a neuralnetwork, and to draw attention to the PDE perspective in analyzing andinterpreting ConvNets in the broader Deep Learning community.", "output": "A Novel Convolutional Neural Network Architecture with a Continuous Symmetry."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An attractive book cover is important for the success of a book. In thispaper, we apply Generative Adversarial Networks (GANs) to the book coversdomain, using different methods for training in order to obtain bettergenerated images. We interleave GANs with knowledge graphs to alter the inputtitle to obtain multiple possible options for any given title, which are thenused as an augmented input to the generator. Finally, we use the discriminatorobtained during the training phase to select the best images generated with newtitles. Our method performed better at generating book covers than previousattempts, and the knowledge graph gives better options to the book author oreditor compared to using GANs alone.", "output": "Interleaving GANs with knowledge graphs to support design creativity for book covers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Maintaining a balance between the supply and demand of products by optimizingreplenishment decisions is one of the most important challenges in the supplychain industry. This paper presents a novel reinforcement learning frameworkcalled MARLIM, to address the inventory management problem for a single-echelonmulti-products supply chain with stochastic demands and lead-times. Within thiscontext, controllers are developed through single or multiple agents in acooperative setting. Numerical experiments on real data demonstrate thebenefits of reinforcement learning methods over traditional baselines.", "output": "MARLIM: Multi-Agent Reinforcement Learning for Inventory Management."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph and hypergraph representation learning has attracted increasingattention from various research fields. Despite the decent performance andfruitful applications of Graph Neural Networks (GNNs), Hypergraph NeuralNetworks (HGNNs), and their well-designed variants, on some commonly usedbenchmark graphs and hypergraphs, they are outperformed by even a simpleMulti-Layer Perceptron. This observation motivates a reexamination of thedesign paradigm of the current GNNs and HGNNs and poses challenges ofextracting graph features effectively. In this work, a universal featureencoder for both graph and hypergraph representation learning is designed,called UniG-Encoder. The architecture starts with a forward transformation ofthe topological relationships of connected nodes into edge or hyperedgefeatures via a normalized projection matrix. The resulting edge/hyperedgefeatures, together with the original node features, are fed into a neuralnetwork. The encoded node embeddings are then derived from the reversedtransformation, described by the transpose of the projection matrix, of thenetwork's output, which can be further used for tasks such as nodeclassification. The proposed architecture, in contrast to the traditionalspectral-based and/or message passing approaches, simultaneously andcomprehensively exploits the node features and graph/hypergraph topologies inan efficient and unified manner, covering both heterophilic and homophilicgraphs. The designed projection matrix, encoding the graph features, isintuitive and interpretable. Extensive experiments are conducted anddemonstrate the superior performance of the proposed framework on twelverepresentative hypergraph datasets and six real-world graph datasets, comparedto the state-of-the-art methods. Our implementation is available online at", "output": "UniG-Encoder: A Universal Feature Encoder for Graph and Hypergraph Node Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "(Economic) nonlinear model predictive control ((e)NMPC) requires dynamicsystem models that are sufficiently accurate in all relevant state-spaceregions. These models must also be computationally cheap enough to ensurereal-time tractability. Data-driven surrogate models for mechanistic models canbe used to reduce the computational burden of (e)NMPC; however, such models aretypically trained by system identification for maximum average predictionaccuracy on simulation samples and perform suboptimally as part of actual(e)NMPC. We present a method for end-to-end reinforcement learning of dynamicsurrogate models for optimal performance in (e)NMPC applications, resulting inpredictive controllers that strike a favorable balance between controlperformance and computational demand. We validate our method on twoapplications derived from an established nonlinear continuous stirred-tankreactor model. We compare the controller performance to that of MPCs utilizingmodels trained by the prevailing maximum prediction accuracy paradigm, andmodel-free neural network controllers trained using reinforcement learning. Weshow that our method matches the performance of the model-free neural networkcontrollers while consistently outperforming models derived from systemidentification. Additionally, we show that the MPC policies can react tochanges in the control setting without retraining.", "output": "End-to-End Reinforcement Learning of Koopman Models for Economic Nonlinear MPC."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider convex relaxations for recovering low-rank tensors based onconstrained minimization over a ball induced by the tensor nuclear norm,recently introduced in cite{tensor_tSVD}. We build on a recent line of resultsthat considered convex relaxations for the recovery of low-rank matrices andestablished that under a strict complementarity condition (SC), both theconvergence rate and per-iteration runtime of standard gradient methods mayimprove dramatically. We develop the appropriate strict complementaritycondition for the tensor nuclear norm ball and obtain the following mainresults under this condition: 1. When the objective to minimize is of the form$f(mX)=g(mAmX)+langle{mC,mX}rangle$ , where $g$ is strongly convex and$mA$ is a linear map (e.g., least squares), a quadratic growth bound holds,which implies linear convergence rates for standard projected gradient methods,despite the fact that $f$ need not be strongly convex. 2. For a smoothobjective function, when initialized in certain proximity of an optimalsolution which satisfies SC, standard projected gradient methods only requireSVD computations (for projecting onto the tensor nuclear norm ball) of rankthat matches the tubal rank of the optimal solution. In particular, when thetubal rank is constant, this implies nearly linear (in the size of the tensor)runtime per iteration, as opposed to super linear without further assumptions.3. For a nonsmooth objective function which admits a popular smoothsaddle-point formulation, we derive similar results to the latter for the wellknown extragradient method. An additional contribution which may be ofindependent interest, is the rigorous extension of many basic results regardingtensors of arbitrary order, which were previously obtained only for third-ordertensors.", "output": "Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Machine Learning (GML) has numerous applications, such as node/graphclassification and link prediction, in real-world domains. Providinghuman-understandable explanations for GML models is a challenging yetfundamental task to foster their adoption, but validating explanations for linkprediction models has received little attention. In this paper, we providequantitative metrics to assess the quality of link prediction explanations,with or without ground-truth. State-of-the-art explainability methods for GraphNeural Networks are evaluated using these metrics. We discuss how underlyingassumptions and technical details specific to the link prediction task, such asthe choice of distance between node embeddings, can influence the quality ofthe explanations.", "output": "Evaluating Link Prediction Explanations for Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present novel cross-sectional and longitudinal claim count models forvehicle insurance built upon the Combined Actuarial Neural Network (CANN)framework proposed by Mario W\"uthrich and Michael Merz. The CANN approachcombines a classical actuarial model, such as a generalized linear model, witha neural network. This blending of models results in a two-component modelcomprising a classical regression model and a neural network part. The CANNmodel leverages the strengths of both components, providing a solid foundationand interpretability from the classical model while harnessing the flexibilityand capacity to capture intricate relationships and interactions offered by theneural network. In our proposed models, we use well-known log-linear claimcount regression models for the classical regression part and a multilayerperceptron (MLP) for the neural network part. The MLP part is used to processtelematics car driving data given as a vector characterizing the drivingbehavior of each insured driver. In addition to the Poisson and negativebinomial distributions for cross-sectional data, we propose a procedure fortraining our CANN model with a multivariate negative binomial (MVNB)specification. By doing so, we introduce a longitudinal model that accounts forthe dependence between contracts from the same insured. Our results reveal thatthe CANN models exhibit superior performance compared to log-linear models thatrely on manually engineered telematics features.", "output": "Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predictive variability due to data ambiguities has typically been addressedvia construction of dedicated models with built-in probabilistic capabilitiesthat are trained to predict uncertainty estimates as variables of interest.These approaches require distinct architectural components and trainingmechanisms, may include restrictive assumptions and exhibit overconfidence,i.e., high confidence in imprecise predictions. In this work, we propose apost-hoc sampling strategy for estimating predictive uncertainty accounting fordata ambiguity. The method can generate different plausible outputs for a giveninput and does not assume parametric forms of predictive distributions. It isarchitecture agnostic and can be applied to any feed-forward deterministicnetwork without changes to the architecture or training procedure. Experimentson regression tasks on imaging and non-imaging input data show the method'sability to generate diverse and multi-modal predictive distributions, and adesirable correlation of the estimated uncertainty with the prediction error.", "output": "Quantification of Predictive Uncertainty via Inference-Time Sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the widespread application of personalized online services,click-through rate (CTR) prediction has received more and more attention andresearch. The most prominent features of CTR prediction are its multi-fieldcategorical data format, and vast and daily-growing data volume. The largecapacity of neural models helps digest such massive amounts of data under thesupervised learning paradigm, yet they fail to utilize the substantial data toits full potential, since the 1-bit click signal is not sufficient to guide themodel to learn capable representations of features and instances. Theself-supervised learning paradigm provides a more promising pretrain-finetunesolution to better exploit the large amount of user click logs, and learn moregeneralized and effective representations. However, self-supervised learningfor CTR prediction is still an open question, since current works on this lineare only preliminary and rudimentary. To this end, we propose a Model-agnosticpretraining (MAP) framework that applies feature corruption and recovery onmulti-field categorical data, and more specifically, we derive two practicalalgorithms: masked feature prediction (MFP) and replaced feature detection(RFD). MFP digs into feature interactions within each instance through maskingand predicting a small portion of input features, and introduces noisecontrastive estimation (NCE) to handle large feature spaces. RFD further turnsMFP into a binary classification mode through replacing and detecting changesin input features, making it even simpler and more effective for CTRpretraining. Our extensive experiments on two real-world large-scale datasets(i.e., Avazu, Criteo) demonstrate the advantages of these two methods onseveral strong backbones (e.g., DCNv2, DeepFM), and achieve newstate-of-the-art performance in terms of both effectiveness and efficiency forCTR prediction.", "output": "MAP: A Model-agnostic Pretraining Framework for Click-through Rate Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Label Distribution Learning (LDL) is a novel machine learning paradigm thatassigns label distribution to each instance. Many LDL methods proposed toleverage label correlation in the learning process to solve theexponential-sized output space; among these, many exploited the low-rankstructure of label distribution to capture label correlation. However, recentstudies disclosed that label distribution matrices are typically full-rank,posing challenges to those works exploiting low-rank label correlation. Notethat multi-label is generally low-rank; low-rank label correlation is widelyadopted in multi-label learning (MLL) literature. Inspired by that, weintroduce an auxiliary MLL process in LDL and capture low-rank labelcorrelation on that MLL rather than LDL. In such a way, low-rank labelcorrelation is appropriately exploited in our LDL methods. We conductcomprehensive experiments and demonstrate that our methods are superior toexisting LDL methods. Besides, the ablation studies justify the advantages ofexploiting low-rank label correlation in the auxiliary MLL.", "output": "Exploiting Multi-Label Correlation in Label Distribution Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The turbulent jet ignition concept using prechambers is a promising solutionto achieve stable combustion at lean conditions in large gas engines, leadingto high efficiency at low emission levels. Due to the wide range of design andoperating parameters for large gas engine prechambers, the preferred method forevaluating different designs is computational fluid dynamics (CFD), as testingin test bed measurement campaigns is time-consuming and expensive. However, thesignificant computational time required for detailed CFD simulations due to thecomplexity of solving the underlying physics also limits its applicability. Inoptimization settings similar to the present case, i.e., where the evaluationof the objective function(s) is computationally costly, Bayesian optimizationhas largely replaced classical design-of-experiment. Thus, the present studydeals with the computationally efficient Bayesian optimization of large gasengine prechambers design using CFD simulation. Reynolds-averaged-Navier-Stokessimulations are used to determine the target values as a function of theselected prechamber design parameters. The results indicate that the chosenstrategy is effective to find a prechamber design that achieves the desiredtarget values.", "output": "Finding the Optimum Design of Large Gas Engines Prechambers Using CFD and Bayesian Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multitask learning is a powerful framework that enables one to simultaneouslylearn multiple related tasks by sharing information between them. Quantifyinguncertainty in the estimated tasks is of pivotal importance for many downstreamapplications, such as online or active learning. In this work, we provide novelmultitask confidence intervals in the challenging agnostic setting, i.e., whenneither the similarity between tasks nor the tasks' features are available tothe learner. The obtained intervals do not require i.i.d. data and can bedirectly applied to bound the regret in online learning. Through a refinedanalysis of the multitask information gain, we obtain new regret guaranteesthat, depending on a task similarity parameter, can significantly improve overtreating tasks independently. We further propose a novel online learningalgorithm that achieves such improved regret without knowing this parameter inadvance, i.e., automatically adapting to task similarity. As a second keyapplication of our results, we introduce a novel multitask active learningsetup where several tasks must be simultaneously optimized, but only one ofthem can be queried for feedback by the learner at each round. For thisproblem, we design a no-regret algorithm that uses our confidence intervals todecide which task should be queried. Finally, we empirically validate ourbounds and algorithms on synthetic and real-world (drug discovery) data.", "output": "Multitask Learning with No Regret: from Improved Confidence Bounds to Active Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "How to enable learnability for new classes while keeping the capability wellon old classes has been a crucial challenge for class incremental learning.Beyond the normal case, long-tail class incremental learning and few-shot classincremental learning are also proposed to consider the data imbalance and datascarcity, respectively, which are common in real-world implementations andfurther exacerbate the well-known problem of catastrophic forgetting. Existingmethods are specifically proposed for one of the three tasks. In this paper, weoffer a unified solution to the misalignment dilemma in the three tasks.Concretely, we propose neural collapse terminus that is a fixed structure withthe maximal equiangular inter-class separation for the whole label space. Itserves as a consistent target throughout the incremental training to avoiddividing the feature space incrementally. For CIL and LTCIL, we further proposea prototype evolving scheme to drive the backbone features into our neuralcollapse terminus smoothly. Our method also works for FSCIL with only minoradaptations. Theoretical analysis indicates that our method holds the neuralcollapse optimality in an incremental fashion regardless of data imbalance ordata scarcity. We also design a generalized case where we do not know the totalnumber of classes and whether the data distribution is normal, long-tail, orfew-shot for each coming session, to test the generalizability of our method.Extensive experiments with multiple datasets are conducted to demonstrate theeffectiveness of our unified solution to all the three tasks and thegeneralized case.", "output": "Neural Collapse Terminus: A Unified Solution for Class Incremental Learning and Its Variants."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient exploration in complex environments remains a major challenge forreinforcement learning (RL). Compared to previous Thompson sampling-inspiredmechanisms that enable temporally extended exploration, i.e., deep exploration,we focus on deep exploration in distributional RL. We develop here a generalpurpose approach, Bag of Policies (BoP), that can be built on top of any returndistribution estimator by maintaining a population of its copies. BoP consistsof an ensemble of multiple heads that are updated independently. Duringtraining, each episode is controlled by only one of the heads and the collectedstate-action pairs are used to update all heads off-policy, leading to distinctlearning signals for each head which diversify learning and behaviour. To testwhether optimistic ensemble method can improve on distributional RL as did onscalar RL, by e.g. Bootstrapped DQN, we implement the BoP approach with apopulation of distributional actor-critics using Bayesian Distributional PolicyGradients (BDPG). The population thus approximates a posterior distribution ofreturn distributions along with a posterior distribution of policies. Anotherbenefit of building upon BDPG is that it allows to analyze global posterioruncertainty along with local curiosity bonus simultaneously for exploration. AsBDPG is already an optimistic method, this pairing helps to investigate ifoptimism is accumulatable in distributional RL. Overall BoP results in greaterrobustness and speed during learning as demonstrated by our experimentalresults on ALE Atari games.", "output": "Bag of Policies for Distributional Deep Exploration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This study investigated the potential of end-to-end deep learning tools as amore effective substitute for FEM in predicting stress-strain fields within 2Dcross sections of arterial wall. We first proposed a U-Net based fullyconvolutional neural network (CNN) to predict the von Mises stress and straindistribution based on the spatial arrangement of calcification within arterialwall cross-sections. Further, we developed a conditional generative adversarialnetwork (cGAN) to enhance, particularly from the perceptual perspective, theprediction accuracy of stress and strain field maps for arterial walls withvarious calcification quantities and spatial configurations. On top of U-Netand cGAN, we also proposed their ensemble approaches, respectively, to furtherimprove the prediction accuracy of field maps. Our dataset, consisting of inputand output images, was generated by implementing boundary conditions andextracting stress-strain field maps. The trained U-Net models can accuratelypredict von Mises stress and strain fields, with structural similarity indexscores (SSIM) of 0.854 and 0.830 and mean squared errors of 0.017 and 0.018 forstress and strain, respectively, on a reserved test set. Meanwhile, the cGANmodels in a combination of ensemble and transfer learning techniquesdemonstrate high accuracy in predicting von Mises stress and strain fields, asevidenced by SSIM scores of 0.890 for stress and 0.803 for strain.Additionally, mean squared errors of 0.008 for stress and 0.017 for strainfurther support the model's performance on a designated test set. Overall, thisstudy developed a surrogate model for finite element analysis, which canaccurately and efficiently predict stress-strain fields of arterial wallsregardless of complex geometries and boundary conditions.", "output": "Deep Learning-based Prediction of Stress and Strain Maps in Arterial Walls for Improved Cardiovascular Risk Assessment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, Variational Quantum Algorithms (VQAs) have emerged as apromising approach for solving optimization problems on quantum computers inthe NISQ era. However, one limitation of VQAs is their reliance onfixed-structure circuits, which may not be taylored for specific problems orhardware configurations. A leading strategy to address this issue areAdaptative VQAs, which dynamically modify the circuit structure by adding andremoving gates, and optimize their parameters during the training. SeveralAdaptative VQAs, based on heuristics such as circuit shallowness, entanglementcapability and hardware compatibility, have already been proposed in theliterature, but there is still lack of a systematic comparison between thedifferent methods. In this paper, we aim to fill this gap by analyzing threeAdaptative VQAs: Evolutionary Variational Quantum Eigensolver (EVQE), VariableAnsatz (VAns), already proposed in the literature, and Random Adapt-VQE(RA-VQE), a random approach we introduce as a baseline. In order to comparethese algorithms to traditional VQAs, we also include the Quantum ApproximateOptimization Algorithm (QAOA) in our analysis. We apply these algorithms toQUBO problems and study their performance by examining the quality of thesolutions found and the computational times required. Additionally, weinvestigate how the choice of the hyperparameters can impact the overallperformance of the algorithms, highlighting the importance of selecting anappropriate methodology for hyperparameter tuning. Our analysis sets benchmarksfor Adaptative VQAs designed for near-term quantum devices and providesvaluable insights to guide future research in this area.", "output": "Benchmarking Adaptative Variational Quantum Algorithms on QUBO Instances."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Job scheduling is a well-known Combinatorial Optimization problem withendless applications. Well planned schedules bring many benefits in the contextof automated systems: among others, they limit production costs and waste.Nevertheless, the NP-hardness of this problem makes it essential to useheuristics whose design is difficult, requires specialized knowledge and oftenproduces methods tailored to the specific task. This paper presents an originalend-to-end Deep Reinforcement Learning approach to scheduling thatautomatically learns dispatching rules. Our technique is inspired by naturallanguage encoder-decoder models for sequence processing and has never beenused, to the best of our knowledge, for scheduling purposes. We applied andtested our method in particular to some benchmark instances of Job ShopProblem, but this technique is general enough to be potentially used to tackleother different optimal job scheduling tasks with minimal intervention. Resultsdemonstrate that we outperform many classical approaches exploiting prioritydispatching rules and show competitive results on state-of-the-art DeepReinforcement Learning ones.", "output": "Job Shop Scheduling via Deep Reinforcement Learning: a Sequence to Sequence approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Going beyond stochastic gradient descent (SGD), what new phenomena emerge inwide neural networks trained by adaptive optimizers like Adam? Here we show:The same dichotomy between feature learning and kernel behaviors (as in SGD)holds for general optimizers as well, including Adam -- albeit with a nonlinearnotion of \"kernel.\" We derive the corresponding \"neural tangent\" and \"maximalupdate\" limits for any architecture. Two foundational advances underlie theabove results: 1) A new Tensor Program language, NEXORT, that can express howadaptive optimizers process gradients into updates. 2) The introduction ofbra-ket notation to drastically simplify expressions and calculations in TensorPrograms. This work summarizes and generalizes all previous results in theTensor Programs series of papers.", "output": "Tensor Programs IVb: Adaptive Optimization in the Infinite-Width Limit."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial training (AT) is widely considered the state-of-the-art techniquefor improving the robustness of deep neural networks (DNNs) against adversarialexamples (AE). Nevertheless, recent studies have revealed that adversariallytrained models are prone to unfairness problems, restricting theirapplicability. In this paper, we empirically observe that this limitation maybe attributed to serious adversarial confidence overfitting, i.e., certainadversarial examples with overconfidence. To alleviate this problem, we proposeHAM, a straightforward yet effective framework via adaptive Hard Adversarialexample Mining.HAM concentrates on mining hard adversarial examples whilediscarding the easy ones in an adaptive fashion. Specifically, HAM identifieshard AEs in terms of their step sizes needed to cross the decision boundarywhen calculating loss value. Besides, an early-dropping mechanism isincorporated to discard the easy examples at the initial stages of AEgeneration, resulting in efficient AT. Extensive experimental results onCIFAR-10, SVHN, and Imagenette demonstrate that HAM achieves significantimprovement in robust fairness while reducing computational cost compared toseveral state-of-the-art adversarial training methods. The code will be madepublicly available.", "output": "Hard Adversarial Example Mining for Improving Robust Fairness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The current work investigates the capability of Large language models (LLMs)that are explicitly trained on large corpuses of medical knowledge (Med-PaLM 2)to predict psychiatric functioning from patient interviews and clinicaldescriptions without being trained to do so. To assess this, n = 145 depressionand n =115 PTSD assessments and n = 46 clinical case studies across highprevalence/high comorbidity disorders (Depressive, Anxiety, Psychotic, traumaand stress, Addictive disorders) were analyzed using prompts to extractestimated clinical scores and diagnoses. Results demonstrate that Med-PaLM 2 iscapable of assessing psychiatric functioning across a range of psychiatricconditions with the strongest performance being the prediction of depressionscores based on standardized assessments (Accuracy range= 0.80 - 0.84) whichwere statistically indistinguishable from human clinical raters t(1,144) =1.20; p = 0.23. Results show the potential for general clinical language modelsto flexibly predict psychiatric risk based on free descriptions of functioningfrom both patients and clinicians.", "output": "The Capability of Large Language Models to Measure Psychiatric Functioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One of the key objects of binary classification is the regression function,i.e., the conditional expectation of the class labels given the inputs. Withthe regression function not only a Bayes optimal classifier can be defined, butit also encodes the corresponding misclassification probabilities. The paperpresents a resampling framework to construct exact, distribution-free andnon-asymptotically guaranteed confidence regions for the true regressionfunction for any user-chosen confidence level. Then, specific algorithms aresuggested to demonstrate the framework. It is proved that the constructedconfidence regions are strongly consistent, that is, any false model isexcluded in the long run with probability one. The exclusion is quantified withprobably approximately correct type bounds, as well. Finally, the algorithmsare validated via numerical experiments, and the methods are compared toapproximate asymptotic confidence ellipsoids.", "output": "Distribution-Free Inference for the Regression Function of Binary Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning models are known to be vulnerable to adversarial evasionattacks as illustrated by image classification models. Thoroughly understandingsuch attacks is critical in order to ensure the safety and robustness ofcritical AI tasks. However, most evasion attacks are difficult to deployagainst a majority of AI systems because they have focused on image domain withonly few constraints. An image is composed of homogeneous, numerical,continuous, and independent features, unlike many other input types to AIsystems used in practice. Furthermore, some input types include additionalsemantic and functional constraints that must be observed to generate realisticadversarial inputs. In this work, we propose a new framework to enable thegeneration of adversarial inputs irrespective of the input type and taskdomain. Given an input and a set of pre-defined input transformations, ourframework discovers a sequence of transformations that result in a semanticallycorrect and functional adversarial input. We demonstrate the generality of ourapproach on several diverse machine learning tasks with various inputrepresentations. We also show the importance of generating adversarial examplesas they enable the deployment of mitigation techniques.", "output": "URET: Universal Robustness Evaluation Toolkit (for Evasion)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Fine-tuning language models in a downstream task is the standard approach formany state-of-the-art methodologies in the field of NLP. However, when thedistribution between the source task and target task drifts, textit{e.g.},conversational environments, these gains tend to be diminished. This articleproposes a sequence of pre-training steps (a curriculum) guided by \"datahacking\" and grammar analysis that allows further gradual adaptation betweenpre-training distributions. In our experiments, we acquire a considerableimprovement from our method compared to other known pre-training approaches forthe MultiWoZ task.", "output": "Curricular Transfer Learning for Sentence Encoded Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Distribution shifts are a serious concern in modern statistical learning asthey can systematically change the properties of the data away from the truth.We focus on Wasserstein distribution shifts, where every data point may undergoa slight perturbation, as opposed to the Huber contamination model where afraction of observations are outliers. We formulate and study shifts beyondindependent perturbations, exploring Joint Distribution Shifts, where theper-observation perturbations can be coordinated. We analyze several importantstatistical problems, including location estimation, linear regression, andnon-parametric density estimation. Under a squared loss for mean estimation andprediction error in linear regression, we find the exact minimax risk, a leastfavorable perturbation, and show that the sample mean and least squaresestimators are respectively optimal. This holds for both independent and jointshifts, but the least favorable perturbations and minimax risks differ. Forother problems, we provide nearly optimal estimators and precise finite-samplebounds. We also introduce several tools for bounding the minimax risk underdistribution shift, such as a smoothing technique for location families, andgeneralizations of classical tools including least favorable sequences ofpriors, the modulus of continuity, Le Cam's, Fano's, and Assouad's methods.", "output": "Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU,DPU), deploying deep learning models on edge devices with fixed-point hardwareis still challenging due to complex model quantization and conversion. Existingmodel quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], andQualcomm AIMET [3] supports only a limited set of quantization schemes (e.g.,only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deeplearning models cannot be easily quantized for diverse fixed-point hardwares,mainly due to slightly different quantization requirements. In this paper, weenvision a new type of model quantization approach called MRQ (modelre-quantization), which takes existing quantized models and quickly transformsthe models to meet different quantization requirements (e.g., asymmetric -&gt;symmetric, non-power-of-2 scale -&gt; power-of-2 scale). Re-quantization is muchsimpler than quantizing from scratch because it avoids costly re-training andprovides support for multiple quantization schemes simultaneously. To minimizere-quantization error, we developed a new set of re-quantization algorithmsincluding weight correction and rounding error folding. We have demonstratedthat MobileNetV2 QAT model [7] can be quickly re-quantized into two differentquantization schemes (i.e., symmetric and symmetric+power-of-2 scale) with lessthan 0.64 units of accuracy loss. We believe our work is the first to leveragethis concept of re-quantization for model quantization and models obtained fromthe re-quantization process have been successfully deployed on NNA in the EchoShow devices.", "output": "MRQ:Support Multiple Quantization Schemes through Model Re-Quantization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Global Climate Models (GCMs) are the primary tool to simulate climateevolution and assess the impacts of climate change. However, they often operateat a coarse spatial resolution that limits their accuracy in reproducinglocal-scale phenomena. Statistical downscaling methods leveraging deep learningoffer a solution to this problem by approximating local-scale climate fieldsfrom coarse variables, thus enabling regional GCM projections. Typically,climate fields of different variables of interest are downscaled independently,resulting in violations of fundamental physical properties acrossinterconnected variables. This study investigates the scope of this problemand, through an application on temperature, lays the foundation for a frameworkintroducing multi-variable hard constraints that guarantees physicalrelationships between groups of downscaled climate variables.", "output": "Multi-variable Hard Physical Constraints for Climate Model Downscaling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-label image recognition in the low-label regime is a task of greatchallenge and practical significance. Previous works have focused on learningthe alignment between textual and visual spaces to compensate for limited imagelabels, yet may suffer from reduced accuracy due to the scarcity ofhigh-quality multi-label annotations. In this research, we leverage thepowerful alignment between textual and visual features pretrained with millionsof auxiliary image-text pairs. We introduce an efficient and effectiveframework called Evidence-guided Dual Context Optimization (DualCoOp++), whichserves as a unified approach for addressing partial-label and zero-shotmulti-label recognition. In DualCoOp++ we separately encode evidential,positive, and negative contexts for target classes as parametric components ofthe linguistic input (i.e., prompts). The evidential context aims to discoverall the related visual content for the target class, and serves as guidance toaggregate positive and negative contexts from the spatial domain of the image,enabling better distinguishment between similar categories. Additionally, weintroduce a Winner-Take-All module that promotes inter-class interaction duringtraining, while avoiding the need for extra parameters and costs. As DualCoOp++imposes minimal additional learnable overhead on the pretrained vision-languageframework, it enables rapid adaptation to multi-label recognition tasks withlimited annotations and even unseen classes. Experiments on standardmulti-label recognition benchmarks across two challenging low-label settingsdemonstrate the superior performance of our approach compared tostate-of-the-art methods.", "output": "DualCoOp++: Fast and Effective Adaptation to Multi-Label Recognition with Limited Annotations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Identification of nonlinear dynamical systems has been popularized by sparseidentification of the nonlinear dynamics (SINDy) via the sequentiallythresholded least squares (STLS) algorithm. Many extensions SINDy have emergedin the literature to deal with experimental data which are finite in length andnoisy. Recently, the computationally intensive method of ensemblingbootstrapped SINDy models (E-SINDy) was proposed for model identification,handling finite, highly noisy data. While the extensions of SINDy are numerous,their sparsity-promoting estimators occasionally provide sparse approximationsof the dynamics as opposed to exact recovery. Furthermore, these estimatorssuffer under multicollinearity, e.g. the irrepresentable condition for theLasso. In this paper, we demonstrate that the Trimmed Lasso for robustidentification of models (TRIM) can provide exact recovery under more severenoise, finite data, and multicollinearity as opposed to E-SINDy. Additionally,the computational cost of TRIM is asymptotically equal to STLS since thesparsity parameter of the TRIM can be solved efficiently by convex solvers. Wecompare these methodologies on challenging nonlinear systems, specifically theLorenz 63 system, the Bouc Wen oscillator from the nonlinear dynamics benchmarkof No\"el and Schoukens, 2016, and a time delay system describing tool cuttingdynamics. This study emphasizes the comparisons between STLS, reweighted$ell_1$ minimization, and Trimmed Lasso in identification with respect toproblems faced by practitioners: the problem of finite and noisy data, theperformance of the sparse regression of when the library grows in dimension(multicollinearity), and automatic methods for choice of regularizationparameters.", "output": "Exact identification of nonlinear dynamical systems by Trimmed Lasso."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning seeks to enable deep learners to train on a series oftasks of unknown length without suffering from the catastrophic forgetting ofprevious tasks. One effective solution is replay, which involves storing fewprevious experiences in memory and replaying them when learning the currenttask. However, there is still room for improvement when it comes to selectingthe most informative samples for storage and determining the optimal number ofsamples to be stored. This study aims to address these issues with a novelcomparison of the commonly used reservoir sampling to various alternativepopulation strategies and providing a novel detailed analysis of how to findthe optimal number of stored samples.", "output": "Improving Replay Sample Selection and Storage for Less Forgetting in Continual Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Preprints play an increasingly critical role in academic communities. Thereare many reasons driving researchers to post their manuscripts to preprintservers before formal submission to journals or conferences, but the use ofpreprints has also sparked considerable controversy, especially surrounding theclaim of priority. In this paper, a case study of computer science preprintssubmitted to arXiv from 2008 to 2017 is conducted to quantify how manypreprints have eventually been printed in peer-reviewed venues. Among thosepublished manuscripts, some are published under different titles and without anupdate to their preprints on arXiv. In the case of these manuscripts, thetraditional fuzzy matching method is incapable of mapping the preprint to thefinal published version. In view of this issue, we introduce a semantics-basedmapping method with the employment of Bidirectional Encoder Representationsfrom Transformers (BERT). With this new mapping method and a plurality of datasources, we find that 66% of all sampled preprints are published underunchanged titles and 11% are published under different titles and with othermodifications. A further analysis was then performed to investigate why thesepreprints but not others were accepted for publication. Our comparison revealsthat in the field of computer science, published preprints feature adequaterevisions, multiple authorship, detailed abstract and introduction, extensiveand authoritative references and available source code.", "output": "How many preprints have actually been printed and why: a case study of computer science preprints on arXiv."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Depth completion, which aims to generate high-quality dense depth maps fromsparse depth maps, has attracted increasing attention in recent years. Previouswork usually employs RGB images as guidance, and introduces iterative spatialpropagation to refine estimated coarse depth maps. However, most of thepropagation refinement methods require several iterations and suffer from afixed receptive field, which may contain irrelevant and useless informationwith very sparse input. In this paper, we address these two challengessimultaneously by revisiting the idea of deformable convolution. We propose aneffective architecture that leverages deformable kernel convolution as asingle-pass refinement module, and empirically demonstrate its superiority. Tobetter understand the function of deformable convolution and exploit it fordepth completion, we further systematically investigate a variety ofrepresentative strategies. Our study reveals that, different from prior work,deformable convolution needs to be applied on an estimated depth map with arelatively high density for better performance. We evaluate our model on thelarge-scale KITTI dataset and achieve state-of-the-art level performance inboth accuracy and inference speed. Our code is available at", "output": "Revisiting Deformable Convolution for Depth Completion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have revolutionized NLP by solving downstreamtasks with little to no labeled data. Despite their versatile abilities, thelarger question of their ability to reason remains ill-understood. This paperaddresses reasoning in math word problems (MWPs) by studying symbolic versionsof the numeric problems, since a symbolic expression is a \"concise explanation\"of the numeric answer. We create and use a symbolic version of the SVAMPdataset and find that GPT-3's davinci-002 model also has good zero-shotaccuracy on symbolic MWPs. To evaluate the faithfulness of the model'sreasoning, we go beyond accuracy and additionally evaluate the alignmentbetween the final answer and the outputted reasoning, which correspond tonumeric and symbolic answers respectively for MWPs. We explore a self-promptingapproach to encourage the symbolic reasoning to align with the numeric answer,thus equipping the LLM with the ability to provide a concise and verifiablereasoning and making it more interpretable. Surprisingly, self-prompting alsoimproves the symbolic accuracy to be higher than both the numeric and symbolicaccuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will bereleased for future research on symbolic math problems.", "output": "Reasoning in Large Language Models Through Symbolic Math Word Problems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have shown that deep neural networks (DNNs) are vulnerable toadversarial attacks, including evasion and backdoor (poisoning) attacks. On thedefense side, there have been intensive efforts on improving both empirical andprovable robustness against evasion attacks; however, the provable robustnessagainst backdoor attacks still remains largely unexplored. In this paper, wefocus on certifying the machine learning model robustness against generalthreat models, especially backdoor attacks. We first provide a unifiedframework via randomized smoothing techniques and show how it can beinstantiated to certify the robustness against both evasion and backdoorattacks. We then propose the first robust training process, RAB, to smooth thetrained model and certify its robustness against backdoor attacks. We prove therobustness bound for machine learning models trained with RAB and prove thatour robustness bound is tight. In addition, we theoretically show that it ispossible to train the robust smoothed models efficiently for simple models suchas K-nearest neighbor classifiers, and we propose an exact smooth-trainingalgorithm that eliminates the need to sample from a noise distribution for suchmodels. Empirically, we conduct comprehensive experiments for different machinelearning (ML) models such as DNNs, support vector machines, and K-NN models onMNIST, CIFAR-10, and ImageNette datasets and provide the first benchmark forcertified robustness against backdoor attacks. In addition, we evaluate K-NNmodels on a spambase tabular dataset to demonstrate the advantages of theproposed exact algorithm. Both the theoretic analysis and the comprehensiveevaluation on diverse ML models and datasets shed light on further robustlearning strategies against general training time attacks.", "output": "RAB: Provable Robustness Against Backdoor Attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the degree-Rips construction from topological data analysis,which provides a density-sensitive, multiparameter hierarchical clusteringalgorithm. We analyze its stability to perturbations of the input data usingthe correspondence-interleaving distance, a metric for hierarchical clusteringsthat we introduce. Taking certain one-parameter slices of degree-Rips recoverswell-known methods for density-based clustering, but we show that these methodsare unstable. However, we prove that degree-Rips, as a multiparameter object,is stable, and we propose an alternative approach for taking slices ofdegree-Rips, which yields a one-parameter hierarchical clustering algorithmwith better stability properties. We prove that this algorithm is consistent,using the correspondence-interleaving distance. We provide an algorithm forextracting a single clustering from one-parameter hierarchical clusterings,which is stable with respect to the correspondence-interleaving distance. And,we integrate these methods into a pipeline for density-based clustering, whichwe call Persistable. Adapting tools from multiparameter persistent homology, wepropose visualization tools that guide the selection of all parameters of thepipeline. We demonstrate Persistable on benchmark datasets, showing that itidentifies multi-scale cluster structure in data.", "output": "Stable and consistent density-based clustering via multiparameter persistence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Albeit being a prevalent architecture searching approach, differentiablearchitecture search (DARTS) is largely hindered by its substantial memory costsince the entire supernet resides in the memory. This is where the single-pathDARTS comes in, which only chooses a single-path submodel at each step. Whilebeing memory-friendly, it also comes with low computational costs. Nonetheless,we discover a critical issue of single-path DARTS that has not been primarilynoticed. Namely, it also suffers from severe performance collapse since toomany parameter-free operations like skip connections are derived, just likeDARTS does. In this paper, we propose a new algorithm called RObustifyingMemory-Efficient NAS (ROME) to give a cure. First, we disentangle the topologysearch from the operation search to make searching and evaluation consistent.We then adopt Gumbel-Top2 reparameterization and gradient accumulation torobustify the unwieldy bi-level optimization. We verify ROME extensively across15 benchmarks to demonstrate its effectiveness and robustness.", "output": "ROME: Robustifying Memory-Efficient NAS via Topology Disentanglement and Gradient Accumulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a novel interpretable tree based algorithm for prediction in aregression setting. Our motivation is to estimate the unknown regressionfunction from a functional decomposition perspective in which the functionalcomponents correspond to lower order interaction terms. The idea is to modifythe random forest algorithm by keeping certain leaves after they are splitinstead of deleting them. This leads to non-binary trees which we refer to asplanted trees. An extension to a forest leads to our random planted forestalgorithm. Additionally, the maximum number of covariates which can interactwithin a leaf can be bounded. If we set this interaction bound to one, theresulting estimator is a sum of one-dimensional functions. In the other extremecase, if we do not set a limit, the resulting estimator and corresponding modelplace no restrictions on the form of the regression function. In a simulationstudy we find encouraging prediction and visualisation properties of our randomplanted forest method. We also develop theory for an idealized version ofrandom planted forests in cases where the interaction bound is low. We showthat if it is smaller than three, the idealized version achieves asymptoticallyoptimal convergence rates up to a logarithmic factor. Code is available onGitHub ", "output": "Random Planted Forest: a directly interpretable tree ensemble."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As neural networks become more popular, the need for accompanying uncertaintyestimates increases. There are currently two main approaches to test thequality of these estimates. Most methods output a density. They can be comparedby evaluating their loglikelihood on a test set. Other methods output aprediction interval directly. These methods are often tested by examining thefraction of test points that fall inside the corresponding predictionintervals. Intuitively both approaches seem logical. However, we demonstratethrough both theoretical arguments and simulations that both ways of evaluatingthe quality of uncertainty estimates have serious flaws. Firstly, bothapproaches cannot disentangle the separate components that jointly create thepredictive uncertainty, making it difficult to evaluate the quality of theestimates of these components. Secondly, a better loglikelihood does notguarantee better prediction intervals, which is what the methods are often usedfor in practice. Moreover, the current approach to test prediction intervalsdirectly has additional flaws. We show why it is fundamentally flawed to test aprediction or confidence interval on a single test set. At best, marginalcoverage is measured, implicitly averaging out overconfident and underconfidentpredictions. A much more desirable property is pointwise coverage, requiringthe correct coverage for each prediction. We demonstrate through practicalexamples that these effects can result in favoring a method, based on thepredictive uncertainty, that has undesirable behaviour of the confidence orprediction intervals. Finally, we propose a simulation-based testing approachthat addresses these problems while still allowing easy comparison betweendifferent methods.", "output": "How to Evaluate Uncertainty Estimates in Machine Learning for Regression?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A longstanding goal in reinforcement learning is to build intelligent agentsthat show fast learning and a flexible transfer of skills akin to humans andanimals. This paper investigates the integration of two frameworks for tacklingthose goals: episodic control and successor features. Episodic control is acognitively inspired approach relying on episodic memory, an instance-basedmemory model of an agent's experiences. Meanwhile, successor features andgeneralized policy improvement (SF&amp;GPI) is a meta and transfer learningframework allowing to learn policies for tasks that can be efficiently reusedfor later tasks which have a different reward function. Individually, these twotechniques have shown impressive results in vastly improving sample efficiencyand the elegant reuse of previously learned policies. Thus, we outline acombination of both approaches in a single reinforcement learning framework andempirically illustrate its benefits.", "output": "Successor Feature Neural Episodic Control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Integrating knowledge across different domains is an essential feature ofhuman learning. Learning paradigms such as transfer learning, meta learning,and multi-task learning reflect the human learning process by exploiting theprior knowledge for new tasks, encouraging faster learning and goodgeneralization for new tasks. This article gives a detailed view of theselearning paradigms and their comparative analysis. The weakness of one learningalgorithm turns out to be a strength of another, and thus merging them is aprevalent trait in the literature. There are numerous research papers thatfocus on each of these learning paradigms separately and provide acomprehensive overview of them. However, this article provides a review ofresearch studies that combine (two of) these learning algorithms. This surveydescribes how these techniques are combined to solve problems in many differentfields of study, including computer vision, natural language processing,hyperspectral imaging, and many more, in supervised setting only. As a result,the global generic learning network an amalgamation of meta learning, transferlearning, and multi-task learning is introduced here, along with some openresearch questions and future research directions in the multi-task setting.", "output": "Sharing to learn and learning to share -- Fitting together Meta-Learning, Multi-Task Learning, and Transfer Learning: A meta review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual learning is a promising machine learning paradigm to learn newtasks while retaining previously learned knowledge over streaming trainingdata. Till now, rehearsal-based methods, keeping a small part of data from oldtasks as a memory buffer, have shown good performance in mitigatingcatastrophic forgetting for previously learned knowledge. However, most ofthese methods typically treat each new task equally, which may not adequatelyconsider the relationship or similarity between old and new tasks. Furthermore,these methods commonly neglect sample importance in the continual trainingprocess and result in sub-optimal performance on certain tasks. To address thischallenging problem, we propose Relational Experience Replay (RER), a bi-levellearning framework, to adaptively tune task-wise relationships and sampleimportance within each task to achieve a better `stability' and `plasticity'trade-off. As such, the proposed method is capable of accumulating newknowledge while consolidating previously learned old knowledge during continuallearning. Extensive experiments conducted on three publicly available datasets(i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet) show that the proposed methodcan consistently improve the performance of all baselines and surpass currentstate-of-the-art methods.", "output": "Relational Experience Replay: Continual Learning by Adaptively Tuning Task-wise Relationship."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Cross-modal representation learning learns a shared embedding between two ormore modalities to improve performance in a given task compared to using onlyone of the modalities. Cross-modal representation learning from different datatypes -- such as images and time-series data (e.g., audio or text data) --requires a deep metric learning loss that minimizes the distance between themodality embeddings. In this paper, we propose to use the contrastive ortriplet loss, which uses positive and negative identities to create samplepairs with different labels, for cross-modal representation learning betweenimage and time-series modalities (CMR-IS). By adapting the triplet loss forcross-modal representation learning, higher accuracy in the main (time-seriesclassification) task can be achieved by exploiting additional information ofthe auxiliary (image classification) task. We present a triplet loss with adynamic margin for single label and sequence-to-sequence classification tasks.We perform extensive evaluations on synthetic image and time-series data, andon data for offline handwriting recognition (HWR) and on online HWR fromsensor-enhanced pens for classifying written words. Our experiments show animproved classification accuracy, faster convergence, and bettergeneralizability due to an improved cross-modal representation. Furthermore,the more suitable generalizability leads to a better adaptability betweenwriters for online HWR.", "output": "Auxiliary Cross-Modal Representation Learning with Triplet Loss Functions for Online Handwriting Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the rise of the popularity and usage of neural networks, trustworthyuncertainty estimation is becoming increasingly essential. One of the mostprominent uncertainty estimation methods is Deep Ensembles (Lakshminarayanan etal., 2017) . A classical parametric model has uncertainty in the parameters dueto the fact that the data on which the model is build is a random sample. Amodern neural network has an additional uncertainty component since theoptimization of the network is random. Lakshminarayanan et al. (2017) notedthat Deep Ensembles do not incorporate the classical uncertainty induced by theeffect of finite data. In this paper, we present a computationally cheapextension of Deep Ensembles for the regression setting, called BootstrappedDeep Ensembles, that explicitly takes this classical effect of finite data intoaccount using a modified version of the parametric bootstrap. We demonstratethrough an experimental study that our method significantly improves uponstandard Deep Ensembles", "output": "Confident Neural Network Regression with Bootstrapped Deep Ensembles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As one of the most pervasive applications of machine learning, recommendersystems are playing an important role on assisting human decision making. Thesatisfaction of users and the interests of platforms are closely related to thequality of the generated recommendation results. However, as a highlydata-driven system, recommender system could be affected by data or algorithmicbias and thus generate unfair results, which could weaken the reliance of thesystems. As a result, it is crucial to address the potential unfairnessproblems in recommendation settings. Recently, there has been growing attentionon fairness considerations in recommender systems with more and more literatureon approaches to promote fairness in recommendation. However, the studies arerather fragmented and lack a systematic organization, thus making it difficultto penetrate for new researchers to the domain. This motivates us to provide asystematic survey of existing works on fairness in recommendation. This surveyfocuses on the foundations for fairness in recommendation literature. It firstpresents a brief introduction about fairness in basic machine learning taskssuch as classification and ranking in order to provide a general overview offairness research, as well as introduce the more complex situations andchallenges that need to be considered when studying fairness in recommendersystems. After that, the survey will introduce fairness in recommendation witha focus on the taxonomies of current fairness definitions, the typicaltechniques for improving fairness, as well as the datasets for fairness studiesin recommendation. The survey also talks about the challenges and opportunitiesin fairness research with the hope of promoting the fair recommendationresearch area and beyond.", "output": "Fairness in Recommendation: Foundations, Methods and Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We deal with a general distributed constrained online learning problem withprivacy over time-varying networks, where a class of nondecomposable objectivesare considered. Under this setting, each node only controls a part of theglobal decision, and the goal of all nodes is to collaboratively minimize theglobal cost over a time horizon $T$ while guarantees the security of thetransmitted information. For such problems, we first design a novel genericalgorithm framework, named as DPSDA, of differentially private distributedonline learning using the Laplace mechanism and the stochastic variants of dualaveraging method. Note that in the dual updates, all nodes of DPSDA employ thenoise-corrupted gradients for more generality. Then, we propose two algorithms,named as DPSDA-C and DPSDA-PS, under this framework. In DPSDA-C, the nodesimplement a circulation-based communication in the primal updates so as toalleviate the disagreements over time-varying undirected networks. In addition,for the extension to time-varying directed ones, the nodes implement thebroadcast-based push-sum dynamics in DPSDA-PS, which can achieve averageconsensus over arbitrary directed networks. Theoretical results show that bothalgorithms attain an expected regret upper bound in $mathcal{O}( sqrt{T} )$when the objective function is convex, which matches the best utilityachievable by cutting-edge algorithms. Finally, numerical experiment results onboth synthetic and real-world datasets verify the effectiveness of ouralgorithms.", "output": "Distributed Online Private Learning of Convex Nondecomposable Objectives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Learning with Noisy Labels (LNL) has become an appealing topic, asimperfectly annotated data are relatively cheaper to obtain. Recentstate-of-the-art approaches employ specific selection mechanisms to separateclean and noisy samples and then apply Semi-Supervised Learning (SSL)techniques for improved performance. However, the selection step mostlyprovides a medium-sized and decent-enough clean subset, which overlooks a richset of clean samples. To fulfill this, we propose a novel LNL framework ProMixthat attempts to maximize the utility of clean samples for boosted performance.Key to our method, we propose a matched high confidence selection techniquethat selects those examples with high confidence scores and matched predictionswith given labels to dynamically expand a base clean sample set. To overcomethe potential side effect of excessive clean set selection procedure, wefurther devise a novel SSL framework that is able to train balanced andunbiased classifiers on the separated clean and noisy samples. Extensiveexperiments demonstrate that ProMix significantly advances the currentstate-of-the-art results on multiple benchmarks with different types and levelsof noise. It achieves an average improvement of 2.48% on the CIFAR-N dataset.The code is available at ", "output": "ProMix: Combating Label Noise via Maximizing Clean Sample Utility."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The development of technologies for causal inference with the privacypreservation of distributed data has attracted considerable attention in recentyears. To address this issue, we propose a data collaboration quasi-experiment(DC-QE) that enables causal inference from distributed data with privacypreservation. In our method, first, local parties constructdimensionality-reduced intermediate representations from the private data.Second, they share intermediate representations, instead of private data forprivacy preservation. Third, propensity scores were estimated from the sharedintermediate representations. Finally, the treatment effects were estimatedfrom propensity scores. Our method can reduce both random errors and biases,whereas existing methods can only reduce random errors in the estimation oftreatment effects. Through numerical experiments on both artificial andreal-world data, we confirmed that our method can lead to better estimationresults than individual analyses. Dimensionality-reduction loses some of theinformation in the private data and causes performance degradation. However, weobserved that in the experiments, sharing intermediate representations withmany parties to resolve the lack of subjects and covariates, our methodimproved performance enough to overcome the degradation caused bydimensionality-reduction. With the spread of our method, intermediaterepresentations can be published as open data to help researchers findcausalities and accumulated as a knowledge base.", "output": "Collaborative causal inference on distributed data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual Question Answering (VQA) based on multi-modal data facilitatesreal-life applications such as home robots and medical diagnoses. Onesignificant challenge is to devise a robust decentralized learning frameworkfor various client models where centralized data collection is refrained due toconfidentiality concerns. This work aims to tackle privacy-preserving VQA bydecoupling a multi-modal model into representation modules and a contrastivemodule and leveraging inter-module gradients sharing and inter-client weightsharing. To this end, we propose Bidirectional Contrastive Split Learning(BiCSL) to train a global multi-modal model on the entire data distribution ofdecentralized clients. We employ the contrastive loss that enables a moreefficient self-supervised learning of decentralized modules. Comprehensiveexperiments are conducted on the VQA-v2 dataset based on five SOTA VQA models,demonstrating the effectiveness of the proposed method. Furthermore, we inspectBiCSL's robustness against a dual-key backdoor attack on VQA. Consequently,BiCSL shows much better robustness to the multi-modal adversarial attackcompared to the centralized learning method, which provides a promisingapproach to decentralized multi-modal learning.", "output": "Bidirectional Contrastive Split Learning for Visual Question Answering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the advent of the big data era, the data quality problem is becomingmore critical. Among many factors, data with missing values is one primaryissue, and thus developing effective imputation models is a key topic in theresearch community. Recently, a major research direction is to employ neuralnetwork models such as self-organizing mappings or automatic encoders forfilling missing values. However, these classical methods can hardly discoverinterrelated features and common features simultaneously among data attributes.Especially, it is a very typical problem for classical autoencoders that theyoften learn invalid constant mappings, which dramatically hurts the fillingperformance. To solve the above-mentioned problems, we propose amissing-value-filling model based on a feature-fusion-enhanced autoencoder. Wefirst incorporate into an autoencoder a hidden layer that consists ofde-tracking neurons and radial basis function neurons, which can enhance theability of learning interrelated features and common features. Besides, wedevelop a missing value filling strategy based on dynamic clustering that isincorporated into an iterative optimization process. This design can enhancethe multi-dimensional feature fusion ability and thus improves the dynamiccollaborative missing-value-filling performance. The effectiveness of theproposed model is validated by extensive experiments compared to a variety ofbaseline methods on thirteen data sets.", "output": "A Missing Value Filling Model Based on Feature Fusion Enhanced Autoencoder."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent approximations to backpropagation (BP) have mitigated many of BP'scomputational inefficiencies and incompatibilities with biology, but importantlimitations still remain. Moreover, the approximations significantly decreaseaccuracy in benchmarks, suggesting that an entirely different approach may bemore fruitful. Here, grounded on recent theory for Hebbian learning in softwinner-take-all networks, we present multilayer SoftHebb, i.e. an algorithmthat trains deep neural networks, without any feedback, target, or errorsignals. As a result, it achieves efficiency by avoiding weight transport,non-local plasticity, time-locking of layer updates, iterative equilibria, and(self-) supervisory or other feedback signals -- which were necessary in otherapproaches. Its increased efficiency and biological compatibility do not tradeoff accuracy compared to state-of-the-art bio-plausible learning, but ratherimprove it. With up to five hidden layers and an added linear classifier,accuracies on MNIST, CIFAR-10, STL-10, and ImageNet, respectively reach 99.4%,80.3%, 76.2%, and 27.3%. In conclusion, SoftHebb shows with a radicallydifferent approach from BP that Deep Learning over few layers may be plausiblein the brain and increases the accuracy of bio-plausible machine learning. Codeis available at ", "output": "Hebbian Deep Learning Without Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In peer review systems, reviewers are often asked to evaluate variousfeatures of submissions, such as technical quality or novelty. A score is givento each of the predefined features and based on these the reviewer has toprovide an overall quantitative recommendation. It may be assumed that eachreviewer has her own mapping from the set of features to a recommendation, andthat different reviewers have different mappings in mind. This introduces anelement of arbitrariness known as commensuration bias. In this paper we discussa framework, introduced by Noothigattu, Shah and Procaccia, and then applied bythe organizers of the AAAI 2022 conference. Noothigattu, Shah and Procacciaproposed to aggregate reviewer's mapping by minimizing certain loss functions,and studied axiomatic properties of this approach, in the sense of socialchoice theory. We challenge several of the results and assumptions used intheir work and report a number of negative results. On the one hand, we study atrade-off between some of the axioms proposed and the ability of the method toproperly capture agreements of the majority of reviewers. On the other hand, weshow that dropping a certain unrealistic assumption has dramatic effects,including causing the method to be discontinuous.", "output": "No Agreement Without Loss: Learning and Social Choice in Peer Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Clustering analysis of sequence data continues to address many applicationsin engineering design, aided with the rapid growth of machine learning inapplied science. This paper presents an unsupervised machine learning algorithmto extract defining characteristics of earthquake ground-motion spectra, alsocalled latent features, to aid in ground-motion selection (GMS). In thiscontext, a latent feature is a low-dimensional machine-discovered spectralcharacteristic learned through nonlinear relationships of a neural networkautoencoder. Machine discovered latent features can be combined withtraditionally defined intensity measures and clustering can be performed toselect a representative subgroup from a large ground-motion suite. Theobjective of efficient GMS is to choose characteristic records representativeof what the structure will probabilistically experience in its lifetime. Threeexamples are presented to validate this approach, including the use ofsynthetic and field recorded ground-motion datasets. The presented deepembedding clustering of ground-motion spectra has three main advantages: 1.defining characteristics the represent the sparse spectral content ofground-motions are discovered efficiently through training of the autoencoder,2. domain knowledge is incorporated into the machine learning framework withconditional variables in the deep embedding scheme, and 3. method exhibitsexcellent performance when compared to a benchmark seismic hazard analysis.", "output": "An Unsupervised Machine Learning Approach for Ground-Motion Spectra Clustering and Selection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider the inverse acoustic obstacle problem for sound-soft star-shapedobstacles in two dimensions wherein the boundary of the obstacle is determinedfrom measurements of the scattered field at a collection of receivers outsidethe object. One of the standard approaches for solving this problem is toreformulate it as an optimization problem: finding the boundary of the domainthat minimizes the $L^2$ distance between computed values of the scatteredfield and the given measurement data. The optimization problem iscomputationally challenging since the local set of convexity shrinks withincreasing frequency and results in an increasing number of local minima in thevicinity of the true solution. In many practical experimental settings, lowfrequency measurements are unavailable due to limitations of the experimentalsetup or the sensors used for measurement. Thus, obtaining a good initial guessfor the optimization problem plays a vital role in this environment.We present a neural network warm-start approach for solving the inversescattering problem, where an initial guess for the optimization problem isobtained using a trained neural network. We demonstrate the effectiveness ofour method with several numerical examples. For high frequency problems, thisapproach outperforms traditional iterative methods such as Gauss-Newtoninitialized without any prior (i.e., initialized using a unit circle), orinitialized using the solution of a direct method such as the linear samplingmethod. The algorithm remains robust to noise in the scattered fieldmeasurements and also converges to the true solution for limited aperture data.However, the number of training samples required to train the neural networkscales exponentially in frequency and the complexity of the obstaclesconsidered. We conclude with a discussion of this phenomenon and potentialdirections for future research.", "output": "A Neural Network Warm-Start Approach for the Inverse Acoustic Obstacle Scattering Problem."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a new algorithm for automatically bounding the Taylor remainderseries. In the special case of a scalar function $f: mathbb{R} tomathbb{R}$, our algorithm takes as input a reference point $x_0$, trust region$[a, b]$, and integer $k ge 1$, and returns an interval $I$ such that $f(x) -sum_{i=0}^{k-1} frac {1} {i!} f^{(i)}(x_0) (x - x_0)^i in I (x - x_0)^k$ forall $x in [a, b]$. As in automatic differentiation, the function $f$ isprovided to the algorithm in symbolic form, and must be composed of knownatomic functions.At a high level, our algorithm has two steps. First, for a variety ofcommonly-used elementary functions (e.g., $exp$, $log$), we userecently-developed theory to derive sharp polynomial upper and lower bounds onthe Taylor remainder series. We then recursively combine the bounds for theelementary functions using an interval arithmetic variant of Taylor-modeautomatic differentiation. Our algorithm can make efficient use of machinelearning hardware accelerators, and we provide an open source implementation inJAX.We then turn our attention to applications. Most notably, in a companionpaper we use our new machinery to create the first universalmajorization-minimization optimization algorithms: algorithms that iterativelyminimize an arbitrary loss using a majorizer that is derived automatically,rather than by hand. We also show that our automatically-derived bounds can beused for verified global optimization and numerical integration, and to provesharper versions of Jensen's inequality.", "output": "Automatically Bounding the Taylor Remainder Series: Tighter Bounds and New Applications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The literature on machine learning in the context of data streams is vast andgrowing. However, many of the defining assumptions regarding data-streamlearning tasks are too strong to hold in practice, or are even contradictorysuch that they cannot be met in the contexts of supervised learning. Algorithmsare chosen and designed based on criteria which are often not clearly stated,for problem settings not clearly defined, tested in unrealistic settings,and/or in isolation from related approaches in the wider literature. This putsinto question the potential for real-world impact of many approaches conceivedin such contexts, and risks propagating a misguided research focus. We proposeto tackle these issues by reformulating the fundamental definitions andsettings of supervised data-stream learning with regard to contemporaryconsiderations of concept drift and temporal dependence; and we take a freshlook at what constitutes a supervised data-stream learning task, and areconsideration of algorithms that may be applied to tackle such tasks. Throughand in reflection of this formulation and overview, helped by an informalsurvey of industrial players dealing with real-world data streams, we providerecommendations. Our main emphasis is that learning from data streams does notimpose a single-pass or online-learning approach, or any particular learningregime; and any constraints on memory and time are not specific to streaming.Meanwhile, there exist established techniques for dealing with temporaldependence and concept drift, in other areas of the literature. For the datastreams community, we thus encourage a shift in research focus, from dealingwith often-artificial constraints and assumptions on the learning mode, toissues such as robustness, privacy, and interpretability which are increasinglyrelevant to learning in data streams in academic and industrial settings.", "output": "Learning from Data Streams: An Overview and Update."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to quickly learn a new task with minimal instruction - known asfew-shot learning - is a central aspect of intelligent agents. Classicalfew-shot benchmarks make use of few-shot samples from a single modality, butsuch samples may not be sufficient to characterize an entire concept class. Incontrast, humans use cross-modal information to learn new concepts efficiently.In this work, we demonstrate that one can indeed build a better ${bf visual}$dog classifier by ${bf read}$ing about dogs and ${bf listen}$ing to thembark. To do so, we exploit the fact that recent multimodal foundation modelssuch as CLIP are inherently cross-modal, mapping different modalities to thesame representation space. Specifically, we propose a simple cross-modaladaptation approach that learns from few-shot examples spanning differentmodalities. By repurposing class names as additional one-shot training samples,we achieve SOTA results with an embarrassingly simple linear classifier forvision-language adaptation. Furthermore, we show that our approach can benefitexisting methods such as prefix tuning, adapters, and classifier ensembling.Finally, to explore other modalities beyond vision and language, we constructthe first (to our knowledge) audiovisual few-shot benchmark and use cross-modaltraining to improve the performance of both image and audio classification.", "output": "Multimodality Helps Unimodality: Cross-Modal Few-Shot Learning with Multimodal Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, multiple notions of algorithmic fairness have arisen. Onesuch notion is individual fairness (IF), which requires that individuals whoare similar receive similar treatment. In parallel, matrix estimation (ME) hasemerged as a natural paradigm for handling noisy data with missing values. Inthis work, we connect the two concepts. We show that pre-processing data usingME can improve an algorithm's IF without sacrificing performance. Specifically,we show that using a popular ME method known as singular value thresholding(SVT) to pre-process the data provides a strong IF guarantee under appropriateconditions. We then show that, under analogous conditions, SVT pre-processingalso yields estimates that are consistent and approximately minimax optimal. Assuch, the ME pre-processing step does not, under the stated conditions,increase the prediction error of the base algorithm, i.e., does not impose afairness-performance trade-off. We verify these results on synthetic and realdata.", "output": "Matrix Estimation for Individual Fairness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowing the actual precipitation in space and time is critical inhydrological modelling applications, yet the spatial coverage with rain gaugestations is limited due to economic constraints. Gridded satelliteprecipitation datasets offer an alternative option for estimating the actualprecipitation by covering uniformly large areas, albeit related estimates arenot accurate. To improve precipitation estimates, machine learning is appliedto merge rain gauge-based measurements and gridded satellite precipitationproducts. In this context, observed precipitation plays the role of thedependent variable, while satellite data play the role of predictor variables.Random forests is the dominant machine learning algorithm in relevantapplications. In those spatial predictions settings, point predictions (mostlythe mean or the median of the conditional distribution) of the dependentvariable are issued. The aim of the manuscript is to solve the problem ofprobabilistic prediction of precipitation with an emphasis on extreme quantilesin spatial interpolation settings. Here we propose, issuing probabilisticspatial predictions of precipitation using Light Gradient Boosting Machine(LightGBM). LightGBM is a boosting algorithm, highlighted by prize-winningentries in prediction and forecasting competitions. To assess LightGBM, wecontribute a large-scale application that includes merging daily precipitationmeasurements in contiguous US with PERSIANN and GPM-IMERG satelliteprecipitation data. We focus on extreme quantiles of the probabilitydistribution of the dependent variable, where LightGBM outperforms quantileregression forests (QRF, a variant of random forests) in terms of quantilescore at extreme quantiles. Our study offers understanding of probabilisticpredictions in spatial settings using machine learning.", "output": "Merging satellite and gauge-measured precipitation using LightGBM with an emphasis on extreme quantiles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper focusses on the optimal implementation of a Mean VarianceEstimation network (MVE network) (Nix and Weigend, 1994). This type of networkis often used as a building block for uncertainty estimation methods in aregression setting, for instance Concrete dropout (Gal et al., 2017) and DeepEnsembles (Lakshminarayanan et al., 2017). Specifically, an MVE network assumesthat the data is produced from a normal distribution with a mean function andvariance function. The MVE network outputs a mean and variance estimate andoptimizes the network parameters by minimizing the negative loglikelihood. Inour paper, we present two significant insights. Firstly, the convergencedifficulties reported in recent work can be relatively easily prevented byfollowing the simple yet often overlooked recommendation from the originalauthors that a warm-up period should be used. During this period, only the meanis optimized with a fixed variance. We demonstrate the effectiveness of thisstep through experimentation, highlighting that it should be standard practice.As a sidenote, we examine whether, after the warm-up, it is beneficial to fixthe mean while optimizing the variance or to optimize both simultaneously.Here, we do not observe a substantial difference. Secondly, we introduce anovel improvement of the MVE network: separate regularization of the mean andthe variance estimate. We demonstrate, both on toy examples and on a number ofbenchmark UCI regression data sets, that following the original recommendationsand the novel separate regularization can lead to significant improvements.", "output": "Optimal Training of Mean Variance Estimation Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The goal of continual learning is to improve the performance of recognitionmodels in learning sequentially arrived data. Although most existing works areestablished on the premise of learning from scratch, growing efforts have beendevoted to incorporating the benefits of pre-training. However, how toadaptively exploit the pre-trained knowledge for each incremental task whilemaintaining its generalizability remains an open question. In this work, wepresent an extensive analysis for continual learning on a pre-trained model(CLPM), and attribute the key challenge to a progressive overfitting problem.Observing that selectively reducing the learning rate can almost resolve thisissue in the representation layer, we propose a simple but extremely effectiveapproach named Slow Learner with Classifier Alignment (SLCA), which furtherimproves the classification layer by modeling the class-wise distributions andaligning the classification layers in a post-hoc fashion. Across a variety ofscenarios, our proposal provides substantial improvements for CLPM (e.g., up to49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, SplitCUB-200 and Split Cars-196, respectively), and thus outperformsstate-of-the-art approaches by a large margin. Based on such a strong baseline,critical factors and promising directions are analyzed in-depth to facilitatesubsequent research. Code has been made available at:", "output": "SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Precise and fast prediction methods for ischemic areas comprised of deadtissue, core, and salvageable tissue, penumbra, in acute ischemic stroke (AIS)patients are of significant clinical interest. They play an essential role inimproving diagnosis and treatment planning. Computed Tomography (CT) scan isone of the primary modalities for early assessment in patients with suspectedAIS. CT Perfusion (CTP) is often used as a primary assessment to determinestroke location, severity, and volume of ischemic lesions. Current automaticsegmentation methods for CTP mostly use already processed 3D parametric mapsconventionally used for clinical interpretation by radiologists as input.Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+timeinput, where the spatial information over the volume is ignored. In addition,these methods are only interested in segmenting core regions, while predictingpenumbra can be essential for treatment planning. This paper investigatesdifferent methods to utilize the entire 4D CTP as input to fully exploit thespatio-temporal information, leading us to propose a novel 4D convolutionlayer. Our comprehensive experiments on a local dataset of 152 patients dividedinto three groups show that our proposed models generate more precise resultsthan other methods explored. Adopting the proposed 4D mJ-Net, a DiceCoefficient of 0.53 and 0.23 is achieved for segmenting penumbra and coreareas, respectively. The code is available on", "output": "CT Perfusion is All We Need: 4D CNN Segmentation of Penumbra and Core in Patient With Suspected Ischemic Stroke."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Temporal data, representing chronological observations of complex systems,has always been a typical data structure that can be widely generated by manydomains, such as industry, medicine and finance. Analyzing this type of data isextremely valuable for various applications. Thus, different temporal dataanalysis tasks, eg, classification, clustering and prediction, have beenproposed in the past decades. Among them, causal discovery, learning the causalrelations from temporal data, is considered an interesting yet critical taskand has attracted much research attention. Existing causal discovery works canbe divided into two highly correlated categories according to whether thetemporal data is calibrated, ie, multivariate time series causal discovery, andevent sequence causal discovery. However, most previous surveys are onlyfocused on the time series causal discovery and ignore the second category. Inthis paper, we specify the correlation between the two categories and provide asystematical overview of existing solutions. Furthermore, we provide publicdatasets, evaluation metrics and new perspectives for temporal data causaldiscovery.", "output": "Causal Discovery from Temporal Data: An Overview and New Perspectives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Emotions play an essential role in human communication. Developing computervision models for automatic recognition of emotion expression can aid in avariety of domains, including robotics, digital behavioral healthcare, andmedia analytics. There are three types of emotional representations which aretraditionally modeled in affective computing research: Action Units, ValenceArousal (VA), and Categorical Emotions. As part of an effort to move beyondthese representations towards more fine-grained labels, we describe oursubmission to the newly introduced Emotional Reaction Intensity (ERI)Estimation challenge in the 5th competition for Affective Behavior Analysisin-the-Wild (ABAW). We developed four deep neural networks trained in thevisual domain and a multimodal model trained with both visual and audiofeatures to predict emotion reaction intensity. Our best performing model onthe Hume-Reaction dataset achieved an average Pearson correlation coefficientof 0.4080 on the test set using a pre-trained ResNet50 model. This workprovides a first step towards the development of production-grade models whichpredict emotion reaction intensities rather than discrete emotion categories.", "output": "Computer Vision Estimation of Emotion Reaction Intensity in the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Human intelligence excels at combining basic skills to solve complex tasks.This capability is vital for Artificial Intelligence (AI) and should beembedded in comprehensive intelligent models, enabling them to harness expertmodels for complex task-solving towards Artificial General Intelligence (AGI).Large Language Models (LLMs) show promising learning and reasoning abilities,and can effectively use external models, tools or APIs to tackle complexproblems. In this work, we introduce OpenAGI, an open-source AGI researchplatform designed for multi-step, real-world tasks. Specifically, OpenAGI usesa dual strategy, integrating standard benchmark tasks for benchmarking andevaluation, and open-ended tasks including more expandable models, tools orAPIs for creative problem-solving. Tasks are presented as natural languagequeries to the LLM, which then selects and executes appropriate models. We alsopropose a Reinforcement Learning from Task Feedback (RLTF) mechanism that usestask results to improve the LLM's ability, which creates a self-improving AIfeedback loop. While we acknowledge that AGI is a broad and multifacetedresearch challenge with no singularly defined solution path, the integration ofLLMs with domain-specific expert models, inspired by mirroring the blend ofgeneral and specialized intelligence in humans, offers a promising approachtowards AGI. We are open-sourcing the OpenAGI project's code, dataset,benchmarks, evaluation methods, and demo to foster community involvement in AGIadvancement: ", "output": "OpenAGI: When LLM Meets Domain Experts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In response to recent data regulation requirements, machine unlearning (MU)has emerged as a critical process to remove the influence of specific examplesfrom a given model. Although exact unlearning can be achieved through completemodel retraining using the remaining dataset, the associated computationalcosts have driven the development of efficient, approximate unlearningtechniques. Moving beyond data-centric MU approaches, our study introduces anovel model-based perspective: model sparsification via weight pruning, whichis capable of reducing the gap between exact unlearning and approximateunlearning. We show in both theory and practice that model sparsity can boostthe multi-criteria unlearning performance of an approximate unlearner, closingthe approximation gap, while continuing to be efficient. This leads to a new MUparadigm, termed prune first, then unlearn, which infuses a sparse model priorinto the unlearning process. Building on this insight, we also develop asparsity-aware unlearning method that utilizes sparsity regularization toenhance the training process of approximate unlearning. Extensive experimentsshow that our proposals consistently benefit MU in various unlearningscenarios. A notable highlight is the 77% unlearning efficacy gain offine-tuning (one of the simplest unlearning methods) when using sparsity-awareunlearning. Furthermore, we demonstrate the practical impact of our proposed MUmethods in addressing other machine learning challenges, such as defendingagainst backdoor attacks and enhancing transfer learning. Codes are availableat ", "output": "Model Sparsity Can Simplify Machine Unlearning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Simulating turbulence is critical for many societally important applicationsin aerospace engineering, environmental science, the energy industry, andbiomedicine. Large eddy simulation (LES) has been widely used as an alternativeto direct numerical simulation (DNS) for simulating turbulent flows due to itsreduced computational cost. However, LES is unable to capture all of the scalesof turbulent transport accurately. Reconstructing DNS from low-resolution LESis critical for many scientific and engineering disciplines, but it poses manychallenges to existing super-resolution methods due to the spatio-temporalcomplexity of turbulent flows. In this work, we propose a new physics-guidedneural network for reconstructing the sequential DNS from low-resolution LESdata. The proposed method leverages the partial differential equation thatunderlies the flow dynamics in the design of spatio-temporal modelarchitecture. A degradation-based refinement method is also developed toenforce physical constraints and further reduce the accumulated reconstructionerrors over long periods. The results on two different types of turbulent flowdata confirm the superiority of the proposed method in reconstructing thehigh-resolution DNS data and preserving the physical characteristics of flowtransport.", "output": "Reconstructing Turbulent Flows Using Physics-Aware Spatio-Temporal Dynamics and Test-Time Refinement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The field of radio astronomy is witnessing a boom in the amount of dataproduced per day due to newly commissioned radio telescopes. One of the mostcrucial problems in this field is the automatic classification of extragalacticradio sources based on their morphologies. Most recent contributions in thefield of morphological classification of extragalactic radio sources haveproposed classifiers based on convolutional neural networks. Alternatively,this work proposes gradient boosting machine learning methods accompanied byprincipal component analysis as data-efficient alternatives to convolutionalneural networks. Recent findings have shown the efficacy of gradient boostingmethods in outperforming deep learning methods for classification problems withtabular data. The gradient boosting methods considered in this work are basedon the XGBoost, LightGBM, and CatBoost implementations. This work also studiesthe effect of dataset size on classifier performance. A three-classclassification problem is considered in this work based on the three mainFanaroff-Riley classes: class 0, class I, and class II, using radio sourcesfrom the Best-Heckman sample. All three proposed gradient boosting methodsoutperformed a state-of-the-art convolutional neural networks-based classifierusing less than a quarter of the number of images, with CatBoost having thehighest accuracy. This was mainly due to the superior accuracy of gradientboosting methods in classifying Fanaroff-Riley class II sources, with3$unicode{x2013}$4% higher recall.", "output": "Morphological Classification of Extragalactic Radio Sources Using Gradient Boosting Methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A large amount of new malware is constantly being generated, which must notonly be distinguished from benign samples, but also classified into malwarefamilies. For this purpose, investigating how existing malware families aredeveloped and examining emerging families need to be explored. This paperfocuses on the online processing of incoming malicious samples to assign themto existing families or, in the case of samples from new families, to clusterthem. We experimented with seven prevalent malware families from the EMBERdataset, four in the training set and three additional new families in the testset. Based on the classification score of the multilayer perceptron, wedetermined which samples would be classified and which would be clustered intonew malware families. We classified 97.21% of streaming data with a balancedaccuracy of 95.33%. Then, we clustered the remaining data using aself-organizing map, achieving a purity from 47.61% for four clusters to 77.68%for ten clusters. These results indicate that our approach has the potential tobe applied to the classification and clustering of zero-day malware intomalware families.", "output": "Classification and Online Clustering of Zero-Day Malware."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, significant advancements have been made in time-series forecastingresearch, with an increasing focus on analyzing the nature of time-series data,e.g, channel-independence (CI) and channel-dependence (CD), rather than solelyfocusing on designing sophisticated forecasting models. However, currentresearch has primarily focused on either CI or CD in isolation, and thechallenge of effectively combining these two opposing properties to achieve asynergistic effect remains an unresolved issue. In this paper, we carefullyexamine the opposing properties of CI and CD, and raise a practical questionthat has not been effectively answered, e.g.,\"How to effectively mix the CI andCD properties of time series to achieve better predictive performance?\" Toanswer this question, we propose Mlinear (MIX-Linear), a simple yet effectivemethod based mainly on linear layers. The design philosophy of Mlinear mainlyincludes two aspects:(1) dynamically tuning the CI and CD properties based onthe time semantics of different input time series, and (2) providing deepsupervision to adjust the individual performance of the \"CI predictor\" and \"CDpredictor\". In addition, empirically, we introduce a new loss function thatsignificantly outperforms the widely used mean squared error (MSE) on multipledatasets. Experiments on time-series datasets covering multiple fields andwidely used have demonstrated the superiority of our method over PatchTST whichis the lateset Transformer-based method in terms of the MSE and MAE metrics on7 datasets with identical sequence inputs (336 or 512). Specifically, ourmethod significantly outperforms PatchTST with a ratio of 21:3 at 336 sequencelength input and 29:10 at 512 sequence length input. Additionally, our approachhas a 10 $times$ efficiency advantage at the unit level, taking into accountboth training and inference times.", "output": "Mlinear: Rethink the Linear Model for Time-series Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a latent variable generalisation of neural network softmaxclassification trained with cross-entropy loss, referred to as variationalclassification (VC). Our approach offers a novel probabilistic perspective onthe highly familiar softmax classification model, to which it relates similarlyto how variational and traditional autoencoders relate. We derive a trainingobjective based on the evidence lower bound (ELBO) that is non-trivial tooptimize, and therefore propose an adversarial approach to maximise it. We showthat VC addresses an inherent inconsistency within softmax classification,whilst also allowing more flexible choices of prior distributions in the latentspace in place of implicit assumptions revealed within off-the-shelf softmaxclassifiers. Empirical evaluation on image and text classification datasetsdemonstrates that variational classification maintains prediction accuracywhile improving other desirable properties such as calibration and adversarialrobustness, particularly under distribution shift and low data settings.", "output": "Variational Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Latent Graph Inference (LGI) relaxed the reliance of Graph Neural Networks(GNNs) on a given graph topology by dynamically learning it. However, most ofLGI methods assume to have a (noisy, incomplete, improvable, ...) input graphto rewire and can solely learn regular graph topologies. In the wake of thesuccess of Topological Deep Learning (TDL), we study Latent Topology Inference(LTI) for learning higher-order cell complexes (with sparse and not regulartopology) describing multi-way interactions between data points. To this aim,we introduce the Differentiable Cell Complex Module (DCM), a novel learnablefunction that computes cell probabilities in the complex to improve thedownstream task. We show how to integrate DCM with cell complex message passingnetworks layers and train it in a end-to-end fashion, thanks to a two-stepinference procedure that avoids an exhaustive search across all possible cellsin the input, thus maintaining scalability. Our model is tested on severalhomophilic and heterophilic graph datasets and it is shown to outperform otherstate-of-the-art techniques, offering significant improvements especially incases where an input graph is not provided.", "output": "From Latent Graph to Latent Topology Inference: Differentiable Cell Complex Module."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extensive research has been conducted on fault diagnosis of planetarygearboxes using vibration signals and deep learning (DL) approaches. However,DL-based methods are susceptible to the domain shift problem caused by varyingoperating conditions of the gearbox. Although domain adaptation and datasynthesis methods have been proposed to overcome such domain shifts, they areoften not directly applicable in real-world situations where only healthy datais available in the target domain. To tackle the challenge of extreme domainshift scenarios where only healthy data is available in the target domain, thispaper proposes two novel domain knowledge-informed data synthesis methodsutilizing the health data map (HDMap). The two proposed approaches are referredto as scaled CutPaste and FaultPaste. The HDMap is used to physically representthe vibration signal of the planetary gearbox as an image-like matrix, allowingfor visualization of fault-related features. CutPaste and FaultPaste are thenapplied to generate faulty samples based on the healthy data in the targetdomain, using domain knowledge and fault signatures extracted from the sourcedomain, respectively. In addition to generating realistic faults, the proposedmethods introduce scaling of fault signatures for controlled synthesis offaults with various severity levels. A case study is conducted on a planetarygearbox testbed to evaluate the proposed approaches. The results show that theproposed methods are capable of accurately diagnosing faults, even in cases ofextreme domain shift, and can estimate the severity of faults that have notbeen previously observed in the target domain.", "output": "Domain knowledge-informed Synthetic fault sample generation with Health Data Map for cross-domain Planetary Gearbox Fault Diagnosis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image generative models have enabled high-resolution image synthesisacross different domains, but require users to specify the content they wish togenerate. In this paper, we consider the inverse problem -- given a collectionof different images, can we discover the generative concepts that representeach image? We present an unsupervised approach to discover generative conceptsfrom a collection of images, disentangling different art styles in paintings,objects, and lighting from kitchen scenes, and discovering image classes givenImageNet images. We show how such generative concepts can accurately representthe content of images, be recombined and composed to generate new artistic andhybrid images, and be further used as a representation for downstreamclassification tasks.", "output": "Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study property prediction for crystal materials. A crystal structureconsists of a minimal unit cell that is repeated infinitely in 3D space. How toaccurately represent such repetitive structures in machine learning modelsremains unresolved. Current methods construct graphs by establishing edges onlybetween nearby nodes, thereby failing to faithfully capture infinite repeatingpatterns and distant interatomic interactions. In this work, we propose severalinnovations to overcome these limitations. First, we propose to modelphysics-principled interatomic potentials directly instead of only usingdistances as in many existing methods. These potentials include the Coulombpotential, London dispersion potential, and Pauli repulsion potential. Second,we model the complete set of potentials among all atoms, instead of onlybetween nearby atoms as in existing methods. This is enabled by ourapproximations of infinite potential summations with provable error bounds. Wefurther develop efficient algorithms to compute the approximations. Finally, wepropose to incorporate our computations of complete interatomic potentials intomessage passing neural networks for representation learning. We performexperiments on the JARVIS and Materials Project benchmarks for evaluation.Results show that the use of interatomic potentials and complete interatomicpotentials leads to consistent performance improvements with reasonablecomputational costs. Our code is publicly available as part of the AIRS library(", "output": "Efficient Approximations of Complete Interatomic Potentials for Crystal Property Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have emerged as the emph{de-facto} technique for imagegeneration, yet they entail significant computational overhead, hindering thetechnique's broader application in the research community. We propose aprior-based denoising training framework, the first to incorporate thepre-train and fine-tune paradigm into the diffusion model training process,which substantially improves training efficiency and shows potential infacilitating various downstream tasks. Our approach centers on masking a highproportion (e.g., up to 90%) of the input image and employing masked denoisingscore matching to denoise the visible areas, thereby guiding the diffusionmodel to learn more salient features from training data as prior knowledge. Byutilizing masked learning in a pre-training stage, we efficiently train theViT-based diffusion model on CelebA-HQ $256 times 256$ in the pixel space,achieving a 4x acceleration and enhancing the quality of generated imagescompared to denoising diffusion probabilistic model (DDPM). Moreover, ourmasked pre-training technique can be universally applied to various diffusionmodels that directly generate images in the pixel space, aiding in the learningof pre-trained models with superior generalizability. For instance, a diffusionmodel pre-trained on VGGFace2 attains a 46% quality improvement throughfine-tuning with merely 10% data from a different distribution. Moreover, ourmethod shows the potential to serve as a training paradigm for enhancing theprivacy protection capabilities of diffusion models. Our code is available aturl{", "output": "Masked Diffusion Models Are Fast and Privacy-Aware Learners."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Algorithms for solving the linear classification problem have a long history,dating back at least to 1936 with linear discriminant analysis. For linearlyseparable data, many algorithms can obtain the exact solution to thecorresponding 0-1 loss classification problem efficiently, but for data whichis not linearly separable, it has been shown that this problem, in fullgenerality, is NP-hard. Alternative approaches all involve approximations ofsome kind, including the use of surrogates for the 0-1 loss (for example, thehinge or logistic loss) or approximate combinatorial search, none of which canbe guaranteed to solve the problem exactly. Finding efficient algorithms toobtain an exact i.e. globally optimal solution for the 0-1 loss linearclassification problem with fixed dimension, remains an open problem. Inresearch we report here, we detail the rigorous construction of a newalgorithm, incremental cell enumeration (ICE), that can solve the 0-1 lossclassification problem exactly in polynomial time. We prove correctness usingconcepts from the theory of hyperplane arrangements and oriented matroids. Wedemonstrate the effectiveness of this algorithm on synthetic and real-worlddatasets, showing optimal accuracy both in and out-of-sample, in practicalcomputational time. We also empirically demonstrate how the use of approximateupper bound leads to polynomial time run-time improvements to the algorithmwhilst retaining exactness. To our knowledge, this is the first,rigorously-proven polynomial time, practical algorithm for this long-standingproblem.", "output": "An efficient, provably exact, practical algorithm for the 0-1 loss linear classification problem."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper we adapt the nearest neighbour rule to the contextual banditproblem. Our algorithm handles the fully adversarial setting in which noassumptions at all are made about the data-generation process. When combinedwith a sufficiently fast data-structure for (perhaps approximate) adaptivenearest neighbour search, such as a navigating net, our algorithm is extremelyefficient - having a per trial running time polylogarithmic in both the numberof trials and actions, and taking only quasi-linear space.", "output": "Nearest Neighbour with Bandit Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "DNA methylation is a crucial regulator of gene transcription and has beenlinked to various diseases, including autoimmune diseases and cancers. However,diagnostics based on DNA methylation face challenges due to large feature setsand small sample sizes, resulting in overfitting and suboptimal performance. Toaddress these issues, we propose MIRACLE, a novel interpretable neural networkthat leverages autoencoder-based multi-task learning to integrate multipledatasets and jointly identify common patterns in DNA methylation.MIRACLE's architecture reflects the relationships between methylation sites,genes, and pathways, ensuring biological interpretability and meaningfulness.The network comprises an encoder and a decoder, with a bottleneck layerrepresenting pathway information as the basic unit of heredity. Customizeddefined MaskedLinear Layer is constrained by site-gene-pathway graph adjacencymatrix information, which provides explainability and expresses thesite-gene-pathway hierarchical structure explicitly. And from the embedding,there are different multi-task classifiers to predict diseases.Tested on six datasets, including rheumatoid arthritis, systemic lupuserythematosus, multiple sclerosis, inflammatory bowel disease, psoriasis, andtype 1 diabetes, MIRACLE demonstrates robust performance in identifying commonfunctions of DNA methylation across different phenotypes, with higher accuracyin prediction dieseases than baseline methods. By incorporating biologicalprior knowledge, MIRACLE offers a meaningful and interpretable framework forDNA methylation data analysis in the context of autoimmune diseases.", "output": "MIRACLE: Multi-task Learning based Interpretable Regulation of Autoimmune Diseases through Common Latent Epigenetics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision-based teleoperation offers the possibility to endow robots withhuman-level intelligence to physically interact with the environment, whileonly requiring low-cost camera sensors. However, current vision-basedteleoperation systems are designed and engineered towards a particular robotmodel and deploy environment, which scales poorly as the pool of the robotmodels expands and the variety of the operating environment increases. In thispaper, we propose AnyTeleop, a unified and general teleoperation system tosupport multiple different arms, hands, realities, and camera configurationswithin a single system. Although being designed to provide great flexibility tothe choice of simulators and real hardware, our system can still achieve greatperformance. For real-world experiments, AnyTeleop can outperform a previoussystem that was designed for a specific robot hardware with a higher successrate, using the same robot. For teleoperation in simulation, AnyTeleop leads tobetter imitation learning performance, compared with a previous system that isparticularly designed for that simulator. Project page: <a href=\" http URL</a>", "output": "AnyTeleop: A General Vision-Based Dexterous Robot Arm-Hand Teleoperation System."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the rapid amassing of spatial-temporal (ST) ocean data, manyspatial-temporal data mining (STDM) studies have been conducted to addressvarious oceanic issues, including climate forecasting and disaster warning.Compared with typical ST data (e.g., traffic data), ST ocean data is morecomplicated but with unique characteristics, e.g., diverse regionality and highsparsity. These characteristics make it difficult to design and train STDMmodels on ST ocean data. To the best of our knowledge, a comprehensive surveyof existing studies remains missing in the literature, which hinders not onlycomputer scientists from identifying the research issues in ocean data miningbut also ocean scientists to apply advanced STDM techniques. In this paper, weprovide a comprehensive survey of existing STDM studies for ocean science.Concretely, we first review the widely-used ST ocean datasets and highlighttheir unique characteristics. Then, typical ST ocean data quality enhancementtechniques are explored. Next, we classify existing STDM studies in oceanscience into four types of tasks, i.e., prediction, event detection, patternmining, and anomaly detection, and elaborate on the techniques for these tasks.Finally, promising research opportunities are discussed. This survey can helpscientists from both computer science and ocean science better understand thefundamental concepts, key techniques, and open challenges of STDM for oceanscience.", "output": "Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "For safety-related applications, it is crucial to produce trustworthy deepneural networks whose prediction is associated with confidence that canrepresent the likelihood of correctness for subsequent decision-making.Existing dense binary classification models are prone to being over-confident.To improve model calibration, we propose Adaptive Stochastic Label Perturbation(ASLP) which learns a unique label perturbation level for each training image.ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss,which unifies label perturbation processes including stochastic approaches(like DisturbLabel), and label smoothing, to correct calibration whilemaintaining classification rates. ASLP follows Maximum Entropy Inference ofclassic statistical mechanics to maximise prediction entropy with respect tomissing information. It performs this while: (1) preserving classificationaccuracy on known data as a conservative solution, or (2) specifically improvesmodel calibration degree by minimising the gap between the prediction accuracyand expected confidence of the target training label. Extensive resultsdemonstrate that ASLP can significantly improve calibration degrees of densebinary classification models on both in-distribution and out-of-distributiondata. The code is available on ", "output": "Model Calibration in Dense Classification with Adaptive Label Perturbation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Stochastic gradient descent (SGD) is the simplest deep learning optimizerwith which to train deep neural networks. While SGD can use various learningrates, such as constant or diminishing rates, the previous numerical resultsshowed that SGD performs better than other deep learning optimizers using whenit uses learning rates given by line search methods. In this paper, we performa convergence analysis on SGD with a learning rate given by an Armijo linesearch for nonconvex optimization. The analysis indicates that the upper boundof the expectation of the squared norm of the full gradient becomes small whenthe number of steps and the batch size are large. Next, we show that, for SGDwith the Armijo-line-search learning rate, the number of steps needed fornonconvex optimization is a monotone decreasing convex function of the batchsize; that is, the number of steps needed for nonconvex optimization decreasesas the batch size increases. Furthermore, we show that the stochasticfirst-order oracle (SFO) complexity, which is the stochastic gradientcomputation cost, is a convex function of the batch size; that is, there existsa critical batch size that minimizes the SFO complexity. Finally, we providenumerical results that support our theoretical results. The numerical resultsindicate that the number of steps needed for training deep neural networksdecreases as the batch size increases and that there exist the critical batchsizes that can be estimated from the theoretical results.", "output": "Relationship between Batch Size and Number of Steps Needed for Nonconvex Optimization of Stochastic Gradient Descent using Armijo Line Search."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy theft detection (ETD) and energy consumption forecasting (ECF) are twointerconnected challenges in smart grid systems. Addressing these issuescollectively is crucial for ensuring system security. This paper addresses theinterconnected challenges of ETD and ECF in smart grid systems. The proposedsolution combines long short-term memory (LSTM) and a denoising diffusionprobabilistic model (DDPM) to generate input reconstruction and forecasting. Byleveraging the reconstruction and forecasting errors, the system identifiesinstances of energy theft, with the methods based on reconstruction error andforecasting error complementing each other in detecting different types ofattacks. Through extensive experiments on real-world and synthetic datasets,the proposed scheme outperforms baseline methods in ETD and ECF problems. Theensemble method significantly enhances ETD performance, accurately detectingenergy theft attacks that baseline methods fail to detect. The research offersa comprehensive and effective solution for addressing ETD and ECF challenges,demonstrating promising results and improved security in smart grid systems.", "output": "An Effective LSTM-DDPM Scheme for Energy Theft Detection and Forecasting in Smart Grid."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models and large language models have emerged as leading-edgegenerative models and have sparked a revolutionary impact on various aspects ofhuman life. However, the practical implementation of these models has alsoexposed inherent risks, highlighting their dual nature and raising concernsregarding their trustworthiness. Despite the abundance of literature on thissubject, a comprehensive survey specifically delving into the intersection oflarge-scale generative models and their trustworthiness remains largely absent.To bridge this gap, This paper investigates both the long-standing and emergingthreats associated with these models across four fundamental dimensions:privacy, security, fairness, and responsibility. In this way, we construct anextensive map outlining the trustworthiness of these models, while alsoproviding practical recommendations and identifying future directions. Theseefforts are crucial for promoting the trustworthy deployment of these models,ultimately benefiting society as a whole.", "output": "On the Trustworthiness Landscape of State-of-the-art Generative Models: A Comprehensive Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, bi-level optimization (BLO) has taken center stage in some veryexciting developments in the area of signal processing (SP) and machinelearning (ML). Roughly speaking, BLO is a classical optimization problem thatinvolves two levels of hierarchy (i.e., upper and lower levels), whereinobtaining the solution to the upper-level problem requires solving thelower-level one. BLO has become popular largely because it is powerful inmodeling problems in SP and ML, among others, that involve optimizing nestedobjective functions. Prominent applications of BLO range from resourceallocation for wireless systems to adversarial machine learning. In this work,we focus on a class of tractable BLO problems that often appear in SP and MLapplications. We provide an overview of some basic concepts of this class ofBLO problems, such as their optimality conditions, standard algorithms(including their optimization principles and practical implementations), aswell as how they can be leveraged to obtain state-of-the-art results for anumber of key SP and ML applications. Further, we discuss some recent advancesin BLO theory, its implications for applications, and point out somelimitations of the state-of-the-art that require significant future researchefforts. Overall, we hope that this article can serve to accelerate theadoption of BLO as a generic tool to model, analyze, and innovate on a widearray of emerging SP and ML applications.", "output": "An Introduction to Bi-level Optimization: Foundations and Applications in Signal Processing and Machine Learning."}]